{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ml2_group_assignment.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=green> Introduction </font>\n",
    "\n",
    "This is a continuation of forest_cover_type_detector_gr_a_Part1.\n",
    "\n",
    "Above we import the files created in the previous notebook so that this notebook can run independently\n",
    "\n",
    "Table of contents is an extension of the previous notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=green> Table of contents </font>\n",
    "\n",
    "* [Libaries used](#0)\n",
    "* [0. Import Data](#0)\n",
    "* [__5. Feature Engineering__](#5) \n",
    "  * [5.1 Check for Anomalies and Outliers](#5.1)\n",
    "       * [5.1.1 Outlier Detection Treatment using Inter-Quartile Range rule Function](#5.1.1)\n",
    "       * [5.1.2 Inter-Quartile Range rule: 4 IQR from Median](#5.1.2)\n",
    "       * [5.1.3 Inter-Quartile Range rule: 3 IQR from Median](#5.1.3)\n",
    "  * [5.2 Feature Transformation and Building of new features](#5.2)\n",
    "      * [5.2.1 Bivariate Combinations](#5.2.1)  \n",
    "      * [5.2.2 Polynominal](#5.2.2)\n",
    "      * [5.2.3 ID](#5.2.3)  \n",
    "      * [5.2.4 Distance to Hydrology](#5.2.4)   \n",
    "      * [5.2.5 Horizontal Distance To Roadways ](#5.2.5) \n",
    "      * [5.2.6 Slope](#5.2.6)  \n",
    "      * [5.2.7 Horizontal Distance To Fire Points ](#5.2.7)  \n",
    "      * [5.2.8 Hillshade](#5.2.8) \n",
    "          * [5.2.8.1 Mean Hillshade](#5.2.8.1) \n",
    "          * [5.2.8.2 Hillshade 9am](#5.2.8.2)\n",
    "          * [5.2.8.3 Hillshade Noon](#5.2.8.3)          \n",
    "          * [5.2.8.4 Hillshade 3pm](#5.2.8.4)       \n",
    "          * [5.2.8.5 Hillshade Ratios](#5.2.8.5)        \n",
    "      * [5.2.9 Geoclimate Groping](#5.2.9) \n",
    "* [__6. Feature Selection__](#6)\n",
    "  * [6.0 Prepare Data and Standardization](#6.1)\n",
    "  * [6.1 Single tree](#6.2) \n",
    "  * [6.2 Bagging](#6.2) \n",
    "  * [6.3 Random Forest](#6.3)  \n",
    "  * [6.4 Extra Trees](#6.4)\n",
    "       * [6.4.1 Feature Number Selecion](#6.6.1)       \n",
    "       * [6.4.2 Lasso Regularization](#6.6.2)\n",
    "       * [6.4.3 Filter Methods](#6.6.3)   \n",
    "  * [6.5 Recursive Feature Elimination](#6.5)  \n",
    "  * [6.6 Tree Based Methodologies](#6.6) \n",
    "       * [6.6.1 RandomForestClassifier](#6.6.1)       \n",
    "       * [6.6.2 XGBoost](#6.6.2)\n",
    "       * [6.6.3 Extra trees Classifier](#6.6.3)       \n",
    "  * [6.7 Score of all methods Together](#6.7) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "# <font color=green> Libraries used </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install squarify\n",
    "#!pip install htmltabletomd\n",
    "#!pip install GraphViz\n",
    "#!pip install pygraphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math \n",
    "import seaborn as sns  # Graphing\n",
    "import matplotlib.pyplot as plt\n",
    "import squarify #treemap\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import plotly.graph_objects as go\n",
    "import xgboost as xgb\n",
    "import scipy.stats as stats\n",
    "import htmltabletomd\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "from yellowbrick.target import FeatureCorrelation\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "from yellowbrick.model_selection import rfecv\n",
    "\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "from IPython.display import Image  \n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "from dtreeviz.trees import *\n",
    "\n",
    "\n",
    "from numpy import percentile\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "##  <font color=green>0.Import the Data </font>\n",
    "\n",
    "Let’s load the original Kaggle training and test data and create a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(\"train.csv\")\n",
    "data_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep the original dataset for later comparisons and make a copy for the FE process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = data_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "<a id='5.1'></a>\n",
    "# <font color=green>  5.Feature Engineering<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2'></a>\n",
    "## <font color=green> 5.2 Feature Transformation and Building of new features <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.1'></a>\n",
    "### <font color=green> 5.2.1 Bivariate Combinations <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During feature engineering, we want to try to create a wide variety of interactions between multiple variables in order to create new variables. \n",
    "\n",
    "\n",
    "By manipulating them together, we create opportunities to have new and impactful features which could potentially impact our target variable, thus engineering our features. \n",
    "\n",
    "For this argument, we will create as many bivariate combinations of our predicting variables using the ‘combinations’ method from itertools library.\n",
    "\n",
    "We will not make interactions with the dummy variables as these are either 0 or 1 and we will not get any additional information from making the interaction this way. \n",
    "\n",
    "Furthermore, it is not recommended to use standardization before bivariate combinations as we want to increase the signal. <br>\n",
    "\n",
    "Sources: https://towardsdatascience.com/feature-engineering-combination-polynomial-features-3caa4c77a755 <br>\n",
    "\n",
    "https://samchaaa.medium.com/preprocessing-why-you-should-generate-polynomial-features-first-before-standardizing-892b4326a91d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the bivariate combination we split the dataset for using it.Note this is not the split we will use later for testing the algorithm. This has only the purpose of testing all the combination and selecting the best once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and drop our target variable 'Cover_Type' from dataframe\n",
    "X = data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Test Split (80/20 size), drop duplicates and missing values\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = .2, random_state=37, stratify=y)\n",
    "\n",
    "X_train.drop_duplicates(inplace = True)\n",
    "X_train.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create every possible bivariate combination to be tested for feature engineering, no dummies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take categorical variables prior to our feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = X.columns\n",
    "filtered_column_list = [column for column in column_list if 'Soil_Type' not in column and 'Wilderness_Area' not in column and 'Id' not in column ] \n",
    "interactions = list(combinations(filtered_column_list, 2))\n",
    "interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addition and division has been taken out as it created a lot of noise in the data. The division makes sense if it has a business meaning and the addition only if it is the same scale. \n",
    "\n",
    "However we will add the variables which have the same metrics together in a second step but not in a for loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (key, value) in interactions:\n",
    "    data_test[key + '_x_' + value] = data_test[key] * data_test[value]\n",
    "    #data_test[key + '_+_' + value] = data_test[key] + data_test[value]\n",
    "    #data_test[key + '_divide_' + value] = data_test[key] / data_test[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None) #to make all columns visible in dataframe now that we have many\n",
    "data_test.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.2'></a>\n",
    "### <font color=green> 5.2.2 Polynomial Features <font>\n",
    "    \n",
    "We have just seen how to make two variables interact together, but sometimes the relationship between dependent and independent variables are more complex and not linear. \n",
    "    \n",
    "Polynomials is another way to create new features! A very strong option for new features is increasing the power of a single variable. \n",
    "    \n",
    "For our purposes, we will try and see if all the existing variables, can improve our Baseline by being increased to the  power.<br>\n",
    "Source: https://towardsdatascience.com/feature-engineering-combination-polynomial-features-3caa4c77a755"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we select only the columns we are interested in, this is from column 2 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pf = data_test.iloc[:, 1:10]\n",
    "X_pf.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, when running polynomials we lose the name of our labels. This function gives us the ability to preserve it with the name + the transformation done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialFeatures_labeled(input_df,power):\n",
    "    '''Basically this is a cover for the sklearn preprocessing function. \n",
    "    The problem with that function is if you give it a labeled dataframe, it ouputs an unlabeled dataframe with potentially\n",
    "    a whole bunch of unlabeled columns. \n",
    "\n",
    "    Inputs:\n",
    "    input_df = Your labeled pandas dataframe (list of x's not raised to any power) \n",
    "    power = what order polynomial you want variables up to. (use the same power as you want entered into pp.PolynomialFeatures(power) directly)\n",
    "\n",
    "    Ouput:\n",
    "    Output: This function relies on the powers_ matrix which is one of the preprocessing function's outputs to create logical labels and \n",
    "    outputs a labeled pandas dataframe   \n",
    "    '''\n",
    "    poly = PolynomialFeatures(power)\n",
    "    output_nparray = poly.fit_transform(input_df)\n",
    "    powers_nparray = poly.powers_\n",
    "\n",
    "    input_feature_names = list(input_df.columns)\n",
    "    target_feature_names = [\"Constant Term\"]\n",
    "    for feature_distillation in powers_nparray[1:]:\n",
    "        intermediary_label = \"\"\n",
    "        final_label = \"\"\n",
    "        for i in range(len(input_feature_names)):\n",
    "            if feature_distillation[i] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                variable = input_feature_names[i]\n",
    "                power = feature_distillation[i]\n",
    "                intermediary_label = \"%s^%d\" % (variable,power)\n",
    "                if final_label == \"\":         #If the final label isn't yet specified\n",
    "                    final_label = intermediary_label\n",
    "                else:\n",
    "                    final_label = final_label + \" x \" + intermediary_label\n",
    "        target_feature_names.append(final_label)\n",
    "    output_df = pd.DataFrame(output_nparray, columns = target_feature_names)\n",
    "    return output_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynominal features of degree two gives us 46 new features. Since we have already enough information, we will not go for Polynominal three to avoid dimensionality issues later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_pw2 = PolynomialFeatures_labeled(X_pf,2)\n",
    "pd.set_option('display.max_columns', None)\n",
    "test_df_pw2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some fields duplicated with respect to the original df. This is a side effect of the function as normal features are replicated to the power of one which is still the same value so we delete these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = test_df_pw2.columns\n",
    "cols = [column for column in column_list if '^1' not in column]\n",
    "test_df_pw2=test_df_pw2[cols]\n",
    "test_df_pw2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In here, we concatenate our output to consolidate the ponlynomials with the feature combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.concat([data_test,test_df_pw2], axis=1)\n",
    "data_test.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in a huge dataset. Just for curiosity we ran polynomials to the power of three and see that just the resulting dataframe is already bigger than the consolidated one above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.3'></a>\n",
    "### <font color=green> 5.2.3 ID <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We agree for the test to not remove ID because the ID is the unique indentifier to evaluate\n",
    "\n",
    "\n",
    "For the train we will remove it as it doesn't add any value to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.drop('Id',axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.4'></a>\n",
    "### <font color=green> 5.2.4 Distance To Hydrology <font>\n",
    "#### <font color=green> New Features <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine Vertical distance to Hydrology and Horizontal distance to Hydrology since these two are highly correlated. This suggests to attempt a diagonal distance to hidrology using the Pythagoras theorem.\n",
    "\n",
    "We will call this newly engineered feature, Distance_To_Hydrology\n",
    " \n",
    "Source : https://towardsdatascience.com/types-of-transformations-for-better-normal-distribution-61c22668d3b9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Distance_To_Hydrology'] = data_test['Horizontal_Distance_To_Hydrology']**2 +data_test['Vertical_Distance_To_Hydrology']**2\n",
    "data_test['Distance_To_Hydrology'] = data_test['Distance_To_Hydrology']**0.5\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Square root and logarithm Transformation  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are checking the distribution of the newly created variable and see if further transformation is needed. \n",
    "\n",
    "The Distance to Hydrology inherits skewness from parent variables. It is positively skewed and has zero values. \n",
    "\n",
    "In order to use log we will use log + 1 in order to use logarithm with zero values. \n",
    "\n",
    "Source: https://www.youtube.com/watch?v=_c3dVTRIK9c and \n",
    "\n",
    "Source_2: https://towardsdatascience.com/types-of-transformations-for-better-normal-distribution-61c22668d3b9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a rule of thumb, the skewness can be interpreted as follows:\n",
    "<img src=\"Skew.png\" width=400 height=200 align=\"center\">\n",
    "\n",
    "Source: https://www.marsja.se/transform-skewed-data-using-square-root-log-box-cox-methods-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do some transformations to minimize skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the log10+ 1 logarithm \n",
    "data_test['log10_Distance_To_Hydrology'] = np.log10(data_test['Distance_To_Hydrology']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the square root \n",
    "data_test['sqr_Distance_To_Hydrology'] = data_test['Distance_To_Hydrology']**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm and <font color=darkcyan> Square root Transformation<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, for distance to Hydrology the __square root__ showed a better performance in terms of skewness and is closer to a normal bell shaped than the logarithm transformation. We will be using Square Root as a new feature in the dataset and will frop the others from the dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we only remove log10 but later we will delete also originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.drop(['log10_Distance_To_Hydrology'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.5'></a>\n",
    "### <font color=green> 5.2.5 Horizontal Distance To Roadways <font>\n",
    "\n",
    "#### <font color=green> Square root and logarithm Transformation  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For log transformation there should be no zeros, negative values and the distribution should be positive skewed( bigger than 1 is positive) hence we are using the square root as you can see for logarithm transformation below the distribution did not improve!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm and <font color=darkcyan> Square root Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we have null values we add plus 1 to avoid log of zero.We are using natural log and log10\n",
    "data_test['Sqr_Horizontal_Distance_To_Roadways'] = data_test['Horizontal_Distance_To_Roadways']**0.5\n",
    "data_test['log_Horizontal_Distance_To_Roadways'] = np.log(data_test['Horizontal_Distance_To_Roadways']+1)\n",
    "data_test['log10_Horizontal_Distance_To_Roadways'] = np.log10(data_test['Horizontal_Distance_To_Roadways']+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieved the best result again using square root of the Horizontal Distance to Roadways. Similarly as before, we remove failed experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.drop(['log_Horizontal_Distance_To_Roadways','log10_Horizontal_Distance_To_Roadways'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.6'></a>\n",
    "### <font color=green> 5.2.6 Slope <font>\n",
    "#### <font color=green> Square root and logarithm Transformation  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+ \"Skew before transformation\\n\", data_test['Slope'].skew(), \n",
    "      \"\\nmin\\n\", data_test['Slope'].min(),\n",
    "      \"\\nmax \\n\", data_test['Slope'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm and <font color=darkcyan> Sqrare root Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we have null values we add plus 1 to avoid log of zero\n",
    "data_test['logSlope'] = np.log(data_test['Slope']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['SqrSlope'] = data_test['Slope']**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the skweness for the slope shows better performance when using the square root, we will transform the variable into square root as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.drop(['logSlope'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.7'></a>\n",
    "### <font color=green> 5.2.7 Horizontal Distance To Fire Points  <font>\n",
    "#### <font color=green> Transformation  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm and <font color=darkcyan> Sqrare root Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we have null values we add plus 1 to avoid log of zero\n",
    "data_test['log_Horizontal_Distance_To_firepoints'] = np.log(data_test['Horizontal_Distance_To_Fire_Points']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform with square root\n",
    "data_test['sqr_Horizontal_Distance_To_firepoints'] = data_test['Horizontal_Distance_To_Fire_Points']**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since square root transformation gives the best result in skewness, we will also use sqr for the feature variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.drop(['log_Horizontal_Distance_To_firepoints'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.8'></a>\n",
    "### <font color=green> 5.2.8 Hillshades <font>\n",
    "<a id='5.2.8.1'></a>\n",
    "### <font color=green> 5.2.8.1 Mean Hillshade <font>\n",
    "#### <font color=green> Creation of new Feature: Mean Hillshade <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take the average of Hillshades,which gives you the average light exposure of each cover type during the day\n",
    "data_test['Mean_Hillshade'] = (data_test['Hillshade_9am']+data_test['Hillshade_Noon']+data_test['Hillshade_3pm'])/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Itensity of the Hillshade variables in 3 bin siizes with the bin discretizer\n",
    "est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
    "data_test['Mean_Hillshade_bin'] = est.fit_transform(data_test[['Mean_Hillshade']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[['Mean_Hillshade_bin','Mean_Hillshade']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before transformation\\n\", data_test['Mean_Hillshade'].skew(), \n",
    "      \"\\nmin\\n\", data_test['Mean_Hillshade'].min(),\n",
    "      \"\\nmax\\n\", data_test['Mean_Hillshade'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm Transformation <font color=darkcyan>, Square root Transformation<font color=gold> and BoxCox Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['log_Mean_Hillshade'] = np.log(data_test['Mean_Hillshade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log transformation\\n\", data_test['log_Mean_Hillshade'].skew(), \n",
    "      \"\\nmin\\n\", data_test['log_Mean_Hillshade'].min(),\n",
    "      \"\\nmax\\n\", data_test['log_Mean_Hillshade'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['log10Mean_Hillshade'] = np.log10(data_test['Mean_Hillshade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log10 transformation\\n\", data_test['log10Mean_Hillshade'].skew(), \n",
    "      \"\\nmin\\n\", data_test['log10Mean_Hillshade'].min(),\n",
    "      \"\\nmax\\n\", data_test['log10Mean_Hillshade'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['sqr_Mean_Hillshade'] = data_test['Mean_Hillshade']**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+\"Skew after Square Root transformation\\n\", data_test['sqr_Mean_Hillshade'].skew(), \n",
    "      \"\\nmin\\n\", data_test['sqr_Mean_Hillshade'].min(),\n",
    "      \"\\nmax\\n\", data_test['sqr_Mean_Hillshade'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, the Box-Cox transformation also requires our data to only contain positive numbers\n",
    "# transform training data with Boxcox\n",
    "data_test['Mean_Hillshade_boxcox'], _ = stats.boxcox(data_test['Mean_Hillshade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[93m'+\"Skew after Boxcox transformation\\n\", data_test['Mean_Hillshade_boxcox'].skew(), \n",
    "      \"\\nmin\\n\", data_test['Mean_Hillshade_boxcox'].min(),\n",
    "      \"\\nmax\\n\", data_test['Mean_Hillshade_boxcox'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.normaltest(data_test['Mean_Hillshade_boxcox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import scipy.stats as stats\n",
    "\n",
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(20,10))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_test['Mean_Hillshade'], 'purple')\n",
    "f.add_subplot(335)\n",
    "histPlot(data_test['log_Mean_Hillshade'], 'green')\n",
    "f.add_subplot(334)\n",
    "histPlot(data_test['Mean_Hillshade_boxcox'], 'gold')                    \n",
    "f.add_subplot(332)\n",
    "histPlot(data_test['sqr_Mean_Hillshade'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution did not improve with Square Root and Logarithms Transformation. Hence we use BoxCox which improved the distribution substantially. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.drop(['log10Mean_Hillshade','log_Mean_Hillshade','sqr_Mean_Hillshade'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.8.2'></a>\n",
    "### <font color=green> 5.2.8.2 Hillshade 9am <font>\n",
    "#### <font color=green> Transformation  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before transformation\\n\", data_test['Hillshade_9am'].skew(), \n",
    "      \"\\nmin\\n\", data_test['Hillshade_9am'].min(),\n",
    "      \"\\nmax\\n\", data_test['Hillshade_9am'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm Transformation <font color=darkcyan>, Square root Transformation<font color=gold> and BoxCox Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['log_Hillshade_9am'] = np.log(data_test['Hillshade_9am']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log transformation\\n\", data_test['log_Hillshade_9am'].skew(), \n",
    "      \"\\nmin\\n\", data_test['log_Hillshade_9am'].min(),\n",
    "      \"\\nmax\\n\", data_test['log_Hillshade_9am'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['sqr_Hillshade_9am'] = data_test['Hillshade_9am']**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+\"Skew after Square Root transformation\\n\", data_test['sqr_Hillshade_9am'].skew(), \n",
    "      \"\\nmin\\n\", data_test['sqr_Hillshade_9am'].min(),\n",
    "      \"\\nmax\\n\", data_test['sqr_Hillshade_9am'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, the Box-Cox transformation also requires our data to only contain positive numbers, transform training data with Boxcox\n",
    "data_test['Hillshade_9am_boxcox'], lam  = stats.boxcox(data_test['Hillshade_9am']+1)\n",
    "#lam is the best lambda for the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[93m'+\"Skew after Boxcox transformation\\n\", data_test['Hillshade_9am_boxcox'].skew(), \n",
    "      \"\\nmin\\n\", data_test['Hillshade_9am_boxcox'].min(),\n",
    "      \"\\nmax\\n\", data_test['Hillshade_9am_boxcox'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(20,10))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_test['Hillshade_9am'], 'purple')\n",
    "f.add_subplot(335)\n",
    "histPlot(data_test['log_Hillshade_9am'], 'green')\n",
    "f.add_subplot(334)\n",
    "histPlot(data_test['Hillshade_9am_boxcox'], 'gold')                    \n",
    "f.add_subplot(332)\n",
    "histPlot(data_test['sqr_Hillshade_9am'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoxCox outperforms the other two for the Hillshade 9am "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.drop(['log_Hillshade_9am','sqr_Hillshade_9am'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.8.3'></a>\n",
    "### <font color=green> 5.2.8.3 Hillshade Noon <font>\n",
    "#### <font color=green> Transformation  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before transformation\\n\", data_test['Hillshade_Noon'].skew(), \n",
    "      \"\\nmin\\n\", data_test['Hillshade_Noon'].min(),\n",
    "      \"\\nmax\\n\", data_test['Hillshade_Noon'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm Transformation <font color=darkcyan>, Square root Transformation<font color=gold> and BoxCox Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['log_Hillshade_Noon'] = np.log(data_test['Hillshade_Noon']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['sqr_Hillshade_Noon'] = data_test['Hillshade_Noon']**0.5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Now, the Box-Cox transformation also requires our data to only contain positive numbers, transform training data with Boxcox\n",
    "data_test['Hillshade_Noon_boxcox'], lam  = stats.boxcox(data_test['Hillshade_Noon'])\n",
    "#lam is the best lambda for the distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box Coc is outperforming the other transformations for Hillshade Noon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.drop(['log_Hillshade_Noon','sqr_Hillshade_Noon'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.8.4'></a>\n",
    "### <font color=green> 5.2.8.4 Hillshade 3pm <font>\n",
    "#### <font color=green> Transformation  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before transformation\\n\", data_test['Hillshade_3pm'].skew(), \n",
    "      \"\\nmin\\n\", data_test['Hillshade_3pm'].min(),\n",
    "      \"\\nmax\\n\", data_test['Hillshade_3pm'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm Transformation <font color=darkcyan>, Square root Transformation<font color=gold> and BoxCox Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['log_Hillshade_3pm'] = np.log(data_test['Hillshade_3pm']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log transformation\\n\", data_test['log_Hillshade_3pm'].skew(), \n",
    "      \"\\nmin\\n\", data_test['log_Hillshade_3pm'].min(),\n",
    "      \"\\nmax\\n\", data_test['log_Hillshade_3pm'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['sqr_Hillshade_3pm'] = data_test['Hillshade_3pm']**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+\"Skew after Square Root transformation\\n\", data_test['sqr_Hillshade_3pm'].skew(), \n",
    "      \"\\nmin\\n\", data_test['sqr_Hillshade_3pm'].min(),\n",
    "      \"\\nmax\\n\", data_test['sqr_Hillshade_3pm'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, the Box-Cox transformation also requires our data to only contain positive numbers, transform training data with Boxcox\n",
    "data_test['Hillshade_3pm_boxcox'], lam  = stats.boxcox(data_test['Hillshade_3pm']+1)\n",
    "#lam is the best lambda for the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[93m'+\"Skew after Boxcox transformation\\n\", data_test['Hillshade_3pm_boxcox'].skew(), \n",
    "      \"\\nmin\\n\", data_test['Hillshade_3pm_boxcox'].min(),\n",
    "      \"\\nmax\\n\", data_test['Hillshade_3pm_boxcox'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(20,10))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_test['Hillshade_3pm'], 'purple')\n",
    "f.add_subplot(335)\n",
    "histPlot(data_test['log_Hillshade_3pm'], 'green')\n",
    "f.add_subplot(334)\n",
    "histPlot(data_test['Hillshade_3pm_boxcox'], 'gold')                    \n",
    "f.add_subplot(332)\n",
    "histPlot(data_test['sqr_Hillshade_3pm'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Hillshade 3pm the data was not highly skwed, we either keep the original or we can use boxcox as it improved the variables as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.drop(['log_Hillshade_3pm','sqr_Hillshade_3pm'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.8.5'></a>\n",
    "### <font color=green> 5.2.8.5 Hillshades  Ratios <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['ratio_Hillshade_3pm'] = data_test['Hillshade_3pm']/255\n",
    "data_test['ratio_Hillshade_Noon'] = data_test['Hillshade_Noon']/255\n",
    "data_test['ratio_Hillshade_9am'] = data_test['Hillshade_9am']/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Hillshade_9am_bin'] = data_test['Hillshade_9am'] > 150\n",
    "data_test['Hillshade_Noon_bin'] = data_test['Hillshade_Noon'] > 150\n",
    "data_test['Hillshade_3pm_bin'] = data_test['Hillshade_3pm'] > 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Hillshade_9am_bin'] = data_test['Hillshade_9am_bin'].astype(int)\n",
    "data_test['Hillshade_Noon_bin'] = data_test['Hillshade_Noon_bin'].astype(int)\n",
    "data_test['Hillshade_3pm_bin'] = data_test['Hillshade_3pm_bin'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Hillshade_allday_bin'] = data_test['Hillshade_9am_bin'] * data_test['Hillshade_Noon_bin'] * data_test['Hillshade_3pm_bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Hillshade_allday_bin'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.8.6'></a>\n",
    "### <font color=green> 5.2.8.6 Aspect <font>\n",
    "#### <font color=green> Transformation  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before transformation\\n\", data_test['Aspect'].skew(), \n",
    "      \"\\nmin\\n\", data_test['Aspect'].min(),\n",
    "      \"\\nmax\\n\", data_test['Aspect'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm Transformation <font color=darkcyan>and Square root Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['sqr_Aspect'] = data_test['Aspect']**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+\"Skew after Square Root transformation\\n\", data_test['sqr_Aspect'].skew(), \n",
    "      \"\\nmin\\n\", data_test['sqr_Aspect'].min(),\n",
    "      \"\\nmax\\n\", data_test['sqr_Aspect'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['log_Aspect'] = np.log(data_test['Aspect']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log transformation\\n\", data_test['log_Aspect'].skew(), \n",
    "      \"\\nmin\\n\", data_test['log_Aspect'].min(),\n",
    "      \"\\nmax\\n\", data_test['log_Aspect'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(20,10))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_test['Aspect'], 'purple')\n",
    "f.add_subplot(332)\n",
    "histPlot(data_test['log_Aspect'], 'green')\n",
    "#f.add_subplot(334)\n",
    "#histPlot(data_test['Hillshade_3pm_boxcox'], 'gold')                    \n",
    "f.add_subplot(333)\n",
    "histPlot(data_test['sqr_Aspect'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For aspect square root turned out to be the best transformation in terms of skeweness. \n",
    "\n",
    "Overall, the best transformations done here are square rt, and boxcox. log transformations did not proved to be benefitial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.drop(['log_Aspect'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In here we are transforming the ratios into a unit scale by dividing by its index. We do so because we think it is much easier to understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['ratio_Hillshade_3pm'] = data_test['Hillshade_3pm']/255\n",
    "data_test['ratio_Hillshade_Noon'] = data_test['Hillshade_Noon']/255\n",
    "data_test['ratio_Hillshade_9am'] = data_test['Hillshade_9am']/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='5.2.8.7'></a>\n",
    "### <font color=green> 5.2.8.7 Aspect in degrees <font>\n",
    "### <font color=green> New Features  <font>\n",
    "The azimuth is the angular direction of the sun, measured from north clockwise in degrees from 0 to 360. An Azimuth of 90 degrees is east.The Cut of values will be between for instance the middle of north and east.\n",
    "    \n",
    "We make a transformation so to get dummies for Aspect ordinal values: north, south, east and west\n",
    "\n",
    "* Aspect_North: from 315 deg to 45 deg\n",
    "* Aspect_East: from 45 deg to 135 deg\n",
    "* Aspect_South: from 135 deg to 225 deg\n",
    "* Aspect_West: from 225 deg to 315 deg    \n",
    "\n",
    "<img src=\"angle_azimuth.png\" width=400 height=200 align=\"center\">\n",
    "    \n",
    "Source:https://www.pveducation.org/pvcdrom/properties-of-sunlight/azimuth-angle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Grouping Aspect in the four directions\n",
    "data_test['Aspect_North']=  np.where(((data_test['Aspect']>=0) & (data_test['Aspect']<45))|((X_test['Aspect']>=315) & (X_test['Aspect']<=360)), 1 ,0)\n",
    "data_test['Aspect_East']= np.where((data_test['Aspect']>=45) & (data_test['Aspect']<135), 1 ,0)\n",
    "data_test['Aspect_South']= np.where((data_test['Aspect']>=135) & (data_test['Aspect']<225), 1 ,0)\n",
    "data_test['Aspect_West']= np.where((data_test['Aspect']>=225) & (data_test['Aspect']<315), 1 ,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='5.2.8.8'></a>\n",
    "### <font color=green> 5.2.8.8 Elevation <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No transformation is done as it is already very symetric distributed \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before transformation\\n\", data_test['Elevation'].skew(), \n",
    "      \"\\nmin\\n\", data_test['Elevation'].min(),\n",
    "      \"\\nmax\\n\", data_test['Elevation'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['binned_elevation'] = [math.floor(v/50.0) for v in data_test['Elevation']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are making more features byt summing and substracing different combinations similar in terms of units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addition and Substraction on the same scale\n",
    "Using for loop was giving us a bad performance hence we are using the features on the same scale which to add or substract "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_test['Road+Fire'] = data_test['Horizontal_Distance_To_Roadways'] + data_test['Horizontal_Distance_To_Fire_Points']\n",
    "data_test['Road-Fire'] = abs(data_test['Horizontal_Distance_To_Roadways'] - data_test['Horizontal_Distance_To_Fire_Points'])\n",
    "data_test['Road+Hydro'] = data_test['Horizontal_Distance_To_Roadways'] + data_test['Horizontal_Distance_To_Hydrology']\n",
    "data_test['Road-Hydro'] = abs(data_test['Horizontal_Distance_To_Roadways'] - data_test['Horizontal_Distance_To_Hydrology'])\n",
    "data_test['Hydro+Fire'] = data_test['Horizontal_Distance_To_Hydrology'] + data_test['Horizontal_Distance_To_Fire_Points']\n",
    "data_test['Hydro-Fire'] = abs(data_test['Horizontal_Distance_To_Hydrology'] - data_test['Horizontal_Distance_To_Fire_Points'])\n",
    "\n",
    "data_test['Road+Fire+Hydro'] = data_test['Horizontal_Distance_To_Roadways']  + data_test['Horizontal_Distance_To_Fire_Points'] + data_test['Horizontal_Distance_To_Hydrology']\n",
    "\n",
    "data_test['Ele+Road+Fire+Hydro'] = data_test['Elevation'] + data_test['Horizontal_Distance_To_Roadways']  + data_test['Horizontal_Distance_To_Fire_Points'] + data_test['Horizontal_Distance_To_Hydrology']\n",
    "\n",
    "data_test['Ele+road'] = data_test['Elevation'] + data_test['Horizontal_Distance_To_Roadways']\n",
    "data_test['Ele-road'] = abs(data_test['Elevation'] - data_test['Horizontal_Distance_To_Roadways'])\n",
    "data_test['Ele+fire'] = data_test['Elevation'] + data_test['Horizontal_Distance_To_Fire_Points']\n",
    "data_test['Ele-fire'] = abs(data_test['Elevation'] - data_test['Horizontal_Distance_To_Fire_Points'])\n",
    "data_test['Ele+hydro'] = data_test['Elevation'] + data_test['Horizontal_Distance_To_Hydrology']\n",
    "data_test['Ele-hydro'] = abs(data_test['Elevation'] - data_test['Horizontal_Distance_To_Hydrology'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['sqr_Road+Fire'] = (data_test['Horizontal_Distance_To_Roadways'] + data_test['Horizontal_Distance_To_Fire_Points'])**0.5\n",
    "data_test['sqr_Road-Fire'] = (abs(data_test['Horizontal_Distance_To_Roadways'] - data_test['Horizontal_Distance_To_Fire_Points']))**0.5\n",
    "data_test['sqr_Road+Hydro'] = (data_test['Horizontal_Distance_To_Roadways'] + data_test['Horizontal_Distance_To_Hydrology'])**0.5\n",
    "data_test['sqr_Road-Hydro'] = (abs(data_test['Horizontal_Distance_To_Roadways'] - data_test['Horizontal_Distance_To_Hydrology']))**0.5\n",
    "data_test['sqr_Hydro+Fire'] = (data_test['Horizontal_Distance_To_Hydrology'] + data_test['Horizontal_Distance_To_Fire_Points'])**0.5\n",
    "data_test['sqr_Hydro-Fire'] = (abs(data_test['Horizontal_Distance_To_Hydrology'] - data_test['Horizontal_Distance_To_Fire_Points']))**0.5\n",
    "\n",
    "data_test['sqr_Road+Fire+Hydro'] = (data_test['Horizontal_Distance_To_Roadways']  + data_test['Horizontal_Distance_To_Fire_Points'] + data_test['Horizontal_Distance_To_Hydrology'])**0.5\n",
    "\n",
    "data_test['sqr_Ele+Road+Fire+Hydro'] = (data_test['Elevation'] + data_test['Horizontal_Distance_To_Roadways']  + data_test['Horizontal_Distance_To_Fire_Points'] + data_test['Horizontal_Distance_To_Hydrology'])**0.5\n",
    "\n",
    "data_test['sqr_Ele+road'] = (data_test['Elevation'] + data_test['Horizontal_Distance_To_Roadways'])**0.5\n",
    "data_test['sqr_Ele-road'] = (abs(data_test['Elevation'] - data_test['Horizontal_Distance_To_Roadways']))**0.5\n",
    "data_test['sqr_Ele+fire'] = (data_test['Elevation'] + data_test['Horizontal_Distance_To_Fire_Points'])**0.5\n",
    "data_test['sqr_Ele-fire'] = (abs(data_test['Elevation'] - data_test['Horizontal_Distance_To_Fire_Points']))**0.5\n",
    "data_test['sqr_Ele+hydro'] = (data_test['Elevation'] + data_test['Horizontal_Distance_To_Hydrology'])**0.5\n",
    "data_test['sqr_Ele-hydro'] = (abs(data_test['Elevation'] - data_test['Horizontal_Distance_To_Hydrology']))**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.2.9 Geoclimate grouping  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> 5.2.9.1 Climatic feature engineering to group soils  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Kaggle competition, there is a reference to John A. Blackard which happened to be one geologist working for the forest federal US agency. In a co-authored paper, he gives further insights on the soil families with a list of codes. These are digits categorizing the soils according to climate and geology. We decide to take this valuable insight and engineer features around this dynamic so to cut down the number of soils\n",
    "\n",
    "From original database donated by John A. Blackard\n",
    "\n",
    "Code Designations:\n",
    "\n",
    "Wilderness Areas:  \t<br>\n",
    "\n",
    "1 - Rawah Wilderness Area <br>\n",
    "2 - Neota Wilderness Area  <br>\n",
    "3 - Comanche Peak Wilderness Area<br>\n",
    "4 - Cache la Poudre Wilderness Area<br>\n",
    "\n",
    "Soil Types:             1 to 40 : based on the USFS Ecological\n",
    "                        Landtype Units (ELUs) for this study area:<br>\n",
    "\n",
    "  Study Code USFS ELU Code\t\t\tDescription<br>\n",
    "\t 1\t   2702\t\tCathedral family - Rock outcrop complex, extremely stony.<br>\n",
    "\t 2\t   2703\t\tVanet - Ratake families complex, very stony.<br>\n",
    "\t 3\t   2704\t\tHaploborolis - Rock outcrop complex, rubbly.<br>\n",
    "\t 4\t   2705\t\tRatake family - Rock outcrop complex, rubbly.<br>\n",
    "\t 5\t   2706\t\tVanet family - Rock outcrop complex complex, rubbly.<br>\n",
    "\t 6\t   2717\t\tVanet - Wetmore families - Rock outcrop complex, stony.<br>\n",
    "\t 7\t   3501\t\tGothic family.<br>\n",
    "\t 8\t   3502\t\tSupervisor - Limber families complex.<br>\n",
    "\t 9\t   4201\t\tTroutville family, very stony.<br>\n",
    "\t10\t   4703\t\tBullwark - Catamount families - Rock outcrop complex, rubbly.<br>\n",
    "\t11\t   4704\t\tBullwark - Catamount families - Rock land complex, rubbly.<br>\n",
    "\t12\t   4744\t\tLegault family - Rock land complex, stony.<br>\n",
    "\t13\t   4758\t\tCatamount family - Rock land - Bullwark family complex, rubbly.<br>\n",
    "\t14\t   5101\t\tPachic Argiborolis - Aquolis complex.<br>\n",
    "\t15\t   5151\t\tunspecified in the USFS Soil and ELU Survey.<br>\n",
    "\t16\t   6101\t\tCryaquolis - Cryoborolis complex.<br>\n",
    "\t17\t   6102\t\tGateview family - Cryaquolis complex.<br>\n",
    "\t18\t   6731\t\tRogert family, very stony.<br>\n",
    "\t19\t   7101\t\tTypic Cryaquolis - Borohemists complex.<br>\n",
    "\t20\t   7102\t\tTypic Cryaquepts - Typic Cryaquolls complex.<br>\n",
    "\t21\t   7103\t\tTypic Cryaquolls - Leighcan family, till substratum complex.<br>\n",
    "\t22\t   7201\t\tLeighcan family, till substratum, extremely bouldery.<br>\n",
    "\t23\t   7202\t\tLeighcan family, till substratum - Typic Cryaquolls complex.<br>\n",
    "\t24\t   7700\t\tLeighcan family, extremely stony.<br>\n",
    "\t25\t   7701\t\tLeighcan family, warm, extremely stony.<br>\n",
    "\t26\t   7702\t\tGranile - Catamount families complex, very stony.<br>\n",
    "\t27\t   7709\t\tLeighcan family, warm - Rock outcrop complex, extremely stony.<br>\n",
    "\t28\t   7710\t\tLeighcan family - Rock outcrop complex, extremely stony.<br>\n",
    "\t29\t   7745\t\tComo - Legault families complex, extremely stony.<br>\n",
    "\t30\t   7746\t\tComo family - Rock land - Legault family complex, extremely stony.<br>\n",
    "\t31\t   7755\t\tLeighcan - Catamount families complex, extremely stony.<br>\n",
    "\t32\t   7756\t\tCatamount family - Rock outcrop - Leighcan family complex, extremely stony.<br>\n",
    "\t33\t   7757\t\tLeighcan - Catamount families - Rock outcrop complex, extremely stony.<br>\n",
    "\t34\t   7790\t\tCryorthents - Rock land complex, extremely stony.<br>\n",
    "\t35\t   8703\t\tCryumbrepts - Rock outcrop - Cryaquepts complex.<br>\n",
    "\t36\t   8707\t\tBross family - Rock land - Cryumbrepts complex, extremely stony.<br>\n",
    "\t37\t   8708\t\tRock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.<br>\n",
    "\t38\t   8771\t\tLeighcan - Moran families - Cryaquolls complex, extremely stony.<br>\n",
    "\t39\t   8772\t\tMoran family - Cryorthents - Leighcan family complex, extremely <br>stony.\n",
    "\t40\t   8776\t\tMoran family - Cryorthents - Rock land complex, extremely stony.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Note:   First digit:  climatic zone       Second digit:  geologic zones\n",
    "                1.  lower montane dry             1.  alluvium\n",
    "                2.  lower montane                 2.  glacial\n",
    "                3.  montane dry                   3.  shale\n",
    "                4.  montane                       4.  sandstone\n",
    "                5.  montane dry and montane       5.  mixed sedimentary\n",
    "                6.  montane and subalpine         6.unspecified in the USFS ELU Survey\n",
    "                7.  subalpine                     7.  igneous and metamorphic\n",
    "                8.  alpine                        8.  volcanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The USFD, an American federal agency for forest service dependent on the department of agriculture has classified soil types according to __climatic zone (first digit)__ and __geology (second digit)__. Because of this, we believe a similar classification can be artificially engineered grouping all similar soils in 7 categories for climate (there is no lower montane dry soils) and 4 for geology (we do not take into consideration shale, sandstone, volcanic or unspecified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> 5.2.9.2 Climatic Zone feature engineering to group soils  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"Lower_Montane_Climate\"] = data_test.loc[:,data_test.columns.str.contains(\"^Soil_Type[23456]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Montane_Dry_Climate'] =data_test.loc[:,data_test.columns.str.contains(\"^Soil_Type[78]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Montane_Climate'] =data_test.loc[:,data_test.columns.str.contains(\"^Soil_Type[1][0123]$|Soil_Type[9]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Montane_Dry_and_Montane_Climate'] =data_test.loc[:,data_test.columns.str.contains(\"^Soil_Type[1][45]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Montante_and_Subalpine_Climate'] =data_test.loc[:,data_test.columns.str.contains(\"^Soil_Type[1][678]$\")].max(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Subalpine_Climate'] =data_test.loc[:,data_test.columns.str.contains(\"^Soil_Type19$|^Soil_Type[2][0-9]$|^Soil_Type[3][0-4]$\")].max(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Alpine_Climate'] =data_test.loc[:,data_test.columns.str.contains(\"^Soil_Type[3][56789]$|Soil_Type40\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> 5.2.9.2 Geological feature engineering to group soils  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Note:   First digit:  climatic zone             Second digit:  geologic zones\n",
    "                1.  lower montane dry                   1.  alluvium\n",
    "                2.  lower montane                       2.  glacial\n",
    "                3.  montane dry                         3.  shale\n",
    "                4.  montane                             4.  sandstone\n",
    "                5.  montane dry and montane             5.  mixed sedimentary\n",
    "                6.  montane and subalpine               6.  unspecified in the USFS ELU Survey\n",
    "                7.  subalpine                           7.  igneous and metamorphic\n",
    "                8.  alpine                              8.  volcanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Alluvium_Soil'] = data_test.loc[:,data_test.columns.str.contains(\"^Soil_Type[1][45679]$|^Soil_Type[2][01]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Glacial_Soil'] =data_test.loc[:,data_test.columns.str.contains(\"^Soil_Type[9]$|^Soil_Type[2][23]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Mixed_Sedimentary_Soil'] =data_test.loc[:,data_test.columns.str.contains(\"^Soil_Type[7-8]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Igneus_and_Metamorphic_Soil'] =data_test.loc[:,data_test.columns.str.contains(\"^Soil_Type[1-6]$|^Soil_Type[1][01238]$|^Soil_Type[3-4]\\d$|^Soil_Type[2][4-9]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the medium paper \"Preprocessing: Why you should generate polynominal features first before standardizing\" mention it is not good practice to standardize the variablesbefore before PolynominalFeatures. This should be done after to not loss the signal of the variables.  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Identify and drop our target variable 'Cover_Type' from dataframe, isolating our independent variables\n",
    "X = data_test.drop('Cover_Type', axis = 1)\n",
    "\n",
    "# Isolate our dependent variable as a feature\n",
    "y = data_test['Cover_Type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Factorization\n",
    "\n",
    "The numerical values present a level of detail that may be much more fine-grained than we need. For instance, the soil level can be represented by different categories (soil family, complex or stony/rubberly). We aggregate the data up which can help to avoid overfitting when the data is more aggregate.\n",
    "\n",
    "__We played with these features but reached a final conclusion of not to considering any more families and complex type. But Stonyness,climate and geogology. For purposes of the assignment we show the code as a RAWNBCONVERT__\n",
    "\n",
    "The only family grouping we do on soils are the ones above, on geological and climate grounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.6 Soil Type Family  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Discretization to bin the soil variable to the family type.<br>\n",
    "\n",
    "__Cathedral__ <br>\n",
    "1 Cathedral family - Rock outcrop complex, extremely stony.<br>\n",
    "\n",
    "__Ratake__ <br>\n",
    "2 Vanet - Ratake families complex, very stony.<br>\n",
    "4 Ratake family - Rock outcrop complex, rubbly.<br>\n",
    "\n",
    "__Vanet__<br>\n",
    "5 Vanet family - Rock outcrop complex complex, rubbly.<br>\n",
    "\n",
    "__Wetmore__<br>\n",
    "6 Vanet - Wetmore families - Rock outcrop complex, stony.<br>\n",
    "\n",
    "__Gothic__<br>\n",
    "7 Gothic family.<br>\n",
    "                    \n",
    "__Limber__ <br>\n",
    "8 Supervisor - Limber families complex. <br>\n",
    "\n",
    "__Troutville__<br>\n",
    "9 Troutville family, very stony.<br>\n",
    "\n",
    "__Legault__<br>\n",
    "12 Legault family - Rock land complex, stony.<br>\n",
    "29 Como - Legault families complex, extremely stony.<br>\n",
    "\n",
    "__Gateview__ <br>\n",
    "17 Gateview family - Cryaquolis complex.<br>\n",
    "\n",
    "__Rogert__<br>\n",
    "18 Rogert family, very stony.<br>\n",
    "\n",
    "\n",
    "__Como__<br>\n",
    "30 Como family - Rock land - Legault family complex, extremely stony.<br>\n",
    "\n",
    "__Bross__<br>\n",
    "36 Bross family - Rock land - Cryumbrepts complex, extremely stony.<br>\n",
    "\n",
    "\n",
    "\n",
    "__Catamount__<br>\n",
    "10 Bullwark - Catamount families - Rock outcrop complex, rubbly.<br>\n",
    "11 Bullwark - Catamount families - Rock land complex, rubbly.<br>\n",
    "13 Catamount family - Rock land - Bullwark family complex, rubbly.<br>\n",
    "26 Granile - Catamount families complex, very stony.<br>\n",
    "32 Catamount family - Rock outcrop - Leighcan family complex, extremely stony.<br>\n",
    "31 Leighcan - Catamount families complex, extremely stony.<br>\n",
    "33 Leighcan - Catamount families - Rock outcrop complex, extremely stony.<br>\n",
    "\n",
    "__Leighcan__<br>\n",
    "21 Typic Cryaquolls - Leighcan family, till substratum complex.<br>\n",
    "22 Leighcan family, till substratum, extremely bouldery.<br>\n",
    "23 Leighcan family, till substratum - Typic Cryaquolls complex.<br>\n",
    "24 Leighcan family, extremely stony.<br>\n",
    "25 Leighcan family, warm, extremely stony.<br>\n",
    "27 Leighcan family, warm - Rock outcrop complex, extremely stony.<br>\n",
    "28 Leighcan family - Rock outcrop complex, extremely stony.<br>\n",
    "\n",
    "__Moran__<br>\n",
    "38 Leighcan - Moran families - Cryaquolls complex, extremely stony.<br>\n",
    "39 Moran family - Cryorthents - Leighcan family complex, extremely stony.<br>\n",
    "40 Moran family - Cryorthents - Rock land complex, extremely stony.<br>\n",
    "\n",
    "__Others__<br> \n",
    "3 Haploborolis - Rock outcrop complex, rubbly.<br>\n",
    "15 unspecified in the USFS Soil and ELU Survey.<br>\n",
    "37 Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.<br>\n",
    "34 Cryorthents - Rock land complex, extremely stony.<br>\n",
    "35 Cryumbrepts - Rock outcrop - Cryaquepts complex.<br>\n",
    "20 Typic Cryaquepts - Typic Cryaquolls complex.<br>\n",
    "14 Pachic Argiborolis - Aquolis complex.<br>\n",
    "16 Cryaquolis - Cryoborolis complex.<br>\n",
    "19 Typic Cryaquolis - Borohemists complex.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOT USED__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Soil Type\n",
    "family_soil_types = {\n",
    "    'Family_Cathedral': ['Soil_Type1'],\n",
    "    'Family_Retake': ['Soil_Type2', 'Soil_Type4'],\n",
    "    'Family_Vanet': ['Soil_Type5'],\n",
    "    'Family_Wetmore': ['Soil_Type6'],\n",
    "    'Family_Gothic': ['Soil_Type7'],\n",
    "    'Family_Limber': ['Soil_Type8'],\n",
    "    'Family_Troutville_': ['Soil_Type9'],\n",
    "    'Family_Legault': ['Soil_Type12', 'Soil_Type29'],\n",
    "    'Family_Gateview': ['Soil_Type17'],\n",
    "    'Family_Rogert': ['Soil_Type18'],\n",
    "    'Family_Como': ['Soil_Type30'],\n",
    "    'Family_Bross': ['Soil_Type36'],\n",
    "    'Family_Catamount': ['Soil_Type10','Soil_Type11','Soil_Type13','Soil_Type26','Soil_Type32','Soil_Type31','Soil_Type33'],\n",
    "    'Family_Leighcan': ['Soil_Type21','Soil_Type22','Soil_Type23','Soil_Type24','Soil_Type25','Soil_Type27','Soil_Type28'],\n",
    "    'Family_Moran': ['Soil_Type38','Soil_Type39','Soil_Type40'],\n",
    "    'Family_Others': ['Soil_Type3','Soil_Type15','Soil_Type37','Soil_Type34','Soil_Type35','Soil_Type20','Soil_Type14','Soil_Type16','Soil_Type19'],\n",
    "} \n",
    "\n",
    "for family in family_soil_types:\n",
    "    data_test[family] = 0\n",
    "    soil_types = family_soil_types[family]\n",
    "    for soil_type in soil_types:\n",
    "        data_test[family] += data_test[soil_type]\n",
    "\n",
    "data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will group the soil types according to their family and according to the complex and stonyness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOT USED__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Complex Type\n",
    "family_complex_types = {\n",
    "    'Rock_outcrop_complex': ['Soil_Type1','Soil_Type2','Soil_Type3','Soil_Type4','Soil_Type5','Soil_Type6','Soil_Type10','Soil_Type27','Soil_Type28','Soil_Type33'],\n",
    "    'Ratake_families_complex': ['Soil_Type2'],\n",
    "    'Limber_families_complex': ['Soil_Type8'],\n",
    "    'Rock_land_complex': ['Soil_Type11','Soil_Type12','Soil_Type34','Soil_Type40'],\n",
    "    'Cryoborolis_complex': ['Soil_Type16','Soil_Type17'],\n",
    "    'Bullwark_family_complex': ['Soil_Type13'],\n",
    "    'Aquolis_complex_': ['Soil_Type14'],\n",
    "    'Borohemists_complex': ['Soil_Type19'],\n",
    "    'Cryaquolls_complex': ['Soil_Type20','Soil_Type23','Soil_Type38'],\n",
    "    'Till_substratum_complex': ['Soil_Type21'],\n",
    "    'Catamount_families_complex': ['Soil_Type26','Soil_Type1','Soil_Type31'],\n",
    "    'Legault_families_complex': ['Soil_Type39','Soil_Type30'],\n",
    "    'Leighcan_family_complex': ['Soil_Type32','Soil_Type39'],\n",
    "    'Cryaquepts_complex': ['Soil_Type35'],\n",
    "    'Cryumbrepts_complex': ['Soil_Type36'],\n",
    "    'Cryorthents_complex': ['Soil_Type37'],\n",
    "    'others_complex': ['Soil_Type7','Soil_Type9','Soil_Type22','Soil_Type24','Soil_Type25','Soil_Type18','Soil_Type15'],\n",
    "} \n",
    "\n",
    "for family in family_complex_types:\n",
    "    data_test[family] = 0\n",
    "    complex_types = family_complex_types[family]\n",
    "    for complex_type in complex_types:\n",
    "        data_test[family] += data_test[complex_type]\n",
    "\n",
    "data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This has been included__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Soil type is a single variable which has been one-hot encoded presumably , so we will reverse engineer the soil type. We will eventually drop the original soil type columns which has the added effect of significantly reducing the total number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original soil features\n",
    "soil_features = [f'Soil_Type{i}' for i in range(1,41)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original soil features\n",
    "data_test.drop(columns = soil_features, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test if elevation makes a difference to take out with the new interaction model improves\n",
    "data_test = data_test.drop(['Elevation^2'], axis = 1)\n",
    "data_test = data_test.drop(['Elevation'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the original scaled variables did not improve nor worsen the model. Since it does not change much the score, we remove it as we have it double in the model with the scaled features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locking all features in a csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the shake of efficiency, we create a csv file to reuse also later on in part III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_conformed = data_test[['Id','Wilderness_Area4',\n",
    " 'Subalpine_Climate',\n",
    " 'Alpine_Climate',\n",
    " 'sqr_Road-Hydro',\n",
    " 'sqr_Road-Fire',\n",
    " 'sqr_Road+Fire',\n",
    " 'binned_elevation',\n",
    " 'Wilderness_Area3',\n",
    " 'Wilderness_Area1',\n",
    " 'Montane_Climate',\n",
    " 'Lower_Montane_Climate',\n",
    " 'Horizontal_Distance_To_Roadways_x_Horizontal_Distance_To_Fire_Points',\n",
    " 'sqr_Road+Fire+Hydro',\n",
    " 'sqr_Ele-hydro',\n",
    " 'sqr_Ele+road',\n",
    " 'sqr_Ele+hydro',\n",
    " 'sqr_Ele+fire',\n",
    " 'sqr_Ele+Road+Fire+Hydro',\n",
    " 'Horizontal_Distance_To_Roadways^2',\n",
    " 'Hillshade_9am_x_Hillshade_Noon',\n",
    " 'Elevation_x_Hillshade_Noon',\n",
    " 'Elevation_x_Hillshade_9am',\n",
    " 'sqr_Road+Hydro',\n",
    " 'sqr_Ele-road',\n",
    " 'sqr_Ele-fire',\n",
    " 'sqr_Distance_To_Hydrology',\n",
    " 'Sqr_Horizontal_Distance_To_Roadways',\n",
    " 'Igneus_and_Metamorphic_Soil',\n",
    " 'Horizontal_Distance_To_Roadways_x_Hillshade_Noon',\n",
    " 'Horizontal_Distance_To_Roadways_x_Hillshade_9am',\n",
    " 'Horizontal_Distance_To_Hydrology_x_Hillshade_3pm',\n",
    " 'Hillshade_Noon_x_Horizontal_Distance_To_Fire_Points',\n",
    " 'Hillshade_9am_boxcox',\n",
    " 'Elevation_x_Horizontal_Distance_To_Roadways',\n",
    " 'Elevation_x_Horizontal_Distance_To_Hydrology']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code below have all features that we did in the feature engineering part and transport this to csv before we actually use the feature selection methods and select only few variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only X_Train replacement\n",
    "data_test_conformed.to_csv('selected_features_test.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
