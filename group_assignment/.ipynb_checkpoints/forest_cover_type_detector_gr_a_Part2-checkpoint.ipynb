{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ml2_group_assignment.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=green> Introduction </font>\n",
    "\n",
    "The assignment is focused on solving the Forest Cover Type Prediction: https://www.kaggle.com/c/forest-cover-type-prediction/overview. This task proposes a classification problem: predict the forest cover type (the predominant kind of tree cover) from strictly cartographic variables (as opposed to remotely sensed data).\n",
    "\n",
    "The study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. Each observation is a 30m x 30m patch. You are asked to predict an integer classification for the forest cover type. The seven types are:\n",
    "\n",
    "1. Spruce/Fir\n",
    "2. Lodgepole Pine\n",
    "3. Ponderosa Pine\n",
    "4. Cottonwood/Willow\n",
    "5. Aspen\n",
    "6. Douglas-fir\n",
    "7. Krummholz\n",
    "\n",
    "The training set (15120 observations) contains both features and the Cover_Type. The test set contains only the features. \n",
    "\n",
    "**You must predict the Cover_Type for every row in the test set (565892 observations).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tree_types.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=green> Table of contents </font>\n",
    "\n",
    "* Data Analysis\n",
    "* Exploratory Data Analysis\n",
    "* Feature Engineering & Selection\n",
    "* Compare Several Machine Learning Models\n",
    "* Perform Hyperparameter Tuning on the Best Model\n",
    "* Interpret Model Results\n",
    "* Evaluate the Best Model with Test Data (replying the initiating question)\n",
    "* Summary & Conclusions\n",
    "\n",
    "# Sections \n",
    "* [Libaries used](#0)\n",
    "* [1. Import Data](#1)\n",
    "* [2. Data analysis](#2)  \n",
    "  * [2.1.Explanation of variables](#2.1)\n",
    "      * [1.2.1 XX](#2.1.1)\n",
    "* [3. Exploratory Data Analysis](#3)\n",
    "  * [3.1 Analysis of the Dataset using EDA](#3.1)\n",
    "  * [3.2 D'Agostino and Pearson's Test](#3.2)  \n",
    "  * [3.3 Checking Variable Completeness ](#3.3)\n",
    "  * [3.4 Correlation Matrix ](#3.4)  \n",
    "  * [3.5 Paired density, scatterplot matrix and 3D Graphics ](#3.5)   \n",
    "  * [3.6 Categorial EDA ](#3.6) \n",
    "      * [3.6.1 Categorial Bar Diagrams](#3.6.1)  \n",
    "      * [3.6.2.Violinplot with Dependent Variable](#3.6.2)  \n",
    "      * [3.6.3.Treemap for categorial Data](#3.6.3) \n",
    "* [4. Baseline Model](#4)\n",
    "  * [4.0 Prepare Data and Standardization](#4.0)\n",
    "  * [4.1 Random Forest](#4.1) \n",
    "  * [4.2 Gradient Boosting](#4.2)  \n",
    "  * [4.3 Decision Trees](#4.3)\n",
    "  * [4.4 K-Nearest Neighbors (KNN)](#4.4)  \n",
    "  * [4.5 Logistic Regression](#4.5) \n",
    "  * [4.6 Naive Bayes](#4.6) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"roosevelt-national-forest.jpeg\" width=1200 height=800 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "# <font color=green> Libraries used </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install squarify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns  # Graphing\n",
    "import matplotlib.pyplot as plt\n",
    "import squarify #treemap\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from yellowbrick.classifier import ROCAUC\n",
    "import warnings\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "##  <font color=green>0.Import the Data </font>\n",
    "Let’s load the training data and create data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"train.csv\")\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep the original dataset for later comparisons and make a copy for the FE process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = data_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(\"test.csv\")\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.1'></a>\n",
    "## <font color=green>  5.Feature Engineering<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.1'></a>\n",
    "### <font color=darkcyan> 5.1 Check for Anomalies and Outliers <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the z-score is sensitive to the  mean and standard deviation and its assumption of a normally distributed variable, we cannot use the z-score for outlier handling because of the skewed data. The disadvantage using percentile it considers always and outlier of the lowest or highest value, even there are no outliers. As the number of observations increases, so does the number of observations considered outliers; After all, using a percentile based method will always flat-out reject a certain percentage of our observations.Thus, we need to use the percentile with care. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.1.1'></a>\n",
    "### <font color=darkcyan> 5.1.1 Outlier Detection Treatment using Inter-Quartile Range rule Function <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IQR is the difference between the 75th and 25th percentile. The IQR is more resistant to outliers. The IQR by definition only covers the middle 50% of the data, so outliers are well outside this range and the presence of a small number of outliers is not likely to change this significantly. If you add an outlier, the IQR will change to another set of data points that are probably not that dissimilar to the previous ones (in most datasets), hence it is “resistant” to change. This is especially the case of a large dataset.\n",
    "\n",
    "Now we are testing different ranges for IQR, namely 2,3 and 4 to check for more extreme outlier values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_function(df, col_name,value_IQR):\n",
    "    ''' this function detects first and third quartile and interquartile range for a given column of a dataframe\n",
    "    then calculates upper and lower limits to determine outliers conservatively\n",
    "    returns the number of lower and uper limit and number of outliers respectively\n",
    "    '''\n",
    "    first_quartile = np.percentile(np.array(df[col_name].tolist()), 25)\n",
    "    third_quartile = np.percentile(np.array(df[col_name].tolist()), 75)\n",
    "    IQR = third_quartile - first_quartile\n",
    "                      \n",
    "    upper_limit = third_quartile+(value_IQR*IQR)\n",
    "    lower_limit = first_quartile-(value_IQR*IQR)\n",
    "    outlier_count = 0\n",
    "                      \n",
    "    for value in df[col_name].tolist():\n",
    "        if (value < lower_limit) | (value > upper_limit):\n",
    "            outlier_count +=1\n",
    "    return lower_limit, upper_limit, outlier_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.1.2'></a>\n",
    "### <font color=darkcyan> 5.1.2 Inter-Quartile Range rule: 4 IQR from Median <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all columns to see if there are any outliers, for all values which are not only 0 and 1\n",
    "for column in [\"Horizontal_Distance_To_Hydrology\",\"Vertical_Distance_To_Hydrology\",\"Horizontal_Distance_To_Roadways\",\"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\",\"Horizontal_Distance_To_Fire_Points\"]:\n",
    "    if outlier_function(data_train, column,4)[2] > 0:\n",
    "        print(\"There are {} outliers in {}\".format(outlier_function(data_train, column,4)[2], column))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 1 record of Hillshade_9am with a zero value, which is a valid value as Hillshade can be zero. This is because there are parts in the mountain that never see the sunlight (blind spots). Hence we keep the value as it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing outliers and testing baseline model\n",
    "# calculate interquartile range\n",
    "from numpy import percentile\n",
    "q25, q75 = percentile(data_train['Vertical_Distance_To_Hydrology'], 25), percentile(data_train['Vertical_Distance_To_Hydrology'], 75)\n",
    "iqr = q75 - q25\n",
    "# calculate the outlier cutoff\n",
    "cut_off = iqr * 4\n",
    "lower, upper = q25 - cut_off, q75 + cut_off\n",
    "# remove outliers\n",
    "data_train_vd_h = data_train[(data_train['Vertical_Distance_To_Hydrology'] > lower) & (data_train['Vertical_Distance_To_Hydrology'] < upper)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the model improves after removing vertical distance to hydrology "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4=data_train_vd_h.drop(labels=['Id','Cover_Type'],axis=1)\n",
    "y4=data_train_vd_h['Cover_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scale_numerical =['Elevation','Aspect','Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology',\n",
    "            'Horizontal_Distance_To_Roadways','Hillshade_9am','Hillshade_Noon','Hillshade_3pm',\n",
    "            'Horizontal_Distance_To_Fire_Points']\n",
    "scaler = StandardScaler()\n",
    "X4[scale_numerical]=scaler.fit_transform(X4[scale_numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y4.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4_train,X4_val,y4_train,y4_val = train_test_split(X4,y4,random_state=37) #seed is 18!Cannot use stratify because the datset is unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=20)\n",
    "model_forest = forest.fit(X4_train,y4_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the previous score with the new score after removing the outliers of vertical distance to Hydrology: previous __forest.score: 0.83968__\n",
    "\n",
    "It improves slightly, hence removing these outliers turns to be the selected approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating accuracy_score\n",
    "forest.score(X4_val,y4_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=darkcyan> Replacing with Median <font>\n",
    "Since removing outliers improved performance of our model, using median values to keep a balanced sample set seems to be a reasonable approach. Otherwise the data becomes unbalanced, for which, other tools have to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med = np.median(data_train['Vertical_Distance_To_Hydrology'])\n",
    "for i in data_train['Vertical_Distance_To_Hydrology']:\n",
    "    if i > upper or i < lower:\n",
    "            data_train['Vertical_Distance_To_Hydrology_n'] = data_train['Vertical_Distance_To_Hydrology'].replace(i, med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xmed=data_train.drop(labels=['Id','Cover_Type','Vertical_Distance_To_Hydrology'],axis=1)\n",
    "ymed=data_train['Cover_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scale_numerical =['Elevation','Aspect','Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology_n',\n",
    "            'Horizontal_Distance_To_Roadways','Hillshade_9am','Hillshade_Noon','Hillshade_3pm',\n",
    "            'Horizontal_Distance_To_Fire_Points']\n",
    "scaler = StandardScaler()\n",
    "Xmed[scale_numerical]=scaler.fit_transform(Xmed[scale_numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymed.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xmed_train,Xmed_val,ymed_train,ymed_val = train_test_split(X4,y4,random_state=37) #seed is 18!Cannot use stratify because the datset is unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=20)\n",
    "model_forest = forest.fit(Xmed_train,ymed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating accuracy_score\n",
    "forest.score(Xmed_val,ymed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, model improves slightly, hence proving right the hypothesis of imputing with median values so to obtain a more balanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.1.3'></a>\n",
    "### <font color=darkcyan> 5.1.2 Inter-Quartile Range rule: 3 IQR from Median <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all columns to see if there are any outliers, for all values which are not only 0\n",
    "# and 1\n",
    "for column in [\"Horizontal_Distance_To_Hydrology\",\"Vertical_Distance_To_Hydrology\",\"Horizontal_Distance_To_Roadways\",\"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\",\"Horizontal_Distance_To_Fire_Points\"]:\n",
    "    if outlier_function(data_train, column,3)[2] > 0:\n",
    "        print(\"There are {} outliers in {}\".format(outlier_function(data_train, column,3)[2], column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing outliers and testing baseline model\n",
    "# calculate interquartile range\n",
    "from numpy import percentile\n",
    "q25, q75 = percentile(data_train['Horizontal_Distance_To_Hydrology'], 25), percentile(data_train['Horizontal_Distance_To_Hydrology'], 75)\n",
    "iqr = q75 - q25\n",
    "# calculate the outlier cutoff\n",
    "cut_off = iqr * 3\n",
    "lower, upper = q25 - cut_off, q75 + cut_off\n",
    "# remove outliers\n",
    "data_train_hd_h3 = data_train[(data_train['Horizontal_Distance_To_Hydrology'] > lower) & (data_train['Horizontal_Distance_To_Hydrology'] < upper)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3_hd=data_train_hd_h3.drop(labels=['Cover_Type'],axis=1)\n",
    "y3_hd=data_train_hd_h3['Cover_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scale_numerical =['Elevation','Aspect','Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology',\n",
    "            'Horizontal_Distance_To_Roadways','Hillshade_9am','Hillshade_Noon','Hillshade_3pm',\n",
    "            'Horizontal_Distance_To_Fire_Points']\n",
    "scaler = StandardScaler()\n",
    "X3_hd[scale_numerical]=scaler.fit_transform(X3_hd[scale_numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_hd,X_val_hd,y_train_hd,y_val_hd = train_test_split (X3_hd,y3_hd,random_state=37) #seed is 18!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=20)\n",
    "model_forest = forest.fit(X_train_hd,y_train_hd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.score(X_val_hd,y_val_hd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### <font color=darkcyan> Vertical Distance To Hydrology <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing outliers and testing baseline model\n",
    "# calculate interquartile range\n",
    "from numpy import percentile\n",
    "q25, q75 = percentile(data_train['Vertical_Distance_To_Hydrology'], 25), percentile(data_train['Vertical_Distance_To_Hydrology'], 75)\n",
    "iqr = q75 - q25\n",
    "# calculate the outlier cutoff\n",
    "cut_off = iqr * 3\n",
    "lower, upper = q25 - cut_off, q75 + cut_off\n",
    "# remove outliers\n",
    "data_train_vd_h3 = data_train[(data_train['Vertical_Distance_To_Hydrology'] > lower) & (data_train['Vertical_Distance_To_Hydrology'] < upper)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3=data_train_vd_h3.drop(labels=['Cover_Type'],axis=1)\n",
    "y3=data_train_vd_h3['Cover_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scale_numerical =['Elevation','Aspect','Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology',\n",
    "            'Horizontal_Distance_To_Roadways','Hillshade_9am','Hillshade_Noon','Hillshade_3pm',\n",
    "            'Horizontal_Distance_To_Fire_Points']\n",
    "scaler = StandardScaler()\n",
    "X3[scale_numerical]=scaler.fit_transform(X3[scale_numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split (X3,y3,random_state=37) #seed is 18!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=20)\n",
    "model_forest = forest.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The improvement from previous removal of outliers of Vertical Distance to Hydrology is not so incremental anymore in addition we would remove several points in the dataset, we will disregard this option of the 49 datapoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating accuracy_score\n",
    "forest.score(X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=darkcyan> Horizontal Distance To Roadways <font>\n",
    "Taking out the outliers of roadways does not improve the model it actually gets worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing outliers and testing baseline model\n",
    "# calculate interquartile range\n",
    "from numpy import percentile\n",
    "q25, q75 = percentile(data_train['Horizontal_Distance_To_Roadways'], 25), percentile(data_train['Horizontal_Distance_To_Roadways'], 75)\n",
    "iqr = q75 - q25\n",
    "# calculate the outlier cutoff\n",
    "cut_off = iqr * 3\n",
    "lower, upper = q25 - cut_off, q75 + cut_off\n",
    "# remove outliers\n",
    "data_train_rw = data_train[(data_train['Horizontal_Distance_To_Roadways'] > lower) & (data_train['Horizontal_Distance_To_Roadways'] < upper)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3_rw=data_train_rw.drop(labels=['Cover_Type'],axis=1)\n",
    "y3_rw=data_train_rw['Cover_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scale_numerical =['Elevation','Aspect','Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology',\n",
    "            'Horizontal_Distance_To_Roadways','Hillshade_9am','Hillshade_Noon','Hillshade_3pm',\n",
    "            'Horizontal_Distance_To_Fire_Points']\n",
    "scaler = StandardScaler()\n",
    "X3_rw[scale_numerical]=scaler.fit_transform(X3_rw[scale_numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split (X3_rw,y3_rw,random_state=37) #seed is 18!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=20)\n",
    "model_forest = forest.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating accuracy_score\n",
    "forest.score(X_val,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=darkcyan> Horizontal Distance To Fire Points <font>\n",
    "Taking out the outliers it does not improve the model either "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing outliers and testing baseline model\n",
    "# calculate interquartile range\n",
    "from numpy import percentile\n",
    "q25, q75 = percentile(data_train['Horizontal_Distance_To_Fire_Points'], 25), percentile(data_train['Horizontal_Distance_To_Fire_Points'], 75)\n",
    "iqr = q75 - q25\n",
    "# calculate the outlier cutoff\n",
    "cut_off = iqr * 3\n",
    "lower, upper = q25 - cut_off, q75 + cut_off\n",
    "# remove outliers\n",
    "data_train_fp = data_train[(data_train['Horizontal_Distance_To_Fire_Points'] > lower) & (data_train['Horizontal_Distance_To_Fire_Points'] < upper)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3_fp=data_train_fp.drop(labels=['Cover_Type'],axis=1)\n",
    "y3_fp=data_train_fp['Cover_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scale_numerical =['Elevation','Aspect','Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology',\n",
    "            'Horizontal_Distance_To_Roadways','Hillshade_9am','Hillshade_Noon','Hillshade_3pm',\n",
    "            'Horizontal_Distance_To_Fire_Points']\n",
    "scaler = StandardScaler()\n",
    "X3_fp[scale_numerical]=scaler.fit_transform(X3_fp[scale_numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split (X3_fp,y3_fp,random_state=37) #seed is 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=20)\n",
    "model_forest = forest.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating accuracy_score\n",
    "forest.score(X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "vars = ['Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology']\n",
    "fig = make_subplots(rows=1, cols=len(vars))\n",
    "for i, var in enumerate(vars):\n",
    "    fig.add_trace(\n",
    "        go.Box(y=data_train[var],\n",
    "        name=var),\n",
    "        row=1, col=i+1\n",
    "    )\n",
    "\n",
    "fig.update_traces(boxpoints='all', jitter=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "vars = ['Horizontal_Distance_To_Roadways', 'Horizontal_Distance_To_Fire_Points']\n",
    "fig = make_subplots(rows=1, cols=len(vars))\n",
    "for i, var in enumerate(vars):\n",
    "    fig.add_trace(\n",
    "        go.Box(y=data_train[var],\n",
    "        name=var),\n",
    "        row=1, col=i+1\n",
    "    )\n",
    "\n",
    "fig.update_traces(boxpoints='all', jitter=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "vars = ['Hillshade_9am', 'Hillshade_Noon']\n",
    "fig = make_subplots(rows=1, cols=len(vars))\n",
    "for i, var in enumerate(vars):\n",
    "    fig.add_trace(\n",
    "        go.Box(y=data_train[var],\n",
    "        name=var),\n",
    "        row=1, col=i+1\n",
    "    )\n",
    "\n",
    "fig.update_traces(boxpoints='all', jitter=.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2'></a>\n",
    "## <font color=green> 5.2 Feature Transformation and Building of new features <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.1'></a>\n",
    "### <font color=green> 5.2.1 ID <font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We agree for the test to not remove ID because the ID is the unique indentifier to evaluate\n",
    "data_train.drop('Id',axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.2.2 Bivariate Combinations <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During feature engineering, we want to try to create a wide variety of interactions between multiple variables in order to create new variables. By manipulating them together, we create opportunities to have new and impactful features which could potentially impact our target variable, thus engineering our features. For this argument, we will create as many bivariate combinations of our predicting variables using the ‘combinations’ method from itertools library.It is also recommeneded to not make interactions with the dummy variables as these are either 0 or 1 and we will not get any additional information from making the interaction this way. Further, it is not recommended to use standardization before bivariate combinations as we want to increase the signal. <br>\n",
    "Source: https://towardsdatascience.com/feature-engineering-combination-polynomial-features-3caa4c77a755 <br>\n",
    "https://samchaaa.medium.com/preprocessing-why-you-should-generate-polynomial-features-first-before-standardizing-892b4326a91d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the bivariate combination we split the dataset for using it.Note this is not the split we will use later for testing the algorithm. This has only the purpose of testing all the combination and selecting the best once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and drop our target variable 'Cover_Type' from dataframe, \n",
    "# isolating our independent variables\n",
    "X = data_train.drop('Cover_Type', axis = 1)\n",
    "\n",
    "# Isolate our dependent variable as a feature\n",
    "y = data_train['Cover_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split (70/30 size), drop duplicates and missing values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = .3, random_state=42, stratify=y)\n",
    "\n",
    "X_train.drop_duplicates(inplace = True)\n",
    "X_train.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create every possible bivariate combination to be tested for feature engineering\n",
    "from itertools import combinations\n",
    "\n",
    "column_list = X_train.columns\n",
    "filtered_column_list = [column for column in column_list if 'Soil_Type' not in column and 'Wilderness_Area' not in column ] \n",
    "interactions = list(combinations(filtered_column_list, 2))\n",
    "interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these combinations, it would be incredibly tedious and time consuming to test individually every single combination. Instead, we will add each combination to a dictionary, and then index the respective dictionary items as arguments in an iterative __random forest regression__ and __logistic Regression__ and will select the top 5 best interaction variables. We only use for simplicity two algorithm and not all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outcome based on the best Random Forest Score \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "interaction_dict = {}\n",
    "i = 1\n",
    "for interaction in interactions:\n",
    "    print(f'Going through interaction {i}')\n",
    "    i += 1\n",
    "    X_train_int = X_train\n",
    "    X_train_int['int'] = X_train_int[interaction[0]] * X_train_int[interaction[1]]\n",
    "    fr3 = RandomForestClassifier(n_estimators=20)\n",
    "    fr3.fit(X_train,y_train)\n",
    "    interaction_dict[fr3.score(X_train_int, y_train)] = interaction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "top_5 = sorted(interaction_dict.keys(), reverse = True)[:5]\n",
    "for interaction in top_5:\n",
    "    print(interaction_dict[interaction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the top  interaction features (which exclude a categorical variable) to existing DF for feature engineered model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (key, value) in interactions:\n",
    "    data_train[key + '_multi_' + value] = data_train[key] * data_train[value]\n",
    "    data_train[key + '_add_' + value] = data_train[key] + data_train[value]\n",
    "    #data_train[key + '_divide_' + value] = data_train[key] / data_train[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pycaret\n",
    "#please note for mac there must be brew instaleld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>...</th>\n",
       "      <th>Soil_Type32</th>\n",
       "      <th>Soil_Type33</th>\n",
       "      <th>Soil_Type34</th>\n",
       "      <th>Soil_Type35</th>\n",
       "      <th>Soil_Type36</th>\n",
       "      <th>Soil_Type37</th>\n",
       "      <th>Soil_Type38</th>\n",
       "      <th>Soil_Type39</th>\n",
       "      <th>Soil_Type40</th>\n",
       "      <th>Cover_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2596</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>510</td>\n",
       "      <td>221</td>\n",
       "      <td>232</td>\n",
       "      <td>148</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2590</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>-6</td>\n",
       "      <td>390</td>\n",
       "      <td>220</td>\n",
       "      <td>235</td>\n",
       "      <td>151</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2785</td>\n",
       "      <td>155</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>118</td>\n",
       "      <td>3090</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>122</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2595</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>-1</td>\n",
       "      <td>391</td>\n",
       "      <td>220</td>\n",
       "      <td>234</td>\n",
       "      <td>150</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0   1       2596      51      3                               258   \n",
       "1   2       2590      56      2                               212   \n",
       "2   3       2804     139      9                               268   \n",
       "3   4       2785     155     18                               242   \n",
       "4   5       2595      45      2                               153   \n",
       "\n",
       "   Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                               0                              510   \n",
       "1                              -6                              390   \n",
       "2                              65                             3180   \n",
       "3                             118                             3090   \n",
       "4                              -1                              391   \n",
       "\n",
       "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm  ...  Soil_Type32  \\\n",
       "0            221             232            148  ...            0   \n",
       "1            220             235            151  ...            0   \n",
       "2            234             238            135  ...            0   \n",
       "3            238             238            122  ...            0   \n",
       "4            220             234            150  ...            0   \n",
       "\n",
       "   Soil_Type33  Soil_Type34  Soil_Type35  Soil_Type36  Soil_Type37  \\\n",
       "0            0            0            0            0            0   \n",
       "1            0            0            0            0            0   \n",
       "2            0            0            0            0            0   \n",
       "3            0            0            0            0            0   \n",
       "4            0            0            0            0            0   \n",
       "\n",
       "   Soil_Type38  Soil_Type39  Soil_Type40  Cover_Type  \n",
       "0            0            0            0           5  \n",
       "1            0            0            0           5  \n",
       "2            0            0            0           2  \n",
       "3            0            0            0           2  \n",
       "4            0            0            0           5  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_train = pd.read_csv(\"train.csv\")\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c557ef2da4b1491c979b8ff004f5f755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Processing: ', max=3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Initiated</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>11:30:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Status</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Preprocessing Data</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  \n",
       "                                                                  \n",
       "Initiated  . . . . . . . . . . . . . . . . . .            11:30:47\n",
       "Status     . . . . . . . . . . . . . . . . . .  Preprocessing Data"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54f1a68a7d340788b98623da12fc3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value=\"Following data types have been inferred automatically, if they are correct press enter to continue…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <td>ID Column</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Elevation</th>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aspect</th>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Slope</th>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilderness_Area1</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilderness_Area2</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilderness_Area3</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilderness_Area4</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type1</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type2</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type3</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type4</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type5</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type6</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type7</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type8</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type9</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type10</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type11</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type12</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type13</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type14</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type15</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type16</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type17</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type18</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type19</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type20</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type21</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type22</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type23</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type24</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type25</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type26</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type27</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type28</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type29</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type30</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type31</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type32</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type33</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type34</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type35</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type36</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type37</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type38</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type39</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type40</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cover_Type</th>\n",
       "      <td>Label</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Data Type\n",
       "Id                                    ID Column\n",
       "Elevation                               Numeric\n",
       "Aspect                                  Numeric\n",
       "Slope                                   Numeric\n",
       "Horizontal_Distance_To_Hydrology        Numeric\n",
       "Vertical_Distance_To_Hydrology          Numeric\n",
       "Horizontal_Distance_To_Roadways         Numeric\n",
       "Hillshade_9am                           Numeric\n",
       "Hillshade_Noon                          Numeric\n",
       "Hillshade_3pm                           Numeric\n",
       "Horizontal_Distance_To_Fire_Points      Numeric\n",
       "Wilderness_Area1                    Categorical\n",
       "Wilderness_Area2                    Categorical\n",
       "Wilderness_Area3                    Categorical\n",
       "Wilderness_Area4                    Categorical\n",
       "Soil_Type1                          Categorical\n",
       "Soil_Type2                          Categorical\n",
       "Soil_Type3                          Categorical\n",
       "Soil_Type4                          Categorical\n",
       "Soil_Type5                          Categorical\n",
       "Soil_Type6                          Categorical\n",
       "Soil_Type7                          Categorical\n",
       "Soil_Type8                          Categorical\n",
       "Soil_Type9                          Categorical\n",
       "Soil_Type10                         Categorical\n",
       "Soil_Type11                         Categorical\n",
       "Soil_Type12                         Categorical\n",
       "Soil_Type13                         Categorical\n",
       "Soil_Type14                         Categorical\n",
       "Soil_Type15                         Categorical\n",
       "Soil_Type16                         Categorical\n",
       "Soil_Type17                         Categorical\n",
       "Soil_Type18                         Categorical\n",
       "Soil_Type19                         Categorical\n",
       "Soil_Type20                         Categorical\n",
       "Soil_Type21                         Categorical\n",
       "Soil_Type22                         Categorical\n",
       "Soil_Type23                         Categorical\n",
       "Soil_Type24                         Categorical\n",
       "Soil_Type25                         Categorical\n",
       "Soil_Type26                         Categorical\n",
       "Soil_Type27                         Categorical\n",
       "Soil_Type28                         Categorical\n",
       "Soil_Type29                         Categorical\n",
       "Soil_Type30                         Categorical\n",
       "Soil_Type31                         Categorical\n",
       "Soil_Type32                         Categorical\n",
       "Soil_Type33                         Categorical\n",
       "Soil_Type34                         Categorical\n",
       "Soil_Type35                         Categorical\n",
       "Soil_Type36                         Categorical\n",
       "Soil_Type37                         Categorical\n",
       "Soil_Type38                         Categorical\n",
       "Soil_Type39                         Categorical\n",
       "Soil_Type40                         Categorical\n",
       "Cover_Type                                Label"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pycaret.classification import *\n",
    "model = setup(data_train, target = 'Cover_Type', polynomial_features = True,polynomial_degree=2,polynomial_threshold=0.3,fold_shuffle=True)\n",
    "model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_81357\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_81357_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
       "      <th id=\"T_81357_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_81357_row0_col0\" class=\"data row0 col0\" >session_id</td>\n",
       "      <td id=\"T_81357_row0_col1\" class=\"data row0 col1\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_81357_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "      <td id=\"T_81357_row1_col1\" class=\"data row1 col1\" >Cover_Type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_81357_row2_col0\" class=\"data row2 col0\" >Target Type</td>\n",
       "      <td id=\"T_81357_row2_col1\" class=\"data row2 col1\" >Multiclass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_81357_row3_col0\" class=\"data row3 col0\" >Label Encoded</td>\n",
       "      <td id=\"T_81357_row3_col1\" class=\"data row3 col1\" >1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_81357_row4_col0\" class=\"data row4 col0\" >Original Data</td>\n",
       "      <td id=\"T_81357_row4_col1\" class=\"data row4 col1\" >(15120, 56)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_81357_row5_col0\" class=\"data row5 col0\" >Missing Values</td>\n",
       "      <td id=\"T_81357_row5_col1\" class=\"data row5 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_81357_row6_col0\" class=\"data row6 col0\" >Numeric Features</td>\n",
       "      <td id=\"T_81357_row6_col1\" class=\"data row6 col1\" >11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_81357_row7_col0\" class=\"data row7 col0\" >Categorical Features</td>\n",
       "      <td id=\"T_81357_row7_col1\" class=\"data row7 col1\" >44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_81357_row8_col0\" class=\"data row8 col0\" >Ordinal Features</td>\n",
       "      <td id=\"T_81357_row8_col1\" class=\"data row8 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_81357_row9_col0\" class=\"data row9 col0\" >High Cardinality Features</td>\n",
       "      <td id=\"T_81357_row9_col1\" class=\"data row9 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_81357_row10_col0\" class=\"data row10 col0\" >High Cardinality Method</td>\n",
       "      <td id=\"T_81357_row10_col1\" class=\"data row10 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_81357_row11_col0\" class=\"data row11 col0\" >Transformed Train Set</td>\n",
       "      <td id=\"T_81357_row11_col1\" class=\"data row11 col1\" >(10583, 54)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_81357_row12_col0\" class=\"data row12 col0\" >Transformed Test Set</td>\n",
       "      <td id=\"T_81357_row12_col1\" class=\"data row12 col1\" >(4537, 54)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_81357_row13_col0\" class=\"data row13 col0\" >Shuffle Train-Test</td>\n",
       "      <td id=\"T_81357_row13_col1\" class=\"data row13 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_81357_row14_col0\" class=\"data row14 col0\" >Stratify Train-Test</td>\n",
       "      <td id=\"T_81357_row14_col1\" class=\"data row14 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_81357_row15_col0\" class=\"data row15 col0\" >Fold Generator</td>\n",
       "      <td id=\"T_81357_row15_col1\" class=\"data row15 col1\" >StratifiedKFold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_81357_row16_col0\" class=\"data row16 col0\" >Fold Number</td>\n",
       "      <td id=\"T_81357_row16_col1\" class=\"data row16 col1\" >10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_81357_row17_col0\" class=\"data row17 col0\" >CPU Jobs</td>\n",
       "      <td id=\"T_81357_row17_col1\" class=\"data row17 col1\" >-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_81357_row18_col0\" class=\"data row18 col0\" >Use GPU</td>\n",
       "      <td id=\"T_81357_row18_col1\" class=\"data row18 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_81357_row19_col0\" class=\"data row19 col0\" >Log Experiment</td>\n",
       "      <td id=\"T_81357_row19_col1\" class=\"data row19 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_81357_row20_col0\" class=\"data row20 col0\" >Experiment Name</td>\n",
       "      <td id=\"T_81357_row20_col1\" class=\"data row20 col1\" >clf-default-name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_81357_row21_col0\" class=\"data row21 col0\" >USI</td>\n",
       "      <td id=\"T_81357_row21_col1\" class=\"data row21 col1\" >b0f8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_81357_row22_col0\" class=\"data row22 col0\" >Imputation Type</td>\n",
       "      <td id=\"T_81357_row22_col1\" class=\"data row22 col1\" >simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_81357_row23_col0\" class=\"data row23 col0\" >Iterative Imputation Iteration</td>\n",
       "      <td id=\"T_81357_row23_col1\" class=\"data row23 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_81357_row24_col0\" class=\"data row24 col0\" >Numeric Imputer</td>\n",
       "      <td id=\"T_81357_row24_col1\" class=\"data row24 col1\" >mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_81357_row25_col0\" class=\"data row25 col0\" >Iterative Imputation Numeric Model</td>\n",
       "      <td id=\"T_81357_row25_col1\" class=\"data row25 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "      <td id=\"T_81357_row26_col0\" class=\"data row26 col0\" >Categorical Imputer</td>\n",
       "      <td id=\"T_81357_row26_col1\" class=\"data row26 col1\" >constant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "      <td id=\"T_81357_row27_col0\" class=\"data row27 col0\" >Iterative Imputation Categorical Model</td>\n",
       "      <td id=\"T_81357_row27_col1\" class=\"data row27 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "      <td id=\"T_81357_row28_col0\" class=\"data row28 col0\" >Unknown Categoricals Handling</td>\n",
       "      <td id=\"T_81357_row28_col1\" class=\"data row28 col1\" >least_frequent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "      <td id=\"T_81357_row29_col0\" class=\"data row29 col0\" >Normalize</td>\n",
       "      <td id=\"T_81357_row29_col1\" class=\"data row29 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
       "      <td id=\"T_81357_row30_col0\" class=\"data row30 col0\" >Normalize Method</td>\n",
       "      <td id=\"T_81357_row30_col1\" class=\"data row30 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
       "      <td id=\"T_81357_row31_col0\" class=\"data row31 col0\" >Transformation</td>\n",
       "      <td id=\"T_81357_row31_col1\" class=\"data row31 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
       "      <td id=\"T_81357_row32_col0\" class=\"data row32 col0\" >Transformation Method</td>\n",
       "      <td id=\"T_81357_row32_col1\" class=\"data row32 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row33\" class=\"row_heading level0 row33\" >33</th>\n",
       "      <td id=\"T_81357_row33_col0\" class=\"data row33 col0\" >PCA</td>\n",
       "      <td id=\"T_81357_row33_col1\" class=\"data row33 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row34\" class=\"row_heading level0 row34\" >34</th>\n",
       "      <td id=\"T_81357_row34_col0\" class=\"data row34 col0\" >PCA Method</td>\n",
       "      <td id=\"T_81357_row34_col1\" class=\"data row34 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row35\" class=\"row_heading level0 row35\" >35</th>\n",
       "      <td id=\"T_81357_row35_col0\" class=\"data row35 col0\" >PCA Components</td>\n",
       "      <td id=\"T_81357_row35_col1\" class=\"data row35 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row36\" class=\"row_heading level0 row36\" >36</th>\n",
       "      <td id=\"T_81357_row36_col0\" class=\"data row36 col0\" >Ignore Low Variance</td>\n",
       "      <td id=\"T_81357_row36_col1\" class=\"data row36 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row37\" class=\"row_heading level0 row37\" >37</th>\n",
       "      <td id=\"T_81357_row37_col0\" class=\"data row37 col0\" >Combine Rare Levels</td>\n",
       "      <td id=\"T_81357_row37_col1\" class=\"data row37 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row38\" class=\"row_heading level0 row38\" >38</th>\n",
       "      <td id=\"T_81357_row38_col0\" class=\"data row38 col0\" >Rare Level Threshold</td>\n",
       "      <td id=\"T_81357_row38_col1\" class=\"data row38 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row39\" class=\"row_heading level0 row39\" >39</th>\n",
       "      <td id=\"T_81357_row39_col0\" class=\"data row39 col0\" >Numeric Binning</td>\n",
       "      <td id=\"T_81357_row39_col1\" class=\"data row39 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row40\" class=\"row_heading level0 row40\" >40</th>\n",
       "      <td id=\"T_81357_row40_col0\" class=\"data row40 col0\" >Remove Outliers</td>\n",
       "      <td id=\"T_81357_row40_col1\" class=\"data row40 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row41\" class=\"row_heading level0 row41\" >41</th>\n",
       "      <td id=\"T_81357_row41_col0\" class=\"data row41 col0\" >Outliers Threshold</td>\n",
       "      <td id=\"T_81357_row41_col1\" class=\"data row41 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row42\" class=\"row_heading level0 row42\" >42</th>\n",
       "      <td id=\"T_81357_row42_col0\" class=\"data row42 col0\" >Remove Multicollinearity</td>\n",
       "      <td id=\"T_81357_row42_col1\" class=\"data row42 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row43\" class=\"row_heading level0 row43\" >43</th>\n",
       "      <td id=\"T_81357_row43_col0\" class=\"data row43 col0\" >Multicollinearity Threshold</td>\n",
       "      <td id=\"T_81357_row43_col1\" class=\"data row43 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row44\" class=\"row_heading level0 row44\" >44</th>\n",
       "      <td id=\"T_81357_row44_col0\" class=\"data row44 col0\" >Clustering</td>\n",
       "      <td id=\"T_81357_row44_col1\" class=\"data row44 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row45\" class=\"row_heading level0 row45\" >45</th>\n",
       "      <td id=\"T_81357_row45_col0\" class=\"data row45 col0\" >Clustering Iteration</td>\n",
       "      <td id=\"T_81357_row45_col1\" class=\"data row45 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row46\" class=\"row_heading level0 row46\" >46</th>\n",
       "      <td id=\"T_81357_row46_col0\" class=\"data row46 col0\" >Polynomial Features</td>\n",
       "      <td id=\"T_81357_row46_col1\" class=\"data row46 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row47\" class=\"row_heading level0 row47\" >47</th>\n",
       "      <td id=\"T_81357_row47_col0\" class=\"data row47 col0\" >Polynomial Degree</td>\n",
       "      <td id=\"T_81357_row47_col1\" class=\"data row47 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row48\" class=\"row_heading level0 row48\" >48</th>\n",
       "      <td id=\"T_81357_row48_col0\" class=\"data row48 col0\" >Trignometry Features</td>\n",
       "      <td id=\"T_81357_row48_col1\" class=\"data row48 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row49\" class=\"row_heading level0 row49\" >49</th>\n",
       "      <td id=\"T_81357_row49_col0\" class=\"data row49 col0\" >Polynomial Threshold</td>\n",
       "      <td id=\"T_81357_row49_col1\" class=\"data row49 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row50\" class=\"row_heading level0 row50\" >50</th>\n",
       "      <td id=\"T_81357_row50_col0\" class=\"data row50 col0\" >Group Features</td>\n",
       "      <td id=\"T_81357_row50_col1\" class=\"data row50 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row51\" class=\"row_heading level0 row51\" >51</th>\n",
       "      <td id=\"T_81357_row51_col0\" class=\"data row51 col0\" >Feature Selection</td>\n",
       "      <td id=\"T_81357_row51_col1\" class=\"data row51 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row52\" class=\"row_heading level0 row52\" >52</th>\n",
       "      <td id=\"T_81357_row52_col0\" class=\"data row52 col0\" >Features Selection Threshold</td>\n",
       "      <td id=\"T_81357_row52_col1\" class=\"data row52 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row53\" class=\"row_heading level0 row53\" >53</th>\n",
       "      <td id=\"T_81357_row53_col0\" class=\"data row53 col0\" >Feature Interaction</td>\n",
       "      <td id=\"T_81357_row53_col1\" class=\"data row53 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row54\" class=\"row_heading level0 row54\" >54</th>\n",
       "      <td id=\"T_81357_row54_col0\" class=\"data row54 col0\" >Feature Ratio</td>\n",
       "      <td id=\"T_81357_row54_col1\" class=\"data row54 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row55\" class=\"row_heading level0 row55\" >55</th>\n",
       "      <td id=\"T_81357_row55_col0\" class=\"data row55 col0\" >Interaction Threshold</td>\n",
       "      <td id=\"T_81357_row55_col1\" class=\"data row55 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row56\" class=\"row_heading level0 row56\" >56</th>\n",
       "      <td id=\"T_81357_row56_col0\" class=\"data row56 col0\" >Fix Imbalance</td>\n",
       "      <td id=\"T_81357_row56_col1\" class=\"data row56 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81357_level0_row57\" class=\"row_heading level0 row57\" >57</th>\n",
       "      <td id=\"T_81357_row57_col0\" class=\"data row57 col0\" >Fix Imbalance Method</td>\n",
       "      <td id=\"T_81357_row57_col1\" class=\"data row57 col1\" >SMOTE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fae92f1afa0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'Simple_Imputer' object has no attribute 'fill_value_categorical'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpycaret\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 2\u001b[0m exp_clf101 \u001b[38;5;241m=\u001b[39m \u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCover_Type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold_shuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pycaret/classification.py:580\u001b[0m, in \u001b[0;36msetup\u001b[0;34m(data, target, train_size, test_data, preprocess, imputation_type, iterative_imputation_iters, categorical_features, categorical_imputation, categorical_iterative_imputer, ordinal_features, high_cardinality_features, high_cardinality_method, numeric_features, numeric_imputation, numeric_iterative_imputer, date_features, ignore_features, normalize, normalize_method, transformation, transformation_method, handle_unknown_categorical, unknown_categorical_method, pca, pca_method, pca_components, ignore_low_variance, combine_rare_levels, rare_level_threshold, bin_numeric_features, remove_outliers, outliers_threshold, remove_multicollinearity, multicollinearity_threshold, remove_perfect_collinearity, create_clusters, cluster_iter, polynomial_features, polynomial_degree, trigonometry_features, polynomial_threshold, group_features, group_names, feature_selection, feature_selection_threshold, feature_selection_method, feature_interaction, feature_ratio, interaction_threshold, fix_imbalance, fix_imbalance_method, data_split_shuffle, data_split_stratify, fold_strategy, fold, fold_shuffle, fold_groups, n_jobs, use_gpu, custom_pipeline, html, session_id, log_experiment, experiment_name, log_plots, log_profile, log_data, silent, verbose, profile, profile_kwargs)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_plots \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    578\u001b[0m     log_plots \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfusion_matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpycaret\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minternal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtabular\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mml_usecase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclassification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mavailable_plots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mavailable_plots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimputation_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimputation_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43miterative_imputation_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miterative_imputation_iters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_imputation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_imputation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_iterative_imputer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_iterative_imputer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mordinal_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mordinal_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhigh_cardinality_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhigh_cardinality_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhigh_cardinality_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhigh_cardinality_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumeric_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumeric_imputation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_imputation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumeric_iterative_imputer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_iterative_imputer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalize_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformation_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformation_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhandle_unknown_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhandle_unknown_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43munknown_categorical_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munknown_categorical_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpca\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpca\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpca_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpca_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpca_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpca_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_low_variance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_low_variance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcombine_rare_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombine_rare_levels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrare_level_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrare_level_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbin_numeric_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbin_numeric_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_outliers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_outliers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutliers_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutliers_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_multicollinearity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_multicollinearity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulticollinearity_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulticollinearity_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_perfect_collinearity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_perfect_collinearity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_clusters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcluster_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcluster_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolynomial_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolynomial_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolynomial_degree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolynomial_degree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrigonometry_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrigonometry_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolynomial_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolynomial_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_selection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_selection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_selection_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_selection_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_selection_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_selection_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_interaction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_interaction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m    \u001b[49m\u001b[43minteraction_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minteraction_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_imbalance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfix_imbalance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_imbalance_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfix_imbalance_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_split_shuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_split_shuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_split_stratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_split_stratify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold_shuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_shuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_gpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_gpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhtml\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhtml\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_experiment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_experiment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_plots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_plots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_profile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_profile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprofile_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pycaret/internal/tabular.py:1745\u001b[0m, in \u001b[0;36msetup\u001b[0;34m(data, target, ml_usecase, available_plots, train_size, test_data, preprocess, imputation_type, iterative_imputation_iters, categorical_features, categorical_imputation, categorical_iterative_imputer, ordinal_features, high_cardinality_features, high_cardinality_method, numeric_features, numeric_imputation, numeric_iterative_imputer, date_features, ignore_features, normalize, normalize_method, transformation, transformation_method, handle_unknown_categorical, unknown_categorical_method, pca, pca_method, pca_components, ignore_low_variance, combine_rare_levels, rare_level_threshold, bin_numeric_features, remove_outliers, outliers_threshold, remove_multicollinearity, multicollinearity_threshold, remove_perfect_collinearity, create_clusters, cluster_iter, polynomial_features, polynomial_degree, trigonometry_features, polynomial_threshold, group_features, group_names, feature_selection, feature_selection_threshold, feature_selection_method, feature_interaction, feature_ratio, interaction_threshold, fix_imbalance, fix_imbalance_method, transform_target, transform_target_method, data_split_shuffle, data_split_stratify, fold_strategy, fold, fold_shuffle, fold_groups, n_jobs, use_gpu, custom_pipeline, html, session_id, log_experiment, experiment_name, log_plots, log_profile, log_data, silent, verbose, profile, profile_kwargs, display)\u001b[0m\n\u001b[1;32m   1742\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaster_model_container: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(master_model_container)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1743\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay_container: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(display_container)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1745\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprep_pipe\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1746\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msetup() succesfully completed......................................\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1748\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:279\u001b[0m, in \u001b[0;36mBaseEstimator.__repr__\u001b[0;34m(self, N_CHAR_MAX)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# use ellipsis for sequences with a lot of elements\u001b[39;00m\n\u001b[1;32m    272\u001b[0m pp \u001b[38;5;241m=\u001b[39m _EstimatorPrettyPrinter(\n\u001b[1;32m    273\u001b[0m     compact\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m     indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    275\u001b[0m     indent_at_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    276\u001b[0m     n_max_elements_to_show\u001b[38;5;241m=\u001b[39mN_MAX_ELEMENTS_TO_SHOW,\n\u001b[1;32m    277\u001b[0m )\n\u001b[0;32m--> 279\u001b[0m repr_ \u001b[38;5;241m=\u001b[39m \u001b[43mpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Use bruteforce ellipsis when there are a lot of non-blank characters\u001b[39;00m\n\u001b[1;32m    282\u001b[0m n_nonblank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(repr_\u001b[38;5;241m.\u001b[39msplit()))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pprint.py:153\u001b[0m, in \u001b[0;36mPrettyPrinter.pformat\u001b[0;34m(self, object)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m    152\u001b[0m     sio \u001b[38;5;241m=\u001b[39m _StringIO()\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sio\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pprint.py:170\u001b[0m, in \u001b[0;36mPrettyPrinter._format\u001b[0;34m(self, object, stream, indent, allowance, context, level)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_readable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m rep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_repr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m max_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_width \u001b[38;5;241m-\u001b[39m indent \u001b[38;5;241m-\u001b[39m allowance\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rep) \u001b[38;5;241m>\u001b[39m max_width:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pprint.py:431\u001b[0m, in \u001b[0;36mPrettyPrinter._repr\u001b[0;34m(self, object, context, level)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_repr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mobject\u001b[39m, context, level):\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28mrepr\u001b[39m, readable, recursive \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m                                            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m readable:\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_readable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/_pprint.py:189\u001b[0m, in \u001b[0;36m_EstimatorPrettyPrinter.format\u001b[0;34m(self, object, context, maxlevels, level)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mobject\u001b[39m, context, maxlevels, level):\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_safe_repr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlevels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchanged_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_changed_only\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/_pprint.py:452\u001b[0m, in \u001b[0;36m_safe_repr\u001b[0;34m(object, context, maxlevels, level, changed_only)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m items:\n\u001b[1;32m    449\u001b[0m     krepr, kreadable, krecur \u001b[38;5;241m=\u001b[39m saferepr(\n\u001b[1;32m    450\u001b[0m         k, context, maxlevels, level, changed_only\u001b[38;5;241m=\u001b[39mchanged_only\n\u001b[1;32m    451\u001b[0m     )\n\u001b[0;32m--> 452\u001b[0m     vrepr, vreadable, vrecur \u001b[38;5;241m=\u001b[39m \u001b[43msaferepr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlevels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchanged_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchanged_only\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m     append(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (krepr\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m), vrepr))\n\u001b[1;32m    456\u001b[0m     readable \u001b[38;5;241m=\u001b[39m readable \u001b[38;5;129;01mand\u001b[39;00m kreadable \u001b[38;5;129;01mand\u001b[39;00m vreadable\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/_pprint.py:419\u001b[0m, in \u001b[0;36m_safe_repr\u001b[0;34m(object, context, maxlevels, level, changed_only)\u001b[0m\n\u001b[1;32m    417\u001b[0m level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m--> 419\u001b[0m     orepr, oreadable, orecur \u001b[38;5;241m=\u001b[39m \u001b[43m_safe_repr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlevels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchanged_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchanged_only\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m     append(orepr)\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m oreadable:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/_pprint.py:419\u001b[0m, in \u001b[0;36m_safe_repr\u001b[0;34m(object, context, maxlevels, level, changed_only)\u001b[0m\n\u001b[1;32m    417\u001b[0m level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m--> 419\u001b[0m     orepr, oreadable, orecur \u001b[38;5;241m=\u001b[39m \u001b[43m_safe_repr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlevels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchanged_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchanged_only\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m     append(orepr)\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m oreadable:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/_pprint.py:442\u001b[0m, in \u001b[0;36m_safe_repr\u001b[0;34m(object, context, maxlevels, level, changed_only)\u001b[0m\n\u001b[1;32m    440\u001b[0m     params \u001b[38;5;241m=\u001b[39m _changed_params(\u001b[38;5;28mobject\u001b[39m)\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 442\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m components \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    444\u001b[0m append \u001b[38;5;241m=\u001b[39m components\u001b[38;5;241m.\u001b[39mappend\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:211\u001b[0m, in \u001b[0;36mBaseEstimator.get_params\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    209\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_param_names():\n\u001b[0;32m--> 211\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_params\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mtype\u001b[39m):\n\u001b[1;32m    213\u001b[0m         deep_items \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mget_params()\u001b[38;5;241m.\u001b[39mitems()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Simple_Imputer' object has no attribute 'fill_value_categorical'"
     ]
    }
   ],
   "source": [
    "from pycaret.classification import *\n",
    "exp_clf101 = setup(data = data_train, target = 'Cover_Type',train_size = 0.7, fold_shuffle=True, session_id = 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "interaction_dict = {}\n",
    "i = 1\n",
    "for interaction in interactions:\n",
    "    print(f'Going through interaction {i}')\n",
    "    i += 1\n",
    "    X_train_int = X_train\n",
    "    X_train_int['int'] = X_train_int[interaction[0]] + X_train_int[interaction[1]]\n",
    "    fr3 = RandomForestClassifier(n_estimators=20)\n",
    "    fr3.fit(X_train,y_train)\n",
    "    interaction_dict[fr3.score(X_train_int, y_train)] = interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top_5 = sorted(interaction_dict.keys(), reverse = True)[:5]\n",
    "for interaction in top_5:\n",
    "    print(interaction_dict[interaction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.2.3 Polynomial Features <font>\n",
    "    \n",
    "We have just seen how to make two variables interact together,but sometimes the relationship between dependent and independent variables are more complex and not linear. Polynomials is another way to create new features! A very strong (usually) option for new features is increasing the power of a single variable. For our purposes, we will try and see if all the existing variables, can improve our Baseline by being increased in power.<br>\n",
    "Source: https://towardsdatascience.com/feature-engineering-combination-polynomial-features-3caa4c77a755"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_dict = {}\n",
    "for feature in X_train_int.columns:\n",
    "    for p in range(2, 5):\n",
    "        X_train_poly = X_train_int\n",
    "        X_train_poly['sq'] = X_train_poly[feature] ** p\n",
    "        fr3 = RandomForestClassifier(n_estimators=20)\n",
    "        fr3.fit(X_train_poly, y_train)\n",
    "        poly_dict[fr3.score(X_train_poly, y_train)] = [feature, p]\n",
    "    \n",
    "poly_dict[max(poly_dict.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5 = sorted(poly_dict.keys(), reverse = True)[:5]\n",
    "for feature in top_5:\n",
    "    print(poly_dict[feature])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In class!!!\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "poly_data_train = pd.DataFrame(poly.fit_transform(data_train))\n",
    "poly_data_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.2'></a>\n",
    "### <font color=green> 5.2.2 Distance To Hydrology <font>\n",
    "#### <font color=green> New Features <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine Vertical distance to Hydrology and Horizontal distance to Hydrology since these two are highly correlated and also we can transform it into one variable which would give the distance to the closest water surface and using Pythagoras \n",
    "theorem for Distance calculation, since we have the horizontal and the vertical Distance. \n",
    "Source : https://towardsdatascience.com/types-of-transformations-for-better-normal-distribution-61c22668d3b9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Distance_To_Hydrology'] = data_train['Horizontal_Distance_To_Hydrology']**2 +data_train['Vertical_Distance_To_Hydrology']**2\n",
    "data_train['Distance_To_Hydrology'] = data_train['Distance_To_Hydrology']**0.5\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Square root and logarithm Transformation  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are checking the distribution of the new created variable if further transformation is needed. The Distance to Hydrology is still positive skewed and has zero values. In order to use log we will use log + 1 in order to use logarithm with zero values. \n",
    "\n",
    "Source: https://www.youtube.com/watch?v=_c3dVTRIK9c and \n",
    "\n",
    "Source_2: https://towardsdatascience.com/types-of-transformations-for-better-normal-distribution-61c22668d3b9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a rule of thump, the skewness can be interpreted as follows:\n",
    "<img src=\"Skew.png\" width=400 height=200 align=\"center\">\n",
    "\n",
    "Source: https://www.marsja.se/transform-skewed-data-using-square-root-log-box-cox-methods-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before transformation\\n\", data_train['Distance_To_Hydrology'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Distance_To_Hydrology'].min(),\n",
    "      \"\\nmax\\n\", data_train['Distance_To_Hydrology'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the log10+ 1 logarithm \n",
    "data_train['log10_Distance_To_Hydrology'] = np.log10(data_train['Distance_To_Hydrology']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the square root \n",
    "data_train['sqr_Distance_To_Hydrology'] = data_train['Distance_To_Hydrology']**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm and <font color=darkcyan> Sqrare root Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m' +\"Skew after Log transformation\\n\", data_train['log10_Distance_To_Hydrology'].skew(), \n",
    "      \"\\nmin\\n\", data_train['log10_Distance_To_Hydrology'].min(),\n",
    "      \"\\nmax\\n\", data_train['log10_Distance_To_Hydrology'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+ \"Skew after Square Root Transformation\\n\", data_train['sqr_Distance_To_Hydrology'].skew(), \n",
    "      \"\\nmin\\n\", data_train['sqr_Distance_To_Hydrology'].min(),\n",
    "      \"\\nmax\\n\", data_train['sqr_Distance_To_Hydrology'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import scipy.stats as stats\n",
    "\n",
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(20,15))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_train['Distance_To_Hydrology'], 'purple')\n",
    "f.add_subplot(332)\n",
    "histPlot(data_train['log10_Distance_To_Hydrology'], 'green')\n",
    "f.add_subplot(333)\n",
    "histPlot(data_train['sqr_Distance_To_Hydrology'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, for distance to Hydrology the __square root__ showed a better performance in terms of skewness and is closer to a normal bell shaped than the logarithm transformation. We will be using Square Root as a new feature in the dataset and will frop the others from the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(['log10_Distance_To_Hydrology'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.3'></a>\n",
    "### <font color=green> 5.2.3 Horizontal Distance To Roadways <font>\n",
    "\n",
    "#### <font color=green> Square root and logarithm Transformation  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For log transformation there should be no zeros, negative values and the distribution should be positive skewed( bigger than 1 is positive) hence we are using the square root as you can see for logarithm transformation below the distribution did not improve!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before Transformation\\n\", data_train['Horizontal_Distance_To_Roadways'].skew(), \n",
    "      \"\\nmin before Transformation\\n\", data_train['Horizontal_Distance_To_Roadways'].min(),\n",
    "      \"\\nmax before Transformation\\n\", data_train['Horizontal_Distance_To_Roadways'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm and <font color=darkcyan> Sqrare root Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we have null values we add plus 1 to avoid log of zero.We are using natural log and log10\n",
    "data_train['Sqr_Horizontal_Distance_To_Roadways'] = data_train['Horizontal_Distance_To_Roadways']**0.5\n",
    "data_train['log_Horizontal_Distance_To_Roadways'] = np.log(data_train['Horizontal_Distance_To_Roadways']+1)\n",
    "data_train['log10_Horizontal_Distance_To_Roadways'] = np.log10(data_train['Horizontal_Distance_To_Roadways']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+ \"Skew after Square Root Transformation\\n\", data_train['Sqr_Horizontal_Distance_To_Roadways'].skew(), \n",
    "      \"\\nmin \\n\", data_train['Sqr_Horizontal_Distance_To_Roadways'].min(),\n",
    "      \"\\nmax \\n\", data_train['Sqr_Horizontal_Distance_To_Roadways'].max(),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m' +\"Skew after log Transformation\\n\", data_train['log_Horizontal_Distance_To_Roadways'].skew(), \n",
    "      \"\\nmin\\n\", data_train['log_Horizontal_Distance_To_Roadways'].min(),\n",
    "      \"\\nmax\\n\", data_train['log_Horizontal_Distance_To_Roadways'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+ \"Skew after log10 transformation\\n\", data_train['log10_Horizontal_Distance_To_Roadways'].skew(), \n",
    "      \"\\nmin \\n\", data_train['log10_Horizontal_Distance_To_Roadways'].min(),\n",
    "      \"\\nmax \\n\", data_train['log10_Horizontal_Distance_To_Roadways'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing if the sqaure root is normally distributed and it shows it is not, however it is less skewed than before\n",
    "stats.normaltest(data_train['Sqr_Horizontal_Distance_To_Roadways'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import scipy.stats as stats\n",
    "\n",
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(15,10))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_train['Horizontal_Distance_To_Roadways'], 'purple')\n",
    "f.add_subplot(334)\n",
    "histPlot(data_train['log_Horizontal_Distance_To_Roadways'], 'green')\n",
    "f.add_subplot(335)\n",
    "histPlot(data_train['log10_Horizontal_Distance_To_Roadways'], 'green')\n",
    "f.add_subplot(332)\n",
    "histPlot(data_train['Sqr_Horizontal_Distance_To_Roadways'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieved the best result for square root of the Horizontal Distance to Roadways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(['log_Horizontal_Distance_To_Roadways','log10_Horizontal_Distance_To_Roadways'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.4'></a>\n",
    "### <font color=green> 5.2.4 Slope <font>\n",
    "#### <font color=green> Square root and logarithm Transformation  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+ \"Skew before transformation\\n\", data_train['Slope'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Slope'].min(),\n",
    "      \"\\nmax \\n\", data_train['Slope'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm and <font color=darkcyan> Sqrare root Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we have null values we add plus 1 to avoid log of zero\n",
    "data_train['logSlope'] = np.log(data_train['Slope']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log transformation\\n\", data_train['logSlope'].skew(), \n",
    "      \"\\nmin\\n\", data_train['logSlope'].min(),\n",
    "      \"\\nmax\\n\", data_train['logSlope'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['SqrSlope'] = data_train['Slope']**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+\"Skew after Square Root transformation\\n\", data_train['SqrSlope'].skew(), \n",
    "      \"\\nmin\\n\", data_train['SqrSlope'].min(),\n",
    "      \"\\nmax\\n\", data_train['SqrSlope'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import scipy.stats as stats\n",
    "\n",
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(15,10))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_train['Slope'], 'purple')\n",
    "f.add_subplot(332)\n",
    "histPlot(data_train['logSlope'], 'green')\n",
    "f.add_subplot(333)\n",
    "histPlot(data_train['SqrSlope'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the skweness for the slope shows better performance when using the square root, we will transform the variable into square root as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(['logSlope'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.5'></a>\n",
    "### <font color=green> 5.2.5 Horizontal Distance To Fire Points  <font>\n",
    "#### <font color=green> Transformation  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before transformation\\n\", data_train['Horizontal_Distance_To_Fire_Points'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Horizontal_Distance_To_Fire_Points'].min(),\n",
    "      \"\\nmax\\n\", data_train['Horizontal_Distance_To_Fire_Points'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm and <font color=darkcyan> Sqrare root Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we have null values we add plus 1 to avoid log of zero\n",
    "data_train['log_Horizontal_Distance_To_firepoints'] = np.log(data_train['Horizontal_Distance_To_Fire_Points']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log transformation\\n\", data_train['log_Horizontal_Distance_To_firepoints'].skew(), \n",
    "      \"\\nmin\\n\", data_train['log_Horizontal_Distance_To_firepoints'].min(),\n",
    "      \"\\nmax\\n\", data_train['log_Horizontal_Distance_To_firepoints'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform with square root\n",
    "data_train['sqr_Horizontal_Distance_To_firepoints'] = data_train['Horizontal_Distance_To_Fire_Points']**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+\"Skew after Square Root transformation\\n\", data_train['sqr_Horizontal_Distance_To_firepoints'].skew(), \n",
    "      \"\\nmin\\n\", data_train['sqr_Horizontal_Distance_To_firepoints'].min(),\n",
    "      \"\\nmax\\n\", data_train['sqr_Horizontal_Distance_To_firepoints'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import scipy.stats as stats\n",
    "\n",
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(15,10))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_train['Horizontal_Distance_To_Fire_Points'], 'purple')\n",
    "f.add_subplot(332)\n",
    "histPlot(data_train['log_Horizontal_Distance_To_firepoints'], 'green')\n",
    "f.add_subplot(333)\n",
    "histPlot(data_train['sqr_Horizontal_Distance_To_firepoints'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since square root transformation gives the best result in skewness, we will also use sqr for the feature variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(['log_Horizontal_Distance_To_firepoints'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.6'></a>\n",
    "### <font color=green> 5.2.6 Hillshades <font>\n",
    "### <font color=green> 5.2.6.1 Mean Hillshade <font>\n",
    "#### <font color=green> Creation of new Feature: Mean Hillshade <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take the average of Hillshades,which gives you the average light exposure of each cover type during the day\n",
    "data_train['Mean_Hillshade'] = (data_train['Hillshade_9am']+data_train['Hillshade_Noon']+data_train['Hillshade_3pm'])/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before transformation\\n\", data_train['Mean_Hillshade'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Mean_Hillshade'].min(),\n",
    "      \"\\nmax\\n\", data_train['Mean_Hillshade'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm Transformation <font color=darkcyan>, Square root Transformation<font color=gold> and BoxCox Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['log_Mean_Hillshade'] = np.log(data_train['Mean_Hillshade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log transformation\\n\", data_train['log_Mean_Hillshade'].skew(), \n",
    "      \"\\nmin\\n\", data_train['log_Mean_Hillshade'].min(),\n",
    "      \"\\nmax\\n\", data_train['log_Mean_Hillshade'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['log10Mean_Hillshade'] = np.log10(data_train['Mean_Hillshade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log10 transformation\\n\", data_train['log10Mean_Hillshade'].skew(), \n",
    "      \"\\nmin\\n\", data_train['log10Mean_Hillshade'].min(),\n",
    "      \"\\nmax\\n\", data_train['log10Mean_Hillshade'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['sqr_Mean_Hillshade'] = data_train['Mean_Hillshade']**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+\"Skew after Square Root transformation\\n\", data_train['sqr_Mean_Hillshade'].skew(), \n",
    "      \"\\nmin\\n\", data_train['sqr_Mean_Hillshade'].min(),\n",
    "      \"\\nmax\\n\", data_train['sqr_Mean_Hillshade'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, the Box-Cox transformation also requires our data to only contain positive numbers\n",
    "# transform training data with Boxcox\n",
    "data_train['Mean_Hillshade_boxcox'], _ = stats.boxcox(data_train['Mean_Hillshade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[93m'+\"Skew after Boxcox transformation\\n\", data_train['Mean_Hillshade_boxcox'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Mean_Hillshade_boxcox'].min(),\n",
    "      \"\\nmax\\n\", data_train['Mean_Hillshade_boxcox'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.normaltest(data_train['Mean_Hillshade_boxcox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import scipy.stats as stats\n",
    "\n",
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(20,10))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_train['Mean_Hillshade'], 'purple')\n",
    "f.add_subplot(335)\n",
    "histPlot(data_train['log_Mean_Hillshade'], 'green')\n",
    "f.add_subplot(334)\n",
    "histPlot(data_train['Mean_Hillshade_boxcox'], 'gold')                    \n",
    "f.add_subplot(332)\n",
    "histPlot(data_train['sqr_Mean_Hillshade'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution did not improve with Square Root and Logarithms Transformation. Hence we use BoxCox which improved the distribution substantially. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(['log10Mean_Hillshade','log_Mean_Hillshade','sqr_Mean_Hillshade'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.2.6.2 Hillshade 9am <font>\n",
    "#### <font color=green> Transformation  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before transformation\\n\", data_train['Hillshade_9am'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Hillshade_9am'].min(),\n",
    "      \"\\nmax\\n\", data_train['Hillshade_9am'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm Transformation <font color=darkcyan>, Square root Transformation<font color=gold> and BoxCox Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['log_Hillshade_9am'] = np.log(data_train['Hillshade_9am']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log transformation\\n\", data_train['log_Hillshade_9am'].skew(), \n",
    "      \"\\nmin\\n\", data_train['log_Hillshade_9am'].min(),\n",
    "      \"\\nmax\\n\", data_train['log_Hillshade_9am'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['sqr_Hillshade_9am'] = data_train['Hillshade_9am']**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+\"Skew after Square Root transformation\\n\", data_train['sqr_Hillshade_9am'].skew(), \n",
    "      \"\\nmin\\n\", data_train['sqr_Hillshade_9am'].min(),\n",
    "      \"\\nmax\\n\", data_train['sqr_Hillshade_9am'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, the Box-Cox transformation also requires our data to only contain positive numbers, transform training data with Boxcox\n",
    "data_train['Hillshade_9am_boxcox'], lam  = stats.boxcox(data_train['Hillshade_9am']+1)\n",
    "#lam is the best lambda for the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[93m'+\"Skew after Boxcox transformation\\n\", data_train['Hillshade_9am_boxcox'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Hillshade_9am_boxcox'].min(),\n",
    "      \"\\nmax\\n\", data_train['Hillshade_9am_boxcox'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import scipy.stats as stats\n",
    "\n",
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(20,10))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_train['Hillshade_9am'], 'purple')\n",
    "f.add_subplot(335)\n",
    "histPlot(data_train['log_Hillshade_9am'], 'green')\n",
    "f.add_subplot(334)\n",
    "histPlot(data_train['Hillshade_9am_boxcox'], 'gold')                    \n",
    "f.add_subplot(332)\n",
    "histPlot(data_train['sqr_Hillshade_9am'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoxCox outperforms the other two for the Hillshade 9am "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(['log_Hillshade_9am','sqr_Hillshade_9am'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.2.6.3 Hillshade Noon <font>\n",
    "#### <font color=green> Transformation  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before transformation\\n\", data_train['Hillshade_Noon'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Hillshade_Noon'].min(),\n",
    "      \"\\nmax\\n\", data_train['Hillshade_Noon'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm Transformation <font color=darkcyan>, Square root Transformation<font color=gold> and BoxCox Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['log_Hillshade_Noon'] = np.log(data_train['Hillshade_Noon']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log transformation\\n\", data_train['log_Hillshade_Noon'].skew(), \n",
    "      \"\\nmin\\n\", data_train['log_Hillshade_Noon'].min(),\n",
    "      \"\\nmax\\n\", data_train['log_Hillshade_Noon'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['sqr_Hillshade_Noon'] = data_train['Hillshade_Noon']**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+\"Skew after Square Root transformation\\n\", data_train['sqr_Hillshade_Noon'].skew(), \n",
    "      \"\\nmin\\n\", data_train['sqr_Hillshade_Noon'].min(),\n",
    "      \"\\nmax\\n\", data_train['sqr_Hillshade_Noon'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, the Box-Cox transformation also requires our data to only contain positive numbers, transform training data with Boxcox\n",
    "data_train['Hillshade_Noon_boxcox'], lam  = stats.boxcox(data_train['Hillshade_Noon'])\n",
    "#lam is the best lambda for the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[93m'+\"Skew after Boxcox transformation\\n\", data_train['Hillshade_Noon_boxcox'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Hillshade_Noon_boxcox'].min(),\n",
    "      \"\\nmax\\n\", data_train['Hillshade_Noon_boxcox'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import scipy.stats as stats\n",
    "\n",
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(20,10))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_train['Hillshade_Noon'], 'purple')\n",
    "f.add_subplot(335)\n",
    "histPlot(data_train['log_Hillshade_Noon'], 'green')\n",
    "f.add_subplot(334)\n",
    "histPlot(data_train['Hillshade_Noon_boxcox'], 'gold')                    \n",
    "f.add_subplot(332)\n",
    "histPlot(data_train['sqr_Hillshade_Noon'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box Coc is outperforming the other transformations for Hillshade Noon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(['log_Hillshade_Noon','sqr_Hillshade_Noon'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.2.6.4 Hillshade 3pm <font>\n",
    "#### <font color=green> Transformation  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before transformation\\n\", data_train['Hillshade_3pm'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Hillshade_3pm'].min(),\n",
    "      \"\\nmax\\n\", data_train['Hillshade_3pm'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm Transformation <font color=darkcyan>, Square root Transformation<font color=gold> and BoxCox Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['log_Hillshade_3pm'] = np.log(data_train['Hillshade_3pm']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log transformation\\n\", data_train['log_Hillshade_3pm'].skew(), \n",
    "      \"\\nmin\\n\", data_train['log_Hillshade_3pm'].min(),\n",
    "      \"\\nmax\\n\", data_train['log_Hillshade_3pm'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['sqr_Hillshade_3pm'] = data_train['Hillshade_3pm']**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+\"Skew after Square Root transformation\\n\", data_train['sqr_Hillshade_3pm'].skew(), \n",
    "      \"\\nmin\\n\", data_train['sqr_Hillshade_3pm'].min(),\n",
    "      \"\\nmax\\n\", data_train['sqr_Hillshade_3pm'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, the Box-Cox transformation also requires our data to only contain positive numbers, transform training data with Boxcox\n",
    "data_train['Hillshade_3pm_boxcox'], lam  = stats.boxcox(data_train['Hillshade_3pm']+1)\n",
    "#lam is the best lambda for the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[93m'+\"Skew after Boxcox transformation\\n\", data_train['Hillshade_3pm_boxcox'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Hillshade_3pm_boxcox'].min(),\n",
    "      \"\\nmax\\n\", data_train['Hillshade_3pm_boxcox'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import scipy.stats as stats\n",
    "\n",
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(20,10))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_train['Hillshade_3pm'], 'purple')\n",
    "f.add_subplot(335)\n",
    "histPlot(data_train['log_Hillshade_3pm'], 'green')\n",
    "f.add_subplot(334)\n",
    "histPlot(data_train['Hillshade_3pm_boxcox'], 'gold')                    \n",
    "f.add_subplot(332)\n",
    "histPlot(data_train['sqr_Hillshade_3pm'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Hillshade 3pm the data was not highly skwed, we either keep the original or we can use boxcox as it improved the variables as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(['log_Hillshade_3pm','sqr_Hillshade_3pm'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.2.7.1 Aspect <font>\n",
    "#### <font color=green> Transformation  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before transformation\\n\", data_train['Aspect'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Aspect'].min(),\n",
    "      \"\\nmax\\n\", data_train['Aspect'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm Transformation <font color=darkcyan>and Square root Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['sqr_Aspect'] = data_train['Aspect']**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+\"Skew after Square Root transformation\\n\", data_train['sqr_Aspect'].skew(), \n",
    "      \"\\nmin\\n\", data_train['sqr_Aspect'].min(),\n",
    "      \"\\nmax\\n\", data_train['sqr_Aspect'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['log_Aspect'] = np.log(data_train['Aspect']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log transformation\\n\", data_train['log_Aspect'].skew(), \n",
    "      \"\\nmin\\n\", data_train['log_Aspect'].min(),\n",
    "      \"\\nmax\\n\", data_train['log_Aspect'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import scipy.stats as stats\n",
    "\n",
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(20,10))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_train['Aspect'], 'purple')\n",
    "f.add_subplot(332)\n",
    "histPlot(data_train['log_Aspect'], 'green')\n",
    "#f.add_subplot(334)\n",
    "#histPlot(data_train['Hillshade_3pm_boxcox'], 'gold')                    \n",
    "f.add_subplot(333)\n",
    "histPlot(data_train['sqr_Aspect'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For aspect square root turned out to be the best transformation in terms of skweness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(['log_Aspect'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.2.6.4 Hillshades  Ratios <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['ratio_Hillshade_3pm'] = data_train['Hillshade_3pm']/255\n",
    "data_train['ratio_Hillshade_Noon'] = data_train['Hillshade_Noon']/255\n",
    "data_train['ratio_Hillshade_9am'] = data_train['Hillshade_9am']/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.2.7.2 Aspect in degrees <font>\n",
    "#### <font color=green> New Features  <font>\n",
    "The azimuth is the angular direction of the sun, measured from north clockwise in degrees from 0 to 360. An Azimuth of 90 degrees is east.The Cut of values will be between for instance the middle of north and east.\n",
    "\n",
    "* Aspect_North: from 315 deg to 45 deg\n",
    "* Aspect_East: from 45 deg to 135 deg\n",
    "* Aspect_South: from 135 deg to 225 deg\n",
    "* Aspect_West: from 225 deg to 315 deg    \n",
    "\n",
    "<img src=\"angle_azimuth.png\" width=400 height=200 align=\"center\">\n",
    "    \n",
    "Source:https://www.pveducation.org/pvcdrom/properties-of-sunlight/azimuth-angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping Aspect in the four directions\n",
    "data_train['Aspect_North']=  np.where(((data_train['Aspect']>=0) & (data_train['Aspect']<45))|((X_train['Aspect']>=315) & (X_train['Aspect']<=360)), 1 ,0)\n",
    "data_train['Aspect_East']= np.where((data_train['Aspect']>=45) & (data_train['Aspect']<135), 1 ,0)\n",
    "data_train['Aspect_South']= np.where((data_train['Aspect']>=135) & (data_train['Aspect']<225), 1 ,0)\n",
    "data_train['Aspect_West']= np.where((data_train['Aspect']>=225) & (data_train['Aspect']<315), 1 ,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.2.8 Elevation <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No transformation as it is already very symetric distributed \n",
    "print('\\033[95m'+\"Skew before transformation\\n\", data_train['Elevation'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Elevation'].min(),\n",
    "      \"\\nmax\\n\", data_train['Elevation'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install htmltabletomd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import htmltabletomd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.2.9 Geoclimate grouping  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> 5.2.9.1 Climatic feature engineering to group soils  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From original database donated by John A. Blackard\n",
    "\n",
    "Code Designations:\n",
    "\n",
    "Wilderness Areas:  \t<br>\n",
    "\n",
    "1 - Rawah Wilderness Area <br>\n",
    "2 - Neota Wilderness Area  <br>\n",
    "3 - Comanche Peak Wilderness Area<br>\n",
    "4 - Cache la Poudre Wilderness Area<br>\n",
    "\n",
    "Soil Types:             1 to 40 : based on the USFS Ecological\n",
    "                        Landtype Units (ELUs) for this study area:<br>\n",
    "\n",
    "  Study Code USFS ELU Code\t\t\tDescription<br>\n",
    "\t 1\t   2702\t\tCathedral family - Rock outcrop complex, extremely stony.<br>\n",
    "\t 2\t   2703\t\tVanet - Ratake families complex, very stony.<br>\n",
    "\t 3\t   2704\t\tHaploborolis - Rock outcrop complex, rubbly.<br>\n",
    "\t 4\t   2705\t\tRatake family - Rock outcrop complex, rubbly.<br>\n",
    "\t 5\t   2706\t\tVanet family - Rock outcrop complex complex, rubbly.<br>\n",
    "\t 6\t   2717\t\tVanet - Wetmore families - Rock outcrop complex, stony.<br>\n",
    "\t 7\t   3501\t\tGothic family.<br>\n",
    "\t 8\t   3502\t\tSupervisor - Limber families complex.<br>\n",
    "\t 9\t   4201\t\tTroutville family, very stony.<br>\n",
    "\t10\t   4703\t\tBullwark - Catamount families - Rock outcrop complex, rubbly.<br>\n",
    "\t11\t   4704\t\tBullwark - Catamount families - Rock land complex, rubbly.<br>\n",
    "\t12\t   4744\t\tLegault family - Rock land complex, stony.<br>\n",
    "\t13\t   4758\t\tCatamount family - Rock land - Bullwark family complex, rubbly.<br>\n",
    "\t14\t   5101\t\tPachic Argiborolis - Aquolis complex.<br>\n",
    "\t15\t   5151\t\tunspecified in the USFS Soil and ELU Survey.<br>\n",
    "\t16\t   6101\t\tCryaquolis - Cryoborolis complex.<br>\n",
    "\t17\t   6102\t\tGateview family - Cryaquolis complex.<br>\n",
    "\t18\t   6731\t\tRogert family, very stony.<br>\n",
    "\t19\t   7101\t\tTypic Cryaquolis - Borohemists complex.<br>\n",
    "\t20\t   7102\t\tTypic Cryaquepts - Typic Cryaquolls complex.<br>\n",
    "\t21\t   7103\t\tTypic Cryaquolls - Leighcan family, till substratum complex.<br>\n",
    "\t22\t   7201\t\tLeighcan family, till substratum, extremely bouldery.<br>\n",
    "\t23\t   7202\t\tLeighcan family, till substratum - Typic Cryaquolls complex.<br>\n",
    "\t24\t   7700\t\tLeighcan family, extremely stony.<br>\n",
    "\t25\t   7701\t\tLeighcan family, warm, extremely stony.<br>\n",
    "\t26\t   7702\t\tGranile - Catamount families complex, very stony.<br>\n",
    "\t27\t   7709\t\tLeighcan family, warm - Rock outcrop complex, extremely stony.<br>\n",
    "\t28\t   7710\t\tLeighcan family - Rock outcrop complex, extremely stony.<br>\n",
    "\t29\t   7745\t\tComo - Legault families complex, extremely stony.<br>\n",
    "\t30\t   7746\t\tComo family - Rock land - Legault family complex, extremely stony.<br>\n",
    "\t31\t   7755\t\tLeighcan - Catamount families complex, extremely stony.<br>\n",
    "\t32\t   7756\t\tCatamount family - Rock outcrop - Leighcan family complex, extremely stony.<br>\n",
    "\t33\t   7757\t\tLeighcan - Catamount families - Rock outcrop complex, extremely stony.<br>\n",
    "\t34\t   7790\t\tCryorthents - Rock land complex, extremely stony.<br>\n",
    "\t35\t   8703\t\tCryumbrepts - Rock outcrop - Cryaquepts complex.<br>\n",
    "\t36\t   8707\t\tBross family - Rock land - Cryumbrepts complex, extremely stony.<br>\n",
    "\t37\t   8708\t\tRock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.<br>\n",
    "\t38\t   8771\t\tLeighcan - Moran families - Cryaquolls complex, extremely stony.<br>\n",
    "\t39\t   8772\t\tMoran family - Cryorthents - Leighcan family complex, extremely <br>stony.\n",
    "\t40\t   8776\t\tMoran family - Cryorthents - Rock land complex, extremely stony.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Note:   First digit:  climatic zone       Second digit:  geologic zones\n",
    "                1.  lower montane dry             1.  alluvium\n",
    "                2.  lower montane                 2.  glacial\n",
    "                3.  montane dry                   3.  shale\n",
    "                4.  montane                       4.  sandstone\n",
    "                5.  montane dry and montane       5.  mixed sedimentary\n",
    "                6.  montane and subalpine         6.unspecified in the USFS ELU Survey\n",
    "                7.  subalpine                     7.  igneous and metamorphic\n",
    "                8.  alpine                        8.  volcanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The USFD, an American federal agency for forest service dependent on the department of agriculture has classified soil types according to __climatic zone (first digit)__ and __geology (second digit)__. Because of this, we believe a similar classification can be artificially engineered grouping all similar soils in 7 categories for climate (there is no lower montane dry soils) and 4 for geology (we do not take into consideration shale, sandstone, volcanic or unspecified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> 5.2.9.2 Climatic Zone feature engineering to group soils  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[\"Lower_Montane_Climate\"] = data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[23456]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Montane_Dry_Climate'] =data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[78]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Montane_Climate'] =data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[1][0123]$|Soil_Type[9]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Montane_Dry_and_Montane_Climate'] =data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[1][45]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Montante_and_Subalpine_Climate'] =data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[1][678]$\")].max(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Subalpine_Climate'] =data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type19$|^Soil_Type[2][0-9]$|^Soil_Type[3][0-4]$\")].max(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Alpine_Climate'] =data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[3][56789]$|Soil_Type40\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> 5.2.9.2 Geological feature engineering to group soils  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The USFD, an American federal agency for forest service dependent on the department of agriculture has classified soil types according to climatic zone (first digit) and geology (second digit). Because of this, we believe a similar classification can be artificially engineered grouping all similar soils in 7 categories for climate (there is no lower montane dry soils) and 4 for geology (we do not take into consideration shale, sandstone, volcanic or unspecified because there are not existing in the dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Note:   First digit:  climatic zone             Second digit:  geologic zones\n",
    "                1.  lower montane dry                   1.  alluvium\n",
    "                2.  lower montane                       2.  glacial\n",
    "                3.  montane dry                         3.  shale\n",
    "                4.  montane                             4.  sandstone\n",
    "                5.  montane dry and montane             5.  mixed sedimentary\n",
    "                6.  montane and subalpine               6.  unspecified in the USFS ELU Survey\n",
    "                7.  subalpine                           7.  igneous and metamorphic\n",
    "                8.  alpine                              8.  volcanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Alluvium_Soil'] = data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[1][45679]$|^Soil_Type[2][01]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Glacial_Soil'] =data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[9]$|^Soil_Type[2][23]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Mixed_Sedimentary_Soil'] =data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[7-8]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Igneus_and_Metamorphic_Soil'] =data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[1-6]$|^Soil_Type[1][01238]$|^Soil_Type[3-4]\\d$|^Soil_Type[2][4-9]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the medium paper \"Preprocessing: Why you should generate polynominal features first before standardizing\" mention it is not good practice to standardize the variablesbefore before PolynominalFeatures. This should be done after to not loss the signal of the variables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and drop our target variable 'Cover_Type' from dataframe, isolating our independent variables\n",
    "X = data_train.drop('Cover_Type', axis = 1)\n",
    "\n",
    "# Isolate our dependent variable as a feature\n",
    "y = data_train['Cover_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split (70/30 size), drop duplicates and missing values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = .3, random_state = 33, stratify=y)\n",
    "\n",
    "X_train.drop_duplicates(inplace = True)\n",
    "X_train.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soil Type Groupings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Factorization\n",
    "\n",
    "The numerical values present a level of detail that may be much more fine-grained than we need. For instance, the soil level can be represented by different categories (soil family, complex or stony/rubberly). We aggregate the data up which can help to avoid overfitting when the data is more aggregate: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.6 Soil Type Family  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Discretization to bin the soil variable to the family type.<br>\n",
    "\n",
    "__Cathedral__ <br>\n",
    "1 Cathedral family - Rock outcrop complex, extremely stony.<br>\n",
    "\n",
    "__Ratake__ <br>\n",
    "2 Vanet - Ratake families complex, very stony.<br>\n",
    "4 Ratake family - Rock outcrop complex, rubbly.<br>\n",
    "\n",
    "__Vanet__<br>\n",
    "5 Vanet family - Rock outcrop complex complex, rubbly.<br>\n",
    "\n",
    "__Wetmore__<br>\n",
    "6 Vanet - Wetmore families - Rock outcrop complex, stony.<br>\n",
    "\n",
    "__Gothic__<br>\n",
    "7 Gothic family.<br>\n",
    "                    \n",
    "__Limber__ <br>\n",
    "8 Supervisor - Limber families complex. <br>\n",
    "\n",
    "__Troutville__<br>\n",
    "9 Troutville family, very stony.<br>\n",
    "\n",
    "__Legault__<br>\n",
    "12 Legault family - Rock land complex, stony.<br>\n",
    "29 Como - Legault families complex, extremely stony.<br>\n",
    "\n",
    "__Gateview__ <br>\n",
    "17 Gateview family - Cryaquolis complex.<br>\n",
    "\n",
    "__Rogert__<br>\n",
    "18 Rogert family, very stony.<br>\n",
    "\n",
    "\n",
    "__Como__<br>\n",
    "30 Como family - Rock land - Legault family complex, extremely stony.<br>\n",
    "\n",
    "__Bross__<br>\n",
    "36 Bross family - Rock land - Cryumbrepts complex, extremely stony.<br>\n",
    "\n",
    "\n",
    "\n",
    "__Catamount__<br>\n",
    "10 Bullwark - Catamount families - Rock outcrop complex, rubbly.<br>\n",
    "11 Bullwark - Catamount families - Rock land complex, rubbly.<br>\n",
    "13 Catamount family - Rock land - Bullwark family complex, rubbly.<br>\n",
    "26 Granile - Catamount families complex, very stony.<br>\n",
    "32 Catamount family - Rock outcrop - Leighcan family complex, extremely stony.<br>\n",
    "31 Leighcan - Catamount families complex, extremely stony.<br>\n",
    "33 Leighcan - Catamount families - Rock outcrop complex, extremely stony.<br>\n",
    "\n",
    "__Leighcan__<br>\n",
    "21 Typic Cryaquolls - Leighcan family, till substratum complex.<br>\n",
    "22 Leighcan family, till substratum, extremely bouldery.<br>\n",
    "23 Leighcan family, till substratum - Typic Cryaquolls complex.<br>\n",
    "24 Leighcan family, extremely stony.<br>\n",
    "25 Leighcan family, warm, extremely stony.<br>\n",
    "27 Leighcan family, warm - Rock outcrop complex, extremely stony.<br>\n",
    "28 Leighcan family - Rock outcrop complex, extremely stony.<br>\n",
    "\n",
    "__Moran__<br>\n",
    "38 Leighcan - Moran families - Cryaquolls complex, extremely stony.<br>\n",
    "39 Moran family - Cryorthents - Leighcan family complex, extremely stony.<br>\n",
    "40 Moran family - Cryorthents - Rock land complex, extremely stony.<br>\n",
    "\n",
    "__Others__<br> \n",
    "3 Haploborolis - Rock outcrop complex, rubbly.<br>\n",
    "15 unspecified in the USFS Soil and ELU Survey.<br>\n",
    "37 Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.<br>\n",
    "34 Cryorthents - Rock land complex, extremely stony.<br>\n",
    "35 Cryumbrepts - Rock outcrop - Cryaquepts complex.<br>\n",
    "20 Typic Cryaquepts - Typic Cryaquolls complex.<br>\n",
    "14 Pachic Argiborolis - Aquolis complex.<br>\n",
    "16 Cryaquolis - Cryoborolis complex.<br>\n",
    "19 Typic Cryaquolis - Borohemists complex.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soil Type\n",
    "family_soil_types = {\n",
    "    'Family_Cathedral': ['Soil_Type1'],\n",
    "    'Family_Retake': ['Soil_Type2', 'Soil_Type4'],\n",
    "    'Family_Vanet': ['Soil_Type5'],\n",
    "    'Family_Wetmore': ['Soil_Type6'],\n",
    "    'Family_Gothic': ['Soil_Type7'],\n",
    "    'Family_Limber': ['Soil_Type8'],\n",
    "    'Family_Troutville_': ['Soil_Type9'],\n",
    "    'Family_Legault': ['Soil_Type12', 'Soil_Type29'],\n",
    "    'Family_Gateview': ['Soil_Type17'],\n",
    "    'Family_Rogert': ['Soil_Type18'],\n",
    "    'Family_Como': ['Soil_Type30'],\n",
    "    'Family_Bross': ['Soil_Type36'],\n",
    "    'Family_Catamount': ['Soil_Type10','Soil_Type11','Soil_Type13','Soil_Type26','Soil_Type32','Soil_Type31','Soil_Type33'],\n",
    "    'Family_Leighcan': ['Soil_Type21','Soil_Type22','Soil_Type23','Soil_Type24','Soil_Type25','Soil_Type27','Soil_Type28'],\n",
    "    'Family_Moran': ['Soil_Type38','Soil_Type39','Soil_Type40'],\n",
    "    'Family_Others': ['Soil_Type3','Soil_Type15','Soil_Type37','Soil_Type34','Soil_Type35','Soil_Type20','Soil_Type14','Soil_Type16','Soil_Type19'],\n",
    "} \n",
    "\n",
    "for family in family_soil_types:\n",
    "    data_train[family] = 0\n",
    "    soil_types = family_soil_types[family]\n",
    "    for soil_type in soil_types:\n",
    "        data_train[family] += data_train[soil_type]\n",
    "\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.6 Soil Type Complex  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will group the soil types according to their family and according to the complex and stonyness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complex Group <br>\n",
    "__Rock_outcrop_complex__ <br>\n",
    "1 Cathedral family - Rock outcrop complex, extremely stony.<br>\n",
    "2 Vanet - Ratake families complex, very stony.<br>\n",
    "3 Haploborolis - Rock outcrop complex, rubbly.<br>\n",
    "4 Ratake family - Rock outcrop complex, rubbly.<br>\n",
    "5 Vanet family - Rock outcrop complex complex, rubbly.<br>\n",
    "6 Vanet - Wetmore families - Rock outcrop complex, stony.<br>\n",
    "10 Bullwark - Catamount families - Rock outcrop complex, rubbly.<br>\n",
    "27 Leighcan family, warm - Rock outcrop complex, extremely stony.<br>\n",
    "28 Leighcan family - Rock outcrop complex, extremely stony.<br>\n",
    "33 Leighcan - Catamount families - Rock outcrop complex, extremely stony.<br>\n",
    "\n",
    "__Ratake_families_complex__<br>\n",
    "2 Vanet - Ratake families complex, very stony.<br>\n",
    "\n",
    "\n",
    "__Limber families complex__<br>\n",
    "8 Supervisor - Limber families complex.<br>\n",
    "\n",
    "__rock land complex__<br>\n",
    "11 Bullwark - Catamount families - Rock land complex, rubbly.<br>\n",
    "12 Legault family - Rock land complex, stony.<br>\n",
    "34 Cryorthents - Rock land complex, extremely stony.<br>\n",
    "40 Moran family - Cryorthents - Rock land complex, extremely stony.<br>\n",
    "\n",
    "__Cryoborolis complex__<br>\n",
    "16 Cryaquolis - Cryoborolis complex.<br>\n",
    "17 Gateview family - Cryaquolis complex.<br>\n",
    "\n",
    "__Bullwark family complex__<br>\n",
    "13 Catamount family - Rock land - Bullwark family complex, rubbly.<br>\n",
    "\n",
    "__Aquolis complex__<br>\n",
    "14 Pachic Argiborolis - Aquolis complex.<br>\n",
    "\n",
    "__Borohemists complex__<br>\n",
    "19 Typic Cryaquolis - Borohemists complex.<br>\n",
    "\n",
    "__Cryaquolls complex__<br>\n",
    "20 Typic Cryaquepts - Typic Cryaquolls complex.<br>\n",
    "23 Leighcan family, till substratum - Typic Cryaquolls complex.<br>\n",
    "38 Leighcan - Moran families - Cryaquolls complex, extremely stony.<br>\n",
    "\n",
    "__till substratum complex__<br>\n",
    "21 Typic Cryaquolls - Leighcan family, till substratum complex.<br>\n",
    "\n",
    "__Catamount families complex__<br>\n",
    "26 Granile - Catamount families complex, very stony.<br>\n",
    "1 Leighcan - Catamount families complex, extremely stony.<br>\n",
    "31 Leighcan - Catamount families complex, extremely stony.<br>\n",
    "\n",
    "__Legault families complex__<br>\n",
    "29 Como - Legault families complex, extremely stony.<br>\n",
    "30 Como family - Rock land - Legault family complex, extremely stony.<br>\n",
    "\n",
    "__Leighcan family complex__<br>\n",
    "32 Catamount family - Rock outcrop - Leighcan family complex, extremely stony.<br>\n",
    "39 Moran family - Cryorthents - Leighcan family complex, extremely stony.<br>\n",
    "\n",
    "__Cryaquepts complex__<br>\n",
    "35 Cryumbrepts - Rock outcrop - Cryaquepts complex.<br>\n",
    "\n",
    "__Cryumbrepts complex__<br>\n",
    "36 Bross family - Rock land - Cryumbrepts complex, extremely stony.<br>\n",
    "\n",
    "__Cryorthents complex__<br>\n",
    "37 Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.<br>\n",
    "\n",
    "__others__ <br>\n",
    "7 Gothic family.<br>\n",
    "9 Troutville family, very stony.<br>\n",
    "22 Leighcan family, till substratum, extremely bouldery.<br>\n",
    "24 Leighcan family, extremely stony.<br>\n",
    "25 Leighcan family, warm, extremely stony.<br>\n",
    "18 Rogert family, very stony.<br>\n",
    "15 unspecified in the USFS Soil and ELU Survey.<br>\n",
    "\n",
    "\n",
    "Source: https://www.kaggle.com/competitions/forest-cover-type-prediction/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.6 Soil Type Stonyness <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex Type\n",
    "family_complex_types = {\n",
    "    'Rock_outcrop_complex': ['Soil_Type1','Soil_Type2','Soil_Type3','Soil_Type4','Soil_Type5','Soil_Type6','Soil_Type10','Soil_Type27','Soil_Type28','Soil_Type33'],\n",
    "    'Ratake_families_complex': ['Soil_Type2'],\n",
    "    'Limber_families_complex': ['Soil_Type8'],\n",
    "    'Rock_land_complex': ['Soil_Type11','Soil_Type12','Soil_Type34','Soil_Type40'],\n",
    "    'Cryoborolis_complex': ['Soil_Type16','Soil_Type17'],\n",
    "    'Bullwark_family_complex': ['Soil_Type13'],\n",
    "    'Aquolis_complex_': ['Soil_Type14'],\n",
    "    'Borohemists_complex': ['Soil_Type19'],\n",
    "    'Cryaquolls_complex': ['Soil_Type20','Soil_Type23','Soil_Type38'],\n",
    "    'Till_substratum_complex': ['Soil_Type21'],\n",
    "    'Catamount_families_complex': ['Soil_Type26','Soil_Type1','Soil_Type31'],\n",
    "    'Legault_families_complex': ['Soil_Type39','Soil_Type30'],\n",
    "    'Leighcan_family_complex': ['Soil_Type32','Soil_Type39'],\n",
    "    'Cryaquepts_complex': ['Soil_Type35'],\n",
    "    'Cryumbrepts_complex': ['Soil_Type36'],\n",
    "    'Cryorthents_complex': ['Soil_Type37'],\n",
    "    'others_complex': ['Soil_Type7','Soil_Type9','Soil_Type22','Soil_Type24','Soil_Type25','Soil_Type18','Soil_Type15'],\n",
    "} \n",
    "\n",
    "for family in family_complex_types:\n",
    "    data_train[family] = 0\n",
    "    complex_types = family_complex_types[family]\n",
    "    for complex_type in complex_types:\n",
    "        data_train[family] += data_train[complex_type]\n",
    "\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stony__ <br>\n",
    "1 Cathedral family - Rock outcrop complex, extremely stony.<br>\n",
    "2 Vanet - Ratake families complex, very stony.<br>\n",
    "6 Vanet - Wetmore families - Rock outcrop complex, stony.<br>\n",
    "9 Troutville family, very stony.<br>\n",
    "12 Legault family - Rock land complex, stony.<br>\n",
    "18 Rogert family, very stony.<br>\n",
    "24 Leighcan family, extremely stony.<br>\n",
    "25 Leighcan family, warm, extremely stony.<br>\n",
    "26 Granile - Catamount families complex, very stony.<br>\n",
    "27 Leighcan family, warm - Rock outcrop complex, extremely stony.<br>\n",
    "28 Leighcan family - Rock outcrop complex, extremely stony.<br>\n",
    "29 Como - Legault families complex, extremely stony.<br>\n",
    "30 Como family - Rock land - Legault family complex, extremely stony.<br>\n",
    "31 Leighcan - Catamount families complex, extremely stony.<br>\n",
    "32 Catamount family - Rock outcrop - Leighcan family complex, extremely stony.<br>\n",
    "33 Leighcan - Catamount families - Rock outcrop complex, extremely stony.<br>\n",
    "34 Cryorthents - Rock land complex, extremely stony.<br>\n",
    "36 Bross family - Rock land - Cryumbrepts complex, extremely stony.<br>\n",
    "37 Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.<br>\n",
    "38 Leighcan - Moran families - Cryaquolls complex, extremely stony.<br>\n",
    "39 Moran family - Cryorthents - Leighcan family complex, extremely stony.<br>\n",
    "40 Moran family - Cryorthents - Rock land complex, extremely stony.<br>\n",
    "\n",
    "__Rubbly__<br>\n",
    "3 Haploborolis - Rock outcrop complex, rubbly.<br>\n",
    "4 Ratake family - Rock outcrop complex, rubbly.<br>\n",
    "5 Vanet family - Rock outcrop complex complex, rubbly.<br>\n",
    "10 Bullwark - Catamount families - Rock outcrop complex, rubbly.<br>\n",
    "11 Bullwark - Catamount families - Rock land complex, rubbly.<br>\n",
    "13 Catamount family - Rock land - Bullwark family complex, rubbly.<br>\n",
    "\n",
    "__others__<br>\n",
    "7 Gothic family.<br>\n",
    "8 Supervisor - Limber families complex.<br>\n",
    "14 Pachic Argiborolis - Aquolis complex.<br>\n",
    "15 unspecified in the USFS Soil and ELU Survey.<br>\n",
    "16 Cryaquolis - Cryoborolis complex.<br>\n",
    "17 Gateview family - Cryaquolis complex.<br>\n",
    "19 Typic Cryaquolis - Borohemists complex.<br>\n",
    "20 Typic Cryaquepts - Typic Cryaquolls complex.<br>\n",
    "21 Typic Cryaquolls - Leighcan family, till substratum complex.<br>\n",
    "22 Leighcan family, till substratum, extremely bouldery.<br>\n",
    "23 Leighcan family, till substratum - Typic Cryaquolls complex.<br>\n",
    "35 Cryumbrepts - Rock outcrop - Cryaquepts complex.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soil Type\n",
    "family_types = {\n",
    "    'Type_Stony': ['Soil_Type1','Soil_Type2', 'Soil_Type6', 'Soil_Type9', 'Soil_Type12', 'Soil_Type18', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40'],\n",
    "    'Type_Rubbly': ['Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type10', 'Soil_Type11', 'Soil_Type13'],\n",
    "    'Type_Other': ['Soil_Type7','Soil_Type8', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type35']\n",
    "} \n",
    "\n",
    "for family in family_types:\n",
    "    data_train[family] = 0\n",
    "    soil_types = family_types[family]\n",
    "    for soil_type in soil_types:\n",
    "        data_train[family] += data_train[soil_type]\n",
    "\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.10 Summary <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th><b>Features</b></th>\n",
    "    <th><b>Transformation</b></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "<td>ID  </td>\n",
    "    <td> Drop</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Distance To Hydrology  </td>\n",
    "    <td><b><i>Square Root</i></b> of the length of the side of horizontal and vertical </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Horizontal Distance To Roadways</td>\n",
    "    <td><b>Square Root</b> of horizontal Distance to Roadways</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> Slope</td>\n",
    "    <td><b><i>Square Root</i></b> Slope</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> Horizontal_Distance To firepoints</td>\n",
    "    <td><b><i>Square Root</i></b> Horizontal Distance to firepoints</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Mean Hillshade</td>\n",
    "    <td><b><i>Box Cox Average</i></b> of all Hillshades features</td>\n",
    "  <tr>      \n",
    "  </tr>\n",
    "    <td>Hillshade 9am</td>\n",
    "    <td><b><i>Box Cox </i></b> Hillshade 9am</td>\n",
    "  <tr>      \n",
    "  </tr>\n",
    "    <td>Hillshade Noon</td>\n",
    "    <td><b><i>Box Cox </i></b> Hillshade Noon</td>\n",
    "  <tr>      \n",
    "  </tr>\n",
    "    <td>Hillshade 3pm</td>\n",
    "    <td><b><i>Box Cox</i></b> Hillshade 3pm</td>\n",
    "  <tr>      \n",
    "  </tr>\n",
    "        <td>Aspect</td>\n",
    "    <td><b><i>Square Root</i></b> Aspect</td>\n",
    "  <tr>      \n",
    "  </tr>\n",
    "    <td>Aspect North, East,South and West</td>\n",
    "    <td><b><i>Grouping</i></b> Aspect</td>\n",
    "  <tr>      \n",
    "  </tr>\n",
    "    <td>Geological Grouping</td>\n",
    "    <td><b><i>Grouping</i></b> Soil Types</td>\n",
    "  <tr>      \n",
    "  </tr>\n",
    "    <td>Climate Grouping</td>\n",
    "    <td><b><i>Grouping</i></b> Soil Types</td>\n",
    "  <tr>      \n",
    "  </tr>\n",
    "     <td>Soil Family</td>\n",
    "    <td><b><i>Grouping</i></b> Soil Families</td>\n",
    "  <tr>      \n",
    "  </tr>\n",
    "     <td>Soil Type Complex</td>\n",
    "    <td><b><i>Grouping</i></b> Soil Complex</td>\n",
    "  <tr>      \n",
    "  </tr> \n",
    "     <td>Soil Type Stonyness</td>\n",
    "    <td><b><i>Grouping</i></b> by Soil stonyness</td>\n",
    "  <tr>      \n",
    "  </tr>     \n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=green> 6.Feature Selection <font>\n",
    "    \n",
    "    \n",
    "## <font color=green> 6.1. Standardization <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the most useful features to train the model can improve the performance of our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset to train and validation set, in order to test our models. We use stratify to have a balanced datset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_train.drop(['Cover_Type'], axis=1)\n",
    "y = data_train['Cover_Type']\n",
    "column_list = X.columns\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=42,stratify=y)\n",
    "print(\"The shape of validation data:{} and {} \".format(X_val.shape,y_val.shape))\n",
    "print(\"The shape of training data:{} and {} \".format(X_train.shape,y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the standardization we need only numerical values, since these has been aleady encoded we use the names to filter out the dummy variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_numerical  = [column for column in column_list if 'Soil' not in column and 'Wilderness_Area' not in  column and 'Aspect_North' not in  column and 'Climate' not in  column and 'Family' not in  column and 'Type' not in  column and 'complex' not in  column and 'Aspect_East' not in  column and 'Aspect_South' not in  column and 'Aspect_West' not in  column ]\n",
    "scale_categorial= [column for column in column_list if column not in scale_numerical ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only the dummy variables filtered \n",
    "data_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_train = data_train.filter(items=scale_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorial_train = data_train.filter(items=scale_categorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#scaler = StandardScaler()\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[scale_numerical] = scaler.fit_transform(X_train[scale_numerical])\n",
    "X_val[scale_numerical] = scaler.fit_transform(X_val[scale_numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green> 6.2. Feature Importance <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no of maximum features we need to select\n",
    "num_feats=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first fit a linear model to the initial dataset to have a baseline to evaluate the data cleaning and feature engineering impact.\n",
    "\n",
    "To facilitate the training process, we will use the `sklearn` library <https://scikit-learn.org/stable/index.html> that provides a wrapper for the preprocessing, training, and evaluation of many machine learning algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "initial_lm_mod = linear_model.LogisticRegression(multi_class='multinomial',\n",
    "   max_iter=1000, penalty='none')\n",
    "\n",
    "#initial_lm_mod = RandomForestRegressor(n_estimators=150)\n",
    "baseline_acc = np.mean(\n",
    "    cross_val_score(initial_lm_mod, X_train,y_train, cv=5))\n",
    "\n",
    "print(f\"Baseline model with Accuracy = {baseline_acc:.4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance(clf, feature_names):\n",
    "    \"\"\"\n",
    "    Function to print the most important features of a logreg classifier\n",
    "    based on the coefficient values\n",
    "    \"\"\"\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            'variable': feature_names, # Feature names\n",
    "            'coefficient': clf.coef_[0] # Feature Coeficients\n",
    "        }\n",
    "    ) \\\n",
    "    .round(decimals=2) \\\n",
    "    .sort_values('coefficient', ascending=False) \\\n",
    "    .style.bar(color=['red', 'green'], align='zero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_feature_importance(\n",
    "    initial_lm_mod.fit(X_val,y_val), \n",
    "    X_train.columns.get_level_values(0).tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance allows you to understand the relationship between the features and the target variable. It also helps you understand what features are irrelevant for the model. <br>\n",
    "* The Most important features are Elevation, Wilderness_Area1, Family_Moran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code from Jorge in Forum, select the best 20 features \n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "selector=SelectKBest(chi2, k=20)\n",
    "# Select the top most important features\n",
    "X_new=selector.fit_transform(X_train, y_train)\n",
    "# Get the indexes for columns selected\n",
    "cols = selector.get_support(indices=True)\n",
    "# Get  columns from original dataframe\n",
    "X_train_new = X_train.iloc[:,cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Decision Tree in the Extra Trees Forest is constructed from the original training sample. Then, at each test node, Each tree is provided with a random sample of k features from the feature-set from which each decision tree must select the best feature to split the data based on some mathematical criteria (typically the Gini Index). This random sample of features leads to the creation of multiple de-correlated decision trees.\n",
    "\n",
    "To perform feature selection using the above forest structure, during the construction of the forest, for each feature, the normalized total reduction in the mathematical criteria used in the decision of feature of split (Gini Index if the Gini Index is used in the construction of the forest) is computed. This value is called the Gini Importance of the feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "\n",
    "# Building the model\n",
    "extra_tree_forest = ExtraTreesClassifier(n_estimators = 5,\n",
    "                                        criterion ='entropy', max_features = 2)\n",
    "  \n",
    "# Training the model\n",
    "extra_tree_forest.fit(X, y)\n",
    "  \n",
    "# Computing the importance of each feature\n",
    "feature_importance = extra_tree_forest.feature_importances_\n",
    "  \n",
    "# Normalizing the individual importances\n",
    "feature_importance_normalized = np.std([tree.feature_importances_ for tree in \n",
    "                                        extra_tree_forest.estimators_],\n",
    "                                        axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting a Bar Graph to compare the models\n",
    "fig, ax = plt.subplots(figsize=(50,20))\n",
    "plt.bar(X.columns, feature_importance_normalized, sort)\n",
    "plt.xlabel('Feature Labels')\n",
    "plt.ylabel('Feature Importances')\n",
    "plt.title('Comparison of different Feature Importances')\n",
    "# Change of fontsize and angle of xticklabels\n",
    "plt.setp( ax.xaxis.get_majorticklabels(), rotation=90, ha=\"right\" )\n",
    "plt.gcf().subplots_adjust(bottom=0.15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green> 6.3. Filter Method <font>\n",
    "### <font color=green> 6.3.1 Chi-squared Selection <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "chi_selector = SelectKBest(chi2, k=num_feats)\n",
    "chi_selector.fit(X_train, y_train)\n",
    "chi_support = chi_selector.get_support()\n",
    "chi_feature = X.loc[:,chi_support].columns.tolist()\n",
    "print(str(len(chi_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Javier code forum, error \n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "listofaccs0 = []\n",
    "listofvars0 =[]\n",
    "\n",
    "#Your code here\n",
    "\n",
    "for k1 in range (0, len(X_train.columns),1):\n",
    "    listofvars0.append(k1)\n",
    "    newX = SelectKBest(chi2, k=k1).fit_transform(X_train,y_train)\n",
    "    \n",
    "    chi_lm_mod = linear_model.LogisticRegression(multi_class='multinomial',\n",
    "    max_iter=1000, penalty='none')\n",
    "    baseline_acc = np.mean(cross_val_score(chi_lm_mod, newX,y_train,cv = 5))\n",
    "    listofaccs0.append(round(baseline_acc,4))\n",
    "\n",
    "plt.figure(figsize(20,15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn \n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "new_features = sklearn.feature_selection.SelectKBest(score_func = chi2, k=40).fit(X_train,y_train)\n",
    "chi2_score = pd.DataFrame(list(zip(X_train.columns, new_features.scores_,new_features.pvalues_)), columns = ['feature','score','pvalue'])\n",
    "chi2_score.sort_values('score', ascending = False)\n",
    "chi2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carlos Code\n",
    "coln=X_train.columns\n",
    "accuracy=[]\n",
    "Best_accuracy=[0,0]\n",
    "baseline_acc_max=0\n",
    "for kval in range(1,len(coln)):\n",
    "    selector = SelectKBest(chi2, k=kval)\n",
    "    trainK = selector.fit_transform(X_train, y_train)\n",
    "    cols = selector.get_support(indices=True)\n",
    "    X_train_newK = X_train.iloc[:,cols]\n",
    "    X_train_newK\n",
    "    initial_lm_mod = linear_model.LogisticRegression(multi_class='multinomial',\n",
    "        max_iter=1000, penalty='none'\n",
    "    )\n",
    "    baseline_acc = np.mean(\n",
    "        cross_val_score(initial_lm_mod, X_train_newK, y_train, cv=5)\n",
    "    )\n",
    "    accuracy.append({baseline_acc})\n",
    "    if baseline_acc > baseline_acc_max:\n",
    "        baseline_acc_max = baseline_acc\n",
    "        Best_accuracy=[kval,baseline_acc_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green> 6.4. Ridge Regularization <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with Ridge (or L2) regularization.<br> We are going to make use of the Ridge Model in sklearn https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification. WE use Ridge instead of Lasso as we have a big number of predictors and for this case Ridge penalizes more when x is big and gives a more robust result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_mod = linear_model.LogisticRegression(multi_class='multinomial', max_iter=10000,penalty='l2')\n",
    "print(\"Accuracy = {:.4}\".format(np.mean(cross_val_score(ridge_mod, X_train, y_train, cv=5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_feature_importance(ridge_mod.fit(X_train,y_train), X_train.columns.get_level_values(0).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar performance w.r.t the un-regularized models. However, you can see how the feature coefficients are smaller than the original ones, due to the regularization.\n",
    "\n",
    "Let's look at how the coefficient weights and accuracy scores change along with the different regularization values.\n",
    "To that end, I have implemented the following piece of code. Do not be overwhelmed by it. It basically defines a list of regularization values to test and train a new Logistic Regression model for one of these regularization values. We keep track of the coefficient values and the accuracy of each of these models to plot them according to the defined regularization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a basic Logisitic Regresion Model that applies L2 (Ridge) regularization\n",
    "ridge_mod = linear_model.LogisticRegression(multi_class='multinomial',max_iter=10000,penalty='l2')\n",
    "\n",
    "# Define a list of 100 regularization values to test (from 0.1 to 0.0001)\n",
    "alphas = 10**np.linspace(-1,-4,100)\n",
    "\n",
    "coefs_ = [] # Array to store the value of the coefficients for each model\n",
    "scores_ = [] # Array to store the accuracy for each model\n",
    "\n",
    "# Go over the regularization values list defined above, train a logreg model for each of the regularization values and evaluate it.\n",
    "for a in alphas:\n",
    "    ridge_mod.set_params(C=a) # Set the regularization parameter \n",
    "    scores_.append(np.mean(cross_val_score(ridge_mod, X_train, y_train, cv=5))) # Appends the accuracy of the model\n",
    "    coefs_.append(ridge_mod.fit(X_train, y_train).coef_.ravel().copy()) # Appends the coefficient of the model\n",
    "\n",
    "# Conver the coefficient and scores arrays to numpy arrays\n",
    "coefs_ = np.array(coefs_)\n",
    "scores_ = np.array(scores_)\n",
    "\n",
    "# Define the figures to plot the values\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "fig.suptitle('Logistic Regression Path', fontsize=20)\n",
    "\n",
    "# Coeff Weights Plot\n",
    "ax1.plot(alphas, coefs_, marker='o')\n",
    "ymin, ymax = plt.ylim()\n",
    "ax1.set_ylabel('Coefficient Weights', fontsize = 15)\n",
    "ax1.set_xlabel('Alpha', fontsize = 15)\n",
    "ax1.axis('tight')\n",
    "\n",
    "# Accuracy Plot\n",
    "ax2.plot(alphas, scores_, marker='o')\n",
    "ymin, ymax = plt.ylim()\n",
    "ax2.set_ylabel('Accuracy Score', fontsize = 15)\n",
    "ax2.set_xlabel('Alpha', fontsize = 15)\n",
    "ax2.axis('tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the left figure, the smaller the alpha value (alpha), the larger the regularization and, consequently, the smaller the weights of the coefficients. This is because, if we check the sklearn documentation, we will see that this value is the: \"Inverse of regularization strength.\"\n",
    "When regularization is large enough (i.e., alpha is small), the values of the coefficients are close to 0 (i.e., null model).\n",
    "As there is a trade-off between variance (i.e., less over-fitted model --> more regularization) and bias (i.e., learning more from the training set --> less regularization), You must find the optimal alpha value. As you can see in the right figure, this value is achieved with small alpha values (i.e., more regularization). This specific value is not always the same since it depends on your data and the prediction problem.\n",
    "To automatize the process of finding the optimal value, you can make use of the LogisticRegressionCV function in sklearn (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html) that performs CV, testing different hyperparameters (that you can provide) and selecting the optimal one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green> 6.3. Filter Method <font>\n",
    "### <font color=green> 6.5. Pearson correlation <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from yellowbrick.target import FeatureCorrelation\n",
    "\n",
    "#X, y = data_train['data'], data_train['Cover_Type']\n",
    "\n",
    "# Create a list of the feature names\n",
    "#features = np.array(data['feature_names'])\n",
    "fig, ax = plt.subplots(figsize=(10,40))         # Sample figsize in inches\n",
    "# Instantiate the visualizer\n",
    "visualizer = FeatureCorrelation(labels=None,sort=True)\n",
    "\n",
    "ax = visualizer.fit(X, y)        # Fit the data to the visualizer\n",
    "visualizer.show()           # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data_train.corrwith(data_train[\"Cover_Type\"])\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.5\n",
    "a=abs(corr)\n",
    "result=a[a>threshold]\n",
    "result=pd.DataFrame(data=a).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y = data_train.copy()\n",
    "X_y['Cover_Type'] = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,40))         # Sample figsize in inches\n",
    "\n",
    "corr_matrix = X_y.corr()\n",
    "\n",
    "# Isolate the column corresponding to `exam_score`\n",
    "corr_target = corr_matrix[['Cover_Type']].drop(labels=['Cover_Type'])\n",
    "\n",
    "sns.heatmap(corr_target, annot=True, fmt='.3', cmap='RdBu_r',ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor_selector(X, y,num_feats):\n",
    "    cor_list = []\n",
    "    feature_name = X.columns.tolist()\n",
    "    # calculate the correlation with y for each feature\n",
    "    for i in X.columns.tolist():\n",
    "        cor = np.corrcoef(X[i], y)[0, 1]\n",
    "        cor_list.append(cor)\n",
    "    # replace NaN with 0\n",
    "    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n",
    "    # feature name\n",
    "    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()\n",
    "    # feature selection? 0 for not select, 1 for select\n",
    "    cor_support = [True if i in cor_feature else False for i in feature_name]\n",
    "    return cor_support, cor_feature\n",
    "cor_support, cor_feature = cor_selector(X, y,num_feats)\n",
    "print(str(len(cor_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green> 6.6. PCA <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.codecademy.com/article/fe-filter-methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
