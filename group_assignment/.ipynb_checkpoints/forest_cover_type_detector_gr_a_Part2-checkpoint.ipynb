{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ml2_group_assignment.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=green> Introduction </font>\n",
    "\n",
    "The assignment is focused on solving the Forest Cover Type Prediction: https://www.kaggle.com/c/forest-cover-type-prediction/overview. This task proposes a classification problem: predict the forest cover type (the predominant kind of tree cover) from strictly cartographic variables (as opposed to remotely sensed data).\n",
    "\n",
    "The study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. Each observation is a 30m x 30m patch. You are asked to predict an integer classification for the forest cover type. The seven types are:\n",
    "\n",
    "1. Spruce/Fir\n",
    "2. Lodgepole Pine\n",
    "3. Ponderosa Pine\n",
    "4. Cottonwood/Willow\n",
    "5. Aspen\n",
    "6. Douglas-fir\n",
    "7. Krummholz\n",
    "\n",
    "The training set (15120 observations) contains both features and the Cover_Type. The test set contains only the features. \n",
    "\n",
    "**You must predict the Cover_Type for every row in the test set (565892 observations).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tree_types.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=green> Table of contents </font>\n",
    "\n",
    "* Data Analysis\n",
    "* Exploratory Data Analysis\n",
    "* Feature Engineering & Selection\n",
    "* Compare Several Machine Learning Models\n",
    "* Perform Hyperparameter Tuning on the Best Model\n",
    "* Interpret Model Results\n",
    "* Evaluate the Best Model with Test Data (replying the initiating question)\n",
    "* Summary & Conclusions\n",
    "\n",
    "# Sections \n",
    "* [Libaries used](#0)\n",
    "* [1. Import Data](#1)\n",
    "* [2. Data analysis](#2)  \n",
    "  * [2.1.Explanation of variables](#2.1)\n",
    "      * [1.2.1 XX](#2.1.1)\n",
    "* [3. Exploratory Data Analysis](#3)\n",
    "  * [3.1 Analysis of the Dataset using EDA](#3.1)\n",
    "  * [3.2 D'Agostino and Pearson's Test](#3.2)  \n",
    "  * [3.3 Checking Variable Completeness ](#3.3)\n",
    "  * [3.4 Correlation Matrix ](#3.4)  \n",
    "  * [3.5 Paired density, scatterplot matrix and 3D Graphics ](#3.5)   \n",
    "  * [3.6 Categorial EDA ](#3.6) \n",
    "      * [3.6.1 Categorial Bar Diagrams](#3.6.1)  \n",
    "      * [3.6.2.Violinplot with Dependent Variable](#3.6.2)  \n",
    "      * [3.6.3.Treemap for categorial Data](#3.6.3) \n",
    "* [4. Baseline Model](#4)\n",
    "  * [4.0 Prepare Data and Standardization](#4.0)\n",
    "  * [4.1 Random Forest](#4.1) \n",
    "  * [4.2 Gradient Boosting](#4.2)  \n",
    "  * [4.3 Decision Trees](#4.3)\n",
    "  * [4.4 K-Nearest Neighbors (KNN)](#4.4)  \n",
    "  * [4.5 Logistic Regression](#4.5) \n",
    "  * [4.6 Naive Bayes](#4.6) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"roosevelt-national-forest.jpeg\" width=1200 height=800 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "# <font color=green> Libraries used </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install squarify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns  # Graphing\n",
    "import matplotlib.pyplot as plt\n",
    "import squarify #treemap\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from yellowbrick.classifier import ROCAUC\n",
    "import warnings\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "##  <font color=green>0.Import the Data </font>\n",
    "Let’s load the training data and create data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"train.csv\")\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep the original dataset for later comparisons and make a copy for the FE process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = data_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(\"test.csv\")\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.1'></a>\n",
    "## <font color=green>  5.Feature Engineering<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.1'></a>\n",
    "### <font color=darkcyan> 5.1 Check for Anomalies and Outliers <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the z-score is sensitive to the  mean and standard deviation and its assumption of a normally distributed variable, we cannot use the z-score for outlier handling because of the skewed data. The disadvantage using percentile it considers always and outlier of the lowest or highest value, even there are no outliers. As the number of observations increases, so does the number of observations considered outliers; After all, using a percentile based method will always flat-out reject a certain percentage of our observations.Thus, we need to use the percentile with care. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.1.1'></a>\n",
    "### <font color=darkcyan> 5.1.1 Outlier Detection Treatment using Inter-Quartile Range rule Function <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IQR is the difference between the 75th and 25th percentile. The IQR is more resistant to outliers. The IQR by definition only covers the middle 50% of the data, so outliers are well outside this range and the presence of a small number of outliers is not likely to change this significantly. If you add an outlier, the IQR will change to another set of data points that are probably not that dissimilar to the previous ones (in most datasets), hence it is “resistant” to change. This is especially the case of a large dataset.\n",
    "\n",
    "Now we are testing different ranges for IQR, namely 2,3 and 4 to check for more extreme outlier values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_function(df, col_name,value_IQR):\n",
    "    ''' this function detects first and third quartile and interquartile range for a given column of a dataframe\n",
    "    then calculates upper and lower limits to determine outliers conservatively\n",
    "    returns the number of lower and uper limit and number of outliers respectively\n",
    "    '''\n",
    "    first_quartile = np.percentile(np.array(df[col_name].tolist()), 25)\n",
    "    third_quartile = np.percentile(np.array(df[col_name].tolist()), 75)\n",
    "    IQR = third_quartile - first_quartile\n",
    "                      \n",
    "    upper_limit = third_quartile+(value_IQR*IQR)\n",
    "    lower_limit = first_quartile-(value_IQR*IQR)\n",
    "    outlier_count = 0\n",
    "                      \n",
    "    for value in df[col_name].tolist():\n",
    "        if (value < lower_limit) | (value > upper_limit):\n",
    "            outlier_count +=1\n",
    "    return lower_limit, upper_limit, outlier_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.1.2'></a>\n",
    "### <font color=darkcyan> 5.1.2 Inter-Quartile Range rule: 4 IQR from Median <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all columns to see if there are any outliers, for all values which are not only 0 and 1\n",
    "for column in [\"Horizontal_Distance_To_Hydrology\",\"Vertical_Distance_To_Hydrology\",\"Horizontal_Distance_To_Roadways\",\"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\",\"Horizontal_Distance_To_Fire_Points\"]:\n",
    "    if outlier_function(data_train, column,4)[2] > 0:\n",
    "        print(\"There are {} outliers in {}\".format(outlier_function(data_train, column,4)[2], column))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 1 record of Hillshade_9am with a zero value, which is a valid value as Hillshade can be zero. This is because there are parts in the mountain that never see the sunlight (blind spots). Hence we keep the value as it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing outliers and testing baseline model\n",
    "# calculate interquartile range\n",
    "from numpy import percentile\n",
    "q25, q75 = percentile(data_train['Vertical_Distance_To_Hydrology'], 25), percentile(data_train['Vertical_Distance_To_Hydrology'], 75)\n",
    "iqr = q75 - q25\n",
    "# calculate the outlier cutoff\n",
    "cut_off = iqr * 4\n",
    "lower, upper = q25 - cut_off, q75 + cut_off\n",
    "# remove outliers\n",
    "data_train_vd_h = data_train[(data_train['Vertical_Distance_To_Hydrology'] > lower) & (data_train['Vertical_Distance_To_Hydrology'] < upper)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the model improves after removing vertical distance to hydrology "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4=data_train_vd_h.drop(labels=['Id','Cover_Type'],axis=1)\n",
    "y4=data_train_vd_h['Cover_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scale_numerical =['Elevation','Aspect','Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology',\n",
    "            'Horizontal_Distance_To_Roadways','Hillshade_9am','Hillshade_Noon','Hillshade_3pm',\n",
    "            'Horizontal_Distance_To_Fire_Points']\n",
    "scaler = StandardScaler()\n",
    "X4[scale_numerical]=scaler.fit_transform(X4[scale_numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y4.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4_train,X4_val,y4_train,y4_val = train_test_split(X4,y4,random_state=37) #seed is 18!Cannot use stratify because the datset is unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=20)\n",
    "model_forest = forest.fit(X4_train,y4_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the previous score with the new score after removing the outliers of vertical distance to Hydrology: previous __forest.score: 0.83968__\n",
    "\n",
    "It improves slightly, hence removing these outliers turns to be the selected approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating accuracy_score\n",
    "forest.score(X4_val,y4_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=orange> Replacing with Median <font>\n",
    "Since removing outliers improved performance of our model, using median values to keep a balanced sample set seems to be a reasonable approach. Otherwise the data becomes unbalanced, for which, other tools have to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med = np.median(data_train['Vertical_Distance_To_Hydrology'])\n",
    "for i in data_train['Vertical_Distance_To_Hydrology']:\n",
    "    if i > upper or i < lower:\n",
    "            data_train['Vertical_Distance_To_Hydrology'] = data_train['Vertical_Distance_To_Hydrology'].replace(i, med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xmed=data_train.drop(labels=['Id','Cover_Type','Vertical_Distance_To_Hydrology'],axis=1)\n",
    "ymed=data_train['Cover_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scale_numerical =['Elevation','Aspect','Slope','Horizontal_Distance_To_Hydrology',\n",
    "            'Horizontal_Distance_To_Roadways','Hillshade_9am','Hillshade_Noon','Hillshade_3pm',\n",
    "            'Horizontal_Distance_To_Fire_Points']\n",
    "scaler = StandardScaler()\n",
    "Xmed[scale_numerical]=scaler.fit_transform(Xmed[scale_numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymed.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xmed_train,Xmed_val,ymed_train,ymed_val = train_test_split(X4,y4,random_state=37) #seed is 18!Cannot use stratify because the datset is unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=20)\n",
    "model_forest = forest.fit(Xmed_train,ymed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating accuracy_score\n",
    "forest.score(Xmed_val,ymed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, model improves slightly, hence proving right the hypothesis of imputing with median values so to obtain a more balanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.1.3'></a>\n",
    "### <font color=darkcyan> 5.1.2 Inter-Quartile Range rule: 3 IQR from Median <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all columns to see if there are any outliers, for all values which are not only 0\n",
    "# and 1\n",
    "for column in [\"Horizontal_Distance_To_Hydrology\",\"Vertical_Distance_To_Hydrology\",\"Horizontal_Distance_To_Roadways\",\"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\",\"Horizontal_Distance_To_Fire_Points\"]:\n",
    "    if outlier_function(data_train, column,3)[2] > 0:\n",
    "        print(\"There are {} outliers in {}\".format(outlier_function(data_train, column,3)[2], column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing outliers and testing baseline model\n",
    "# calculate interquartile range\n",
    "from numpy import percentile\n",
    "q25, q75 = percentile(data_train['Horizontal_Distance_To_Hydrology'], 25), percentile(data_train['Horizontal_Distance_To_Hydrology'], 75)\n",
    "iqr = q75 - q25\n",
    "# calculate the outlier cutoff\n",
    "cut_off = iqr * 3\n",
    "lower, upper = q25 - cut_off, q75 + cut_off\n",
    "# remove outliers\n",
    "data_train_hd_h3 = data_train[(data_train['Horizontal_Distance_To_Hydrology'] > lower) & (data_train['Horizontal_Distance_To_Hydrology'] < upper)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3_hd=data_train_hd_h3.drop(labels=['Cover_Type'],axis=1)\n",
    "y3_hd=data_train_hd_h3['Cover_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scale_numerical =['Elevation','Aspect','Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology',\n",
    "            'Horizontal_Distance_To_Roadways','Hillshade_9am','Hillshade_Noon','Hillshade_3pm',\n",
    "            'Horizontal_Distance_To_Fire_Points']\n",
    "scaler = StandardScaler()\n",
    "X3_hd[scale_numerical]=scaler.fit_transform(X3_hd[scale_numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_hd,X_val_hd,y_train_hd,y_val_hd = train_test_split (X3_hd,y3_hd,random_state=37) #seed is 18!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=20)\n",
    "model_forest = forest.fit(X_train_hd,y_train_hd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.score(X_val_hd,y_val_hd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### <font color=darkcyan> Vertical Distance To Hydrology <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing outliers and testing baseline model\n",
    "# calculate interquartile range\n",
    "from numpy import percentile\n",
    "q25, q75 = percentile(data_train['Vertical_Distance_To_Hydrology'], 25), percentile(data_train['Vertical_Distance_To_Hydrology'], 75)\n",
    "iqr = q75 - q25\n",
    "# calculate the outlier cutoff\n",
    "cut_off = iqr * 3\n",
    "lower, upper = q25 - cut_off, q75 + cut_off\n",
    "# remove outliers\n",
    "data_train_vd_h3 = data_train[(data_train['Vertical_Distance_To_Hydrology'] > lower) & (data_train['Vertical_Distance_To_Hydrology'] < upper)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3=data_train_vd_h3.drop(labels=['Cover_Type'],axis=1)\n",
    "y3=data_train_vd_h3['Cover_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scale_numerical =['Elevation','Aspect','Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology',\n",
    "            'Horizontal_Distance_To_Roadways','Hillshade_9am','Hillshade_Noon','Hillshade_3pm',\n",
    "            'Horizontal_Distance_To_Fire_Points']\n",
    "scaler = StandardScaler()\n",
    "X3[scale_numerical]=scaler.fit_transform(X3[scale_numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split (X3,y3,random_state=37) #seed is 18!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=20)\n",
    "model_forest = forest.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The improvement from previous removal of outliers of Vertical Distance to Hydrology is not so incremental anymore in addition we would remove several points in the dataset, we will disregard this option of the 49 datapoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating accuracy_score\n",
    "forest.score(X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=darkcyan> Horizontal Distance To Roadways <font>\n",
    "Taking out the outliers of roadways does not improve the model it actually gets worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing outliers and testing baseline model\n",
    "# calculate interquartile range\n",
    "from numpy import percentile\n",
    "q25, q75 = percentile(data_train['Horizontal_Distance_To_Roadways'], 25), percentile(data_train['Horizontal_Distance_To_Roadways'], 75)\n",
    "iqr = q75 - q25\n",
    "# calculate the outlier cutoff\n",
    "cut_off = iqr * 3\n",
    "lower, upper = q25 - cut_off, q75 + cut_off\n",
    "# remove outliers\n",
    "data_train_rw = data_train[(data_train['Horizontal_Distance_To_Roadways'] > lower) & (data_train['Horizontal_Distance_To_Roadways'] < upper)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3_rw=data_train_rw.drop(labels=['Cover_Type'],axis=1)\n",
    "y3_rw=data_train_rw['Cover_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scale_numerical =['Elevation','Aspect','Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology',\n",
    "            'Horizontal_Distance_To_Roadways','Hillshade_9am','Hillshade_Noon','Hillshade_3pm',\n",
    "            'Horizontal_Distance_To_Fire_Points']\n",
    "scaler = StandardScaler()\n",
    "X3_rw[scale_numerical]=scaler.fit_transform(X3_rw[scale_numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split (X3_rw,y3_rw,random_state=37) #seed is 18!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=20)\n",
    "model_forest = forest.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating accuracy_score\n",
    "forest.score(X_val,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=darkcyan> Horizontal Distance To Fire Points <font>\n",
    "Taking out the outliers it does not improve the model either "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing outliers and testing baseline model\n",
    "# calculate interquartile range\n",
    "from numpy import percentile\n",
    "q25, q75 = percentile(data_train['Horizontal_Distance_To_Fire_Points'], 25), percentile(data_train['Horizontal_Distance_To_Fire_Points'], 75)\n",
    "iqr = q75 - q25\n",
    "# calculate the outlier cutoff\n",
    "cut_off = iqr * 3\n",
    "lower, upper = q25 - cut_off, q75 + cut_off\n",
    "# remove outliers\n",
    "data_train_fp = data_train[(data_train['Horizontal_Distance_To_Fire_Points'] > lower) & (data_train['Horizontal_Distance_To_Fire_Points'] < upper)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3_fp=data_train_fp.drop(labels=['Cover_Type'],axis=1)\n",
    "y3_fp=data_train_fp['Cover_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scale_numerical =['Elevation','Aspect','Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology',\n",
    "            'Horizontal_Distance_To_Roadways','Hillshade_9am','Hillshade_Noon','Hillshade_3pm',\n",
    "            'Horizontal_Distance_To_Fire_Points']\n",
    "scaler = StandardScaler()\n",
    "X3_fp[scale_numerical]=scaler.fit_transform(X3_fp[scale_numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split (X3_fp,y3_fp,random_state=37) #seed is 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=20)\n",
    "model_forest = forest.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating accuracy_score\n",
    "forest.score(X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "vars = ['Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology']\n",
    "fig = make_subplots(rows=1, cols=len(vars))\n",
    "for i, var in enumerate(vars):\n",
    "    fig.add_trace(\n",
    "        go.Box(y=data_train[var],\n",
    "        name=var),\n",
    "        row=1, col=i+1\n",
    "    )\n",
    "\n",
    "fig.update_traces(boxpoints='all', jitter=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "vars = ['Horizontal_Distance_To_Roadways', 'Horizontal_Distance_To_Fire_Points']\n",
    "fig = make_subplots(rows=1, cols=len(vars))\n",
    "for i, var in enumerate(vars):\n",
    "    fig.add_trace(\n",
    "        go.Box(y=data_train[var],\n",
    "        name=var),\n",
    "        row=1, col=i+1\n",
    "    )\n",
    "\n",
    "fig.update_traces(boxpoints='all', jitter=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "vars = ['Hillshade_9am', 'Hillshade_Noon']\n",
    "fig = make_subplots(rows=1, cols=len(vars))\n",
    "for i, var in enumerate(vars):\n",
    "    fig.add_trace(\n",
    "        go.Box(y=data_train[var],\n",
    "        name=var),\n",
    "        row=1, col=i+1\n",
    "    )\n",
    "\n",
    "fig.update_traces(boxpoints='all', jitter=.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2'></a>\n",
    "## <font color=green> 5.2 Feature Transformation and Building of new features <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.1'></a>\n",
    "### <font color=green> 5.2.1 ID <font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We agree for the test to not remove ID because the ID is the unique indentifier to evaluate\n",
    "#data_train.drop('Id',axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.2.2 Bivariate Combinations <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During feature engineering, we want to try to create a wide variety of interactions between multiple variables in order to create new variables. By manipulating them together, we create opportunities to have new and impactful features which could potentially impact our target variable, thus engineering our features. For this argument, we will create as many bivariate combinations of our predicting variables using the ‘combinations’ method from itertools library.It is also recommeneded to not make interactions with the dummy variables as these are either 0 or 1 and we will not get any additional information from making the interaction this way. Further, it is not recommended to use standardization before bivariate combinations as we want to increase the signal. <br>\n",
    "Source: https://towardsdatascience.com/feature-engineering-combination-polynomial-features-3caa4c77a755 <br>\n",
    "https://samchaaa.medium.com/preprocessing-why-you-should-generate-polynomial-features-first-before-standardizing-892b4326a91d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the bivariate combination we split the dataset for using it.Note this is not the split we will use later for testing the algorithm. This has only the purpose of testing all the combination and selecting the best once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and drop our target variable 'Cover_Type' from dataframe, \n",
    "# isolating our independent variables\n",
    "X = data_train.drop('Cover_Type', axis = 1)\n",
    "\n",
    "# Isolate our dependent variable as a feature\n",
    "y = data_train['Cover_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split (70/30 size), drop duplicates and missing values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = .3, random_state=42, stratify=y)\n",
    "\n",
    "X_train.drop_duplicates(inplace = True)\n",
    "X_train.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create every possible bivariate combination to be tested for feature engineering\n",
    "from itertools import combinations\n",
    "\n",
    "column_list = X_train.columns\n",
    "filtered_column_list = [column for column in column_list if 'Soil_Type' not in column and 'Wilderness_Area' not in column and 'Id' not in column ] \n",
    "interactions = list(combinations(filtered_column_list, 2))\n",
    "interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these combinations, it would be incredibly tedious and time consuming to test individually every single combination. Instead, we will add each combination to a dictionary, and then index the respective dictionary items as arguments in an iterative __random forest regression__ and __logistic Regression__ and will select the top 5 best interaction variables. We only use for simplicity two algorithm and not all."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Outcome based on the best Random Forest Score \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "interaction_dict = {}\n",
    "i = 1\n",
    "for interaction in interactions:\n",
    "    print(f'Going through interaction {i}')\n",
    "    i += 1\n",
    "    X_train_int = X_train\n",
    "    X_train_int['int'] = X_train_int[interaction[0]] * X_train_int[interaction[1]]\n",
    "    fr3 = RandomForestClassifier(n_estimators=20)\n",
    "    fr3.fit(X_train,y_train)\n",
    "    interaction_dict[fr3.score(X_train_int, y_train)] = interaction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "top_5 = sorted(interaction_dict.keys(), reverse = True)[:5]\n",
    "for interaction in top_5:\n",
    "    print(interaction_dict[interaction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the top  interaction features (which exclude a categorical variable) to existing DF for feature engineered model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addition and division has been taken out as it created a lot of noise in the data. The division makes sense if it has a business meaning and the addition only if it is the same scale. However we will add the variables which have the same metrics together in a second step but not in a for loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (key, value) in interactions:\n",
    "    data_train[key + '_x_' + value] = data_train[key] * data_train[value]\n",
    "    #data_train[key + '_+_' + value] = data_train[key] + data_train[value]\n",
    "    #data_train[key + '_divide_' + value] = data_train[key] / data_train[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding only variables with the same metric scale like meters, index, etc \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.2.3 Polynomial Features <font>\n",
    "    \n",
    "We have just seen how to make two variables interact together,but sometimes the relationship between dependent and independent variables are more complex and not linear. Polynomials is another way to create new features! A very strong (usually) option for new features is increasing the power of a single variable. For our purposes, we will try and see if all the existing variables, can improve our Baseline by being increased in power.<br>\n",
    "Source: https://towardsdatascience.com/feature-engineering-combination-polynomial-features-3caa4c77a755"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X_train_int_pf = data_train.iloc[:, 1:10]\n",
    "X_train_int_pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialFeatures_labeled(input_df,power):\n",
    "    '''Basically this is a cover for the sklearn preprocessing function. \n",
    "    The problem with that function is if you give it a labeled dataframe, it ouputs an unlabeled dataframe with potentially\n",
    "    a whole bunch of unlabeled columns. \n",
    "\n",
    "    Inputs:\n",
    "    input_df = Your labeled pandas dataframe (list of x's not raised to any power) \n",
    "    power = what order polynomial you want variables up to. (use the same power as you want entered into pp.PolynomialFeatures(power) directly)\n",
    "\n",
    "    Ouput:\n",
    "    Output: This function relies on the powers_ matrix which is one of the preprocessing function's outputs to create logical labels and \n",
    "    outputs a labeled pandas dataframe   \n",
    "    '''\n",
    "    poly = PolynomialFeatures(power)\n",
    "    output_nparray = poly.fit_transform(input_df)\n",
    "    powers_nparray = poly.powers_\n",
    "\n",
    "    input_feature_names = list(input_df.columns)\n",
    "    target_feature_names = [\"Constant Term\"]\n",
    "    for feature_distillation in powers_nparray[1:]:\n",
    "        intermediary_label = \"\"\n",
    "        final_label = \"\"\n",
    "        for i in range(len(input_feature_names)):\n",
    "            if feature_distillation[i] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                variable = input_feature_names[i]\n",
    "                power = feature_distillation[i]\n",
    "                intermediary_label = \"%s^%d\" % (variable,power)\n",
    "                if final_label == \"\":         #If the final label isn't yet specified\n",
    "                    final_label = intermediary_label\n",
    "                else:\n",
    "                    final_label = final_label + \" x \" + intermediary_label\n",
    "        target_feature_names.append(final_label)\n",
    "    output_df = pd.DataFrame(output_nparray, columns = target_feature_names)\n",
    "    return output_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynominal features of degree two, since we have already enough information, we will not go for Polynominal three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df_pw2 = PolynomialFeatures_labeled(X_train_int_pf,2)\n",
    "pd.set_option('display.max_columns', None)\n",
    "output_df_pw2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few information are doubled with the previous dataframe, so we will have unique values, without ^1  as these columns are not needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = output_df_pw2.columns\n",
    "cols = [column for column in column_list if '^1' not in column]\n",
    "output_df_pw2=output_df_pw2[cols]\n",
    "output_df_pw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.concat([data_train,output_df_pw2], axis=1)\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df_pw3 = PolynomialFeatures_labeled(X_train_int_pf,3)\n",
    "output_df_pw3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.1'></a>\n",
    "### <font color=green> 5.2.1 ID <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We agree for the test to not remove ID because the ID is the unique indentifier to evaluate\n",
    "data_train.drop('Id',axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.2'></a>\n",
    "### <font color=green> 5.2.2 Distance To Hydrology <font>\n",
    "#### <font color=green> New Features <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine Vertical distance to Hydrology and Horizontal distance to Hydrology since these two are highly correlated and also we can transform it into one variable which would give the distance to the closest water surface and using Pythagoras \n",
    "theorem for Distance calculation, since we have the horizontal and the vertical Distance. \n",
    "Source : https://towardsdatascience.com/types-of-transformations-for-better-normal-distribution-61c22668d3b9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Distance_To_Hydrology'] = data_train['Horizontal_Distance_To_Hydrology']**2 +data_train['Vertical_Distance_To_Hydrology']**2\n",
    "data_train['Distance_To_Hydrology'] = data_train['Distance_To_Hydrology']**0.5\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Square root and logarithm Transformation  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are checking the distribution of the new created variable if further transformation is needed. The Distance to Hydrology is still positive skewed and has zero values. In order to use log we will use log + 1 in order to use logarithm with zero values. \n",
    "\n",
    "Source: https://www.youtube.com/watch?v=_c3dVTRIK9c and \n",
    "\n",
    "Source_2: https://towardsdatascience.com/types-of-transformations-for-better-normal-distribution-61c22668d3b9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a rule of thump, the skewness can be interpreted as follows:\n",
    "<img src=\"Skew.png\" width=400 height=200 align=\"center\">\n",
    "\n",
    "Source: https://www.marsja.se/transform-skewed-data-using-square-root-log-box-cox-methods-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before transformation\\n\", data_train['Distance_To_Hydrology'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Distance_To_Hydrology'].min(),\n",
    "      \"\\nmax\\n\", data_train['Distance_To_Hydrology'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the log10+ 1 logarithm \n",
    "data_train['log10_Distance_To_Hydrology'] = np.log10(data_train['Distance_To_Hydrology']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the square root \n",
    "data_train['sqr_Distance_To_Hydrology'] = data_train['Distance_To_Hydrology']**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm and <font color=darkcyan> Sqrare root Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m' +\"Skew after Log transformation\\n\", data_train['log10_Distance_To_Hydrology'].skew(), \n",
    "      \"\\nmin\\n\", data_train['log10_Distance_To_Hydrology'].min(),\n",
    "      \"\\nmax\\n\", data_train['log10_Distance_To_Hydrology'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+ \"Skew after Square Root Transformation\\n\", data_train['sqr_Distance_To_Hydrology'].skew(), \n",
    "      \"\\nmin\\n\", data_train['sqr_Distance_To_Hydrology'].min(),\n",
    "      \"\\nmax\\n\", data_train['sqr_Distance_To_Hydrology'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import scipy.stats as stats\n",
    "\n",
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(20,15))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_train['Distance_To_Hydrology'], 'purple')\n",
    "f.add_subplot(332)\n",
    "histPlot(data_train['log10_Distance_To_Hydrology'], 'green')\n",
    "f.add_subplot(333)\n",
    "histPlot(data_train['sqr_Distance_To_Hydrology'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, for distance to Hydrology the __square root__ showed a better performance in terms of skewness and is closer to a normal bell shaped than the logarithm transformation. We will be using Square Root as a new feature in the dataset and will frop the others from the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(['log10_Distance_To_Hydrology'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.3'></a>\n",
    "### <font color=green> 5.2.3 Horizontal Distance To Roadways <font>\n",
    "\n",
    "#### <font color=green> Square root and logarithm Transformation  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For log transformation there should be no zeros, negative values and the distribution should be positive skewed( bigger than 1 is positive) hence we are using the square root as you can see for logarithm transformation below the distribution did not improve!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before Transformation\\n\", data_train['Horizontal_Distance_To_Roadways'].skew(), \n",
    "      \"\\nmin before Transformation\\n\", data_train['Horizontal_Distance_To_Roadways'].min(),\n",
    "      \"\\nmax before Transformation\\n\", data_train['Horizontal_Distance_To_Roadways'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm and <font color=darkcyan> Square root Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we have null values we add plus 1 to avoid log of zero.We are using natural log and log10\n",
    "data_train['Sqr_Horizontal_Distance_To_Roadways'] = data_train['Horizontal_Distance_To_Roadways']**0.5\n",
    "data_train['log_Horizontal_Distance_To_Roadways'] = np.log(data_train['Horizontal_Distance_To_Roadways']+1)\n",
    "data_train['log10_Horizontal_Distance_To_Roadways'] = np.log10(data_train['Horizontal_Distance_To_Roadways']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+ \"Skew after Square Root Transformation\\n\", data_train['Sqr_Horizontal_Distance_To_Roadways'].skew(), \n",
    "      \"\\nmin \\n\", data_train['Sqr_Horizontal_Distance_To_Roadways'].min(),\n",
    "      \"\\nmax \\n\", data_train['Sqr_Horizontal_Distance_To_Roadways'].max(),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m' +\"Skew after log Transformation\\n\", data_train['log_Horizontal_Distance_To_Roadways'].skew(), \n",
    "      \"\\nmin\\n\", data_train['log_Horizontal_Distance_To_Roadways'].min(),\n",
    "      \"\\nmax\\n\", data_train['log_Horizontal_Distance_To_Roadways'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+ \"Skew after log10 transformation\\n\", data_train['log10_Horizontal_Distance_To_Roadways'].skew(), \n",
    "      \"\\nmin \\n\", data_train['log10_Horizontal_Distance_To_Roadways'].min(),\n",
    "      \"\\nmax \\n\", data_train['log10_Horizontal_Distance_To_Roadways'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing if the sqaure root is normally distributed and it shows it is not, however it is less skewed than before\n",
    "stats.normaltest(data_train['Sqr_Horizontal_Distance_To_Roadways'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import scipy.stats as stats\n",
    "\n",
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(15,10))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_train['Horizontal_Distance_To_Roadways'], 'purple')\n",
    "f.add_subplot(334)\n",
    "histPlot(data_train['log_Horizontal_Distance_To_Roadways'], 'green')\n",
    "f.add_subplot(335)\n",
    "histPlot(data_train['log10_Horizontal_Distance_To_Roadways'], 'green')\n",
    "f.add_subplot(332)\n",
    "histPlot(data_train['Sqr_Horizontal_Distance_To_Roadways'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieved the best result for square root of the Horizontal Distance to Roadways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(['log_Horizontal_Distance_To_Roadways','log10_Horizontal_Distance_To_Roadways'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.4'></a>\n",
    "### <font color=green> 5.2.4 Slope <font>\n",
    "#### <font color=green> Square root and logarithm Transformation  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+ \"Skew before transformation\\n\", data_train['Slope'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Slope'].min(),\n",
    "      \"\\nmax \\n\", data_train['Slope'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm and <font color=darkcyan> Sqrare root Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we have null values we add plus 1 to avoid log of zero\n",
    "data_train['logSlope'] = np.log(data_train['Slope']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log transformation\\n\", data_train['logSlope'].skew(), \n",
    "      \"\\nmin\\n\", data_train['logSlope'].min(),\n",
    "      \"\\nmax\\n\", data_train['logSlope'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['SqrSlope'] = data_train['Slope']**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+\"Skew after Square Root transformation\\n\", data_train['SqrSlope'].skew(), \n",
    "      \"\\nmin\\n\", data_train['SqrSlope'].min(),\n",
    "      \"\\nmax\\n\", data_train['SqrSlope'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import scipy.stats as stats\n",
    "\n",
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(15,10))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_train['Slope'], 'purple')\n",
    "f.add_subplot(332)\n",
    "histPlot(data_train['logSlope'], 'green')\n",
    "f.add_subplot(333)\n",
    "histPlot(data_train['SqrSlope'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the skweness for the slope shows better performance when using the square root, we will transform the variable into square root as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(['logSlope'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.5'></a>\n",
    "### <font color=green> 5.2.5 Horizontal Distance To Fire Points  <font>\n",
    "#### <font color=green> Transformation  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before transformation\\n\", data_train['Horizontal_Distance_To_Fire_Points'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Horizontal_Distance_To_Fire_Points'].min(),\n",
    "      \"\\nmax\\n\", data_train['Horizontal_Distance_To_Fire_Points'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm and <font color=darkcyan> Sqrare root Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we have null values we add plus 1 to avoid log of zero\n",
    "data_train['log_Horizontal_Distance_To_firepoints'] = np.log(data_train['Horizontal_Distance_To_Fire_Points']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log transformation\\n\", data_train['log_Horizontal_Distance_To_firepoints'].skew(), \n",
    "      \"\\nmin\\n\", data_train['log_Horizontal_Distance_To_firepoints'].min(),\n",
    "      \"\\nmax\\n\", data_train['log_Horizontal_Distance_To_firepoints'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform with square root\n",
    "data_train['sqr_Horizontal_Distance_To_firepoints'] = data_train['Horizontal_Distance_To_Fire_Points']**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+\"Skew after Square Root transformation\\n\", data_train['sqr_Horizontal_Distance_To_firepoints'].skew(), \n",
    "      \"\\nmin\\n\", data_train['sqr_Horizontal_Distance_To_firepoints'].min(),\n",
    "      \"\\nmax\\n\", data_train['sqr_Horizontal_Distance_To_firepoints'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import scipy.stats as stats\n",
    "\n",
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(15,10))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_train['Horizontal_Distance_To_Fire_Points'], 'purple')\n",
    "f.add_subplot(332)\n",
    "histPlot(data_train['log_Horizontal_Distance_To_firepoints'], 'green')\n",
    "f.add_subplot(333)\n",
    "histPlot(data_train['sqr_Horizontal_Distance_To_firepoints'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since square root transformation gives the best result in skewness, we will also use sqr for the feature variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(['log_Horizontal_Distance_To_firepoints'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.6'></a>\n",
    "### <font color=green> 5.2.6 Hillshades <font>\n",
    "### <font color=green> 5.2.6.1 Mean Hillshade <font>\n",
    "#### <font color=green> Creation of new Feature: Mean Hillshade <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take the average of Hillshades,which gives you the average light exposure of each cover type during the day\n",
    "data_train['Mean_Hillshade'] = (data_train['Hillshade_9am']+data_train['Hillshade_Noon']+data_train['Hillshade_3pm'])/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Itensity of the Hillshade variables in 3 bin siizes with the bin discretizer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
    "data_train['Mean_Hillshade_bin'] = est.fit_transform(data_train[['Mean_Hillshade']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[['Mean_Hillshade_bin','Mean_Hillshade']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before transformation\\n\", data_train['Mean_Hillshade'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Mean_Hillshade'].min(),\n",
    "      \"\\nmax\\n\", data_train['Mean_Hillshade'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm Transformation <font color=darkcyan>, Square root Transformation<font color=gold> and BoxCox Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['log_Mean_Hillshade'] = np.log(data_train['Mean_Hillshade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log transformation\\n\", data_train['log_Mean_Hillshade'].skew(), \n",
    "      \"\\nmin\\n\", data_train['log_Mean_Hillshade'].min(),\n",
    "      \"\\nmax\\n\", data_train['log_Mean_Hillshade'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['log10Mean_Hillshade'] = np.log10(data_train['Mean_Hillshade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log10 transformation\\n\", data_train['log10Mean_Hillshade'].skew(), \n",
    "      \"\\nmin\\n\", data_train['log10Mean_Hillshade'].min(),\n",
    "      \"\\nmax\\n\", data_train['log10Mean_Hillshade'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['sqr_Mean_Hillshade'] = data_train['Mean_Hillshade']**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+\"Skew after Square Root transformation\\n\", data_train['sqr_Mean_Hillshade'].skew(), \n",
    "      \"\\nmin\\n\", data_train['sqr_Mean_Hillshade'].min(),\n",
    "      \"\\nmax\\n\", data_train['sqr_Mean_Hillshade'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, the Box-Cox transformation also requires our data to only contain positive numbers\n",
    "# transform training data with Boxcox\n",
    "data_train['Mean_Hillshade_boxcox'], _ = stats.boxcox(data_train['Mean_Hillshade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[93m'+\"Skew after Boxcox transformation\\n\", data_train['Mean_Hillshade_boxcox'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Mean_Hillshade_boxcox'].min(),\n",
    "      \"\\nmax\\n\", data_train['Mean_Hillshade_boxcox'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.normaltest(data_train['Mean_Hillshade_boxcox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import scipy.stats as stats\n",
    "\n",
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(20,10))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_train['Mean_Hillshade'], 'purple')\n",
    "f.add_subplot(335)\n",
    "histPlot(data_train['log_Mean_Hillshade'], 'green')\n",
    "f.add_subplot(334)\n",
    "histPlot(data_train['Mean_Hillshade_boxcox'], 'gold')                    \n",
    "f.add_subplot(332)\n",
    "histPlot(data_train['sqr_Mean_Hillshade'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution did not improve with Square Root and Logarithms Transformation. Hence we use BoxCox which improved the distribution substantially. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(['log10Mean_Hillshade','log_Mean_Hillshade','sqr_Mean_Hillshade'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.2.6.2 Hillshade 9am <font>\n",
    "#### <font color=green> Transformation  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before transformation\\n\", data_train['Hillshade_9am'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Hillshade_9am'].min(),\n",
    "      \"\\nmax\\n\", data_train['Hillshade_9am'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm Transformation <font color=darkcyan>, Square root Transformation<font color=gold> and BoxCox Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['log_Hillshade_9am'] = np.log(data_train['Hillshade_9am']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log transformation\\n\", data_train['log_Hillshade_9am'].skew(), \n",
    "      \"\\nmin\\n\", data_train['log_Hillshade_9am'].min(),\n",
    "      \"\\nmax\\n\", data_train['log_Hillshade_9am'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['sqr_Hillshade_9am'] = data_train['Hillshade_9am']**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+\"Skew after Square Root transformation\\n\", data_train['sqr_Hillshade_9am'].skew(), \n",
    "      \"\\nmin\\n\", data_train['sqr_Hillshade_9am'].min(),\n",
    "      \"\\nmax\\n\", data_train['sqr_Hillshade_9am'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, the Box-Cox transformation also requires our data to only contain positive numbers, transform training data with Boxcox\n",
    "data_train['Hillshade_9am_boxcox'], lam  = stats.boxcox(data_train['Hillshade_9am']+1)\n",
    "#lam is the best lambda for the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[93m'+\"Skew after Boxcox transformation\\n\", data_train['Hillshade_9am_boxcox'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Hillshade_9am_boxcox'].min(),\n",
    "      \"\\nmax\\n\", data_train['Hillshade_9am_boxcox'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import scipy.stats as stats\n",
    "\n",
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(20,10))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_train['Hillshade_9am'], 'purple')\n",
    "f.add_subplot(335)\n",
    "histPlot(data_train['log_Hillshade_9am'], 'green')\n",
    "f.add_subplot(334)\n",
    "histPlot(data_train['Hillshade_9am_boxcox'], 'gold')                    \n",
    "f.add_subplot(332)\n",
    "histPlot(data_train['sqr_Hillshade_9am'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoxCox outperforms the other two for the Hillshade 9am "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(['log_Hillshade_9am','sqr_Hillshade_9am'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.2.6.3 Hillshade Noon <font>\n",
    "#### <font color=green> Transformation  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before transformation\\n\", data_train['Hillshade_Noon'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Hillshade_Noon'].min(),\n",
    "      \"\\nmax\\n\", data_train['Hillshade_Noon'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm Transformation <font color=darkcyan>, Square root Transformation<font color=gold> and BoxCox Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['log_Hillshade_Noon'] = np.log(data_train['Hillshade_Noon']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log transformation\\n\", data_train['log_Hillshade_Noon'].skew(), \n",
    "      \"\\nmin\\n\", data_train['log_Hillshade_Noon'].min(),\n",
    "      \"\\nmax\\n\", data_train['log_Hillshade_Noon'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['sqr_Hillshade_Noon'] = data_train['Hillshade_Noon']**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+\"Skew after Square Root transformation\\n\", data_train['sqr_Hillshade_Noon'].skew(), \n",
    "      \"\\nmin\\n\", data_train['sqr_Hillshade_Noon'].min(),\n",
    "      \"\\nmax\\n\", data_train['sqr_Hillshade_Noon'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, the Box-Cox transformation also requires our data to only contain positive numbers, transform training data with Boxcox\n",
    "data_train['Hillshade_Noon_boxcox'], lam  = stats.boxcox(data_train['Hillshade_Noon'])\n",
    "#lam is the best lambda for the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[93m'+\"Skew after Boxcox transformation\\n\", data_train['Hillshade_Noon_boxcox'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Hillshade_Noon_boxcox'].min(),\n",
    "      \"\\nmax\\n\", data_train['Hillshade_Noon_boxcox'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import scipy.stats as stats\n",
    "\n",
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(20,10))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_train['Hillshade_Noon'], 'purple')\n",
    "f.add_subplot(335)\n",
    "histPlot(data_train['log_Hillshade_Noon'], 'green')\n",
    "f.add_subplot(334)\n",
    "histPlot(data_train['Hillshade_Noon_boxcox'], 'gold')                    \n",
    "f.add_subplot(332)\n",
    "histPlot(data_train['sqr_Hillshade_Noon'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box Coc is outperforming the other transformations for Hillshade Noon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(['log_Hillshade_Noon','sqr_Hillshade_Noon'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.2.6.4 Hillshade 3pm <font>\n",
    "#### <font color=green> Transformation  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before transformation\\n\", data_train['Hillshade_3pm'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Hillshade_3pm'].min(),\n",
    "      \"\\nmax\\n\", data_train['Hillshade_3pm'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm Transformation <font color=darkcyan>, Square root Transformation<font color=gold> and BoxCox Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['log_Hillshade_3pm'] = np.log(data_train['Hillshade_3pm']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log transformation\\n\", data_train['log_Hillshade_3pm'].skew(), \n",
    "      \"\\nmin\\n\", data_train['log_Hillshade_3pm'].min(),\n",
    "      \"\\nmax\\n\", data_train['log_Hillshade_3pm'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['sqr_Hillshade_3pm'] = data_train['Hillshade_3pm']**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+\"Skew after Square Root transformation\\n\", data_train['sqr_Hillshade_3pm'].skew(), \n",
    "      \"\\nmin\\n\", data_train['sqr_Hillshade_3pm'].min(),\n",
    "      \"\\nmax\\n\", data_train['sqr_Hillshade_3pm'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, the Box-Cox transformation also requires our data to only contain positive numbers, transform training data with Boxcox\n",
    "data_train['Hillshade_3pm_boxcox'], lam  = stats.boxcox(data_train['Hillshade_3pm']+1)\n",
    "#lam is the best lambda for the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[93m'+\"Skew after Boxcox transformation\\n\", data_train['Hillshade_3pm_boxcox'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Hillshade_3pm_boxcox'].min(),\n",
    "      \"\\nmax\\n\", data_train['Hillshade_3pm_boxcox'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import scipy.stats as stats\n",
    "\n",
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(20,10))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_train['Hillshade_3pm'], 'purple')\n",
    "f.add_subplot(335)\n",
    "histPlot(data_train['log_Hillshade_3pm'], 'green')\n",
    "f.add_subplot(334)\n",
    "histPlot(data_train['Hillshade_3pm_boxcox'], 'gold')                    \n",
    "f.add_subplot(332)\n",
    "histPlot(data_train['sqr_Hillshade_3pm'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Hillshade 3pm the data was not highly skwed, we either keep the original or we can use boxcox as it improved the variables as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(['log_Hillshade_3pm','sqr_Hillshade_3pm'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.2.7.1 Aspect <font>\n",
    "#### <font color=green> Transformation  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[95m'+\"Skew before transformation\\n\", data_train['Aspect'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Aspect'].min(),\n",
    "      \"\\nmax\\n\", data_train['Aspect'].max(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> Results after logarithm Transformation <font color=darkcyan>and Square root Transformation<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['sqr_Aspect'] = data_train['Aspect']**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[96m'+\"Skew after Square Root transformation\\n\", data_train['sqr_Aspect'].skew(), \n",
    "      \"\\nmin\\n\", data_train['sqr_Aspect'].min(),\n",
    "      \"\\nmax\\n\", data_train['sqr_Aspect'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['log_Aspect'] = np.log(data_train['Aspect']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[92m'+\"Skew after log transformation\\n\", data_train['log_Aspect'].skew(), \n",
    "      \"\\nmin\\n\", data_train['log_Aspect'].min(),\n",
    "      \"\\nmax\\n\", data_train['log_Aspect'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import scipy.stats as stats\n",
    "\n",
    "def histPlot(first_feature,col):\n",
    "    sns.distplot(first_feature,color=col,fit = norm,kde = True,kde_kws = {'shade': True, 'linewidth': 3});\n",
    "\n",
    "f = plt.figure(figsize=(20,10))\n",
    "f.add_subplot(331)\n",
    "histPlot(data_train['Aspect'], 'purple')\n",
    "f.add_subplot(332)\n",
    "histPlot(data_train['log_Aspect'], 'green')\n",
    "#f.add_subplot(334)\n",
    "#histPlot(data_train['Hillshade_3pm_boxcox'], 'gold')                    \n",
    "f.add_subplot(333)\n",
    "histPlot(data_train['sqr_Aspect'], 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For aspect square root turned out to be the best transformation in terms of skweness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.drop(['log_Aspect'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.2.6.4 Hillshades  Ratios <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['ratio_Hillshade_3pm'] = data_train['Hillshade_3pm']/255\n",
    "data_train['ratio_Hillshade_Noon'] = data_train['Hillshade_Noon']/255\n",
    "data_train['ratio_Hillshade_9am'] = data_train['Hillshade_9am']/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.2.7.2 Aspect in degrees <font>\n",
    "#### <font color=green> New Features  <font>\n",
    "The azimuth is the angular direction of the sun, measured from north clockwise in degrees from 0 to 360. An Azimuth of 90 degrees is east.The Cut of values will be between for instance the middle of north and east.\n",
    "\n",
    "* Aspect_North: from 315 deg to 45 deg\n",
    "* Aspect_East: from 45 deg to 135 deg\n",
    "* Aspect_South: from 135 deg to 225 deg\n",
    "* Aspect_West: from 225 deg to 315 deg    \n",
    "\n",
    "<img src=\"angle_azimuth.png\" width=400 height=200 align=\"center\">\n",
    "    \n",
    "Source:https://www.pveducation.org/pvcdrom/properties-of-sunlight/azimuth-angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping Aspect in the four directions\n",
    "data_train['Aspect_North']=  np.where(((data_train['Aspect']>=0) & (data_train['Aspect']<45))|((X_train['Aspect']>=315) & (X_train['Aspect']<=360)), 1 ,0)\n",
    "data_train['Aspect_East']= np.where((data_train['Aspect']>=45) & (data_train['Aspect']<135), 1 ,0)\n",
    "data_train['Aspect_South']= np.where((data_train['Aspect']>=135) & (data_train['Aspect']<225), 1 ,0)\n",
    "data_train['Aspect_West']= np.where((data_train['Aspect']>=225) & (data_train['Aspect']<315), 1 ,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.2.8 Elevation <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No transformation as it is already very symetric distributed \n",
    "print('\\033[95m'+\"Skew before transformation\\n\", data_train['Elevation'].skew(), \n",
    "      \"\\nmin\\n\", data_train['Elevation'].min(),\n",
    "      \"\\nmax\\n\", data_train['Elevation'].max(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install htmltabletomd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import htmltabletomd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.2.9 Geoclimate grouping  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> 5.2.9.1 Climatic feature engineering to group soils  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From original database donated by John A. Blackard\n",
    "\n",
    "Code Designations:\n",
    "\n",
    "Wilderness Areas:  \t<br>\n",
    "\n",
    "1 - Rawah Wilderness Area <br>\n",
    "2 - Neota Wilderness Area  <br>\n",
    "3 - Comanche Peak Wilderness Area<br>\n",
    "4 - Cache la Poudre Wilderness Area<br>\n",
    "\n",
    "Soil Types:             1 to 40 : based on the USFS Ecological\n",
    "                        Landtype Units (ELUs) for this study area:<br>\n",
    "\n",
    "  Study Code USFS ELU Code\t\t\tDescription<br>\n",
    "\t 1\t   2702\t\tCathedral family - Rock outcrop complex, extremely stony.<br>\n",
    "\t 2\t   2703\t\tVanet - Ratake families complex, very stony.<br>\n",
    "\t 3\t   2704\t\tHaploborolis - Rock outcrop complex, rubbly.<br>\n",
    "\t 4\t   2705\t\tRatake family - Rock outcrop complex, rubbly.<br>\n",
    "\t 5\t   2706\t\tVanet family - Rock outcrop complex complex, rubbly.<br>\n",
    "\t 6\t   2717\t\tVanet - Wetmore families - Rock outcrop complex, stony.<br>\n",
    "\t 7\t   3501\t\tGothic family.<br>\n",
    "\t 8\t   3502\t\tSupervisor - Limber families complex.<br>\n",
    "\t 9\t   4201\t\tTroutville family, very stony.<br>\n",
    "\t10\t   4703\t\tBullwark - Catamount families - Rock outcrop complex, rubbly.<br>\n",
    "\t11\t   4704\t\tBullwark - Catamount families - Rock land complex, rubbly.<br>\n",
    "\t12\t   4744\t\tLegault family - Rock land complex, stony.<br>\n",
    "\t13\t   4758\t\tCatamount family - Rock land - Bullwark family complex, rubbly.<br>\n",
    "\t14\t   5101\t\tPachic Argiborolis - Aquolis complex.<br>\n",
    "\t15\t   5151\t\tunspecified in the USFS Soil and ELU Survey.<br>\n",
    "\t16\t   6101\t\tCryaquolis - Cryoborolis complex.<br>\n",
    "\t17\t   6102\t\tGateview family - Cryaquolis complex.<br>\n",
    "\t18\t   6731\t\tRogert family, very stony.<br>\n",
    "\t19\t   7101\t\tTypic Cryaquolis - Borohemists complex.<br>\n",
    "\t20\t   7102\t\tTypic Cryaquepts - Typic Cryaquolls complex.<br>\n",
    "\t21\t   7103\t\tTypic Cryaquolls - Leighcan family, till substratum complex.<br>\n",
    "\t22\t   7201\t\tLeighcan family, till substratum, extremely bouldery.<br>\n",
    "\t23\t   7202\t\tLeighcan family, till substratum - Typic Cryaquolls complex.<br>\n",
    "\t24\t   7700\t\tLeighcan family, extremely stony.<br>\n",
    "\t25\t   7701\t\tLeighcan family, warm, extremely stony.<br>\n",
    "\t26\t   7702\t\tGranile - Catamount families complex, very stony.<br>\n",
    "\t27\t   7709\t\tLeighcan family, warm - Rock outcrop complex, extremely stony.<br>\n",
    "\t28\t   7710\t\tLeighcan family - Rock outcrop complex, extremely stony.<br>\n",
    "\t29\t   7745\t\tComo - Legault families complex, extremely stony.<br>\n",
    "\t30\t   7746\t\tComo family - Rock land - Legault family complex, extremely stony.<br>\n",
    "\t31\t   7755\t\tLeighcan - Catamount families complex, extremely stony.<br>\n",
    "\t32\t   7756\t\tCatamount family - Rock outcrop - Leighcan family complex, extremely stony.<br>\n",
    "\t33\t   7757\t\tLeighcan - Catamount families - Rock outcrop complex, extremely stony.<br>\n",
    "\t34\t   7790\t\tCryorthents - Rock land complex, extremely stony.<br>\n",
    "\t35\t   8703\t\tCryumbrepts - Rock outcrop - Cryaquepts complex.<br>\n",
    "\t36\t   8707\t\tBross family - Rock land - Cryumbrepts complex, extremely stony.<br>\n",
    "\t37\t   8708\t\tRock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.<br>\n",
    "\t38\t   8771\t\tLeighcan - Moran families - Cryaquolls complex, extremely stony.<br>\n",
    "\t39\t   8772\t\tMoran family - Cryorthents - Leighcan family complex, extremely <br>stony.\n",
    "\t40\t   8776\t\tMoran family - Cryorthents - Rock land complex, extremely stony.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Note:   First digit:  climatic zone       Second digit:  geologic zones\n",
    "                1.  lower montane dry             1.  alluvium\n",
    "                2.  lower montane                 2.  glacial\n",
    "                3.  montane dry                   3.  shale\n",
    "                4.  montane                       4.  sandstone\n",
    "                5.  montane dry and montane       5.  mixed sedimentary\n",
    "                6.  montane and subalpine         6.unspecified in the USFS ELU Survey\n",
    "                7.  subalpine                     7.  igneous and metamorphic\n",
    "                8.  alpine                        8.  volcanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The USFD, an American federal agency for forest service dependent on the department of agriculture has classified soil types according to __climatic zone (first digit)__ and __geology (second digit)__. Because of this, we believe a similar classification can be artificially engineered grouping all similar soils in 7 categories for climate (there is no lower montane dry soils) and 4 for geology (we do not take into consideration shale, sandstone, volcanic or unspecified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> 5.2.9.2 Climatic Zone feature engineering to group soils  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[\"Lower_Montane_Climate\"] = data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[23456]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Montane_Dry_Climate'] =data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[78]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Montane_Climate'] =data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[1][0123]$|Soil_Type[9]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Montane_Dry_and_Montane_Climate'] =data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[1][45]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Montante_and_Subalpine_Climate'] =data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[1][678]$\")].max(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Subalpine_Climate'] =data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type19$|^Soil_Type[2][0-9]$|^Soil_Type[3][0-4]$\")].max(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Alpine_Climate'] =data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[3][56789]$|Soil_Type40\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green> 5.2.9.2 Geological feature engineering to group soils  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The USFD, an American federal agency for forest service dependent on the department of agriculture has classified soil types according to climatic zone (first digit) and geology (second digit). Because of this, we believe a similar classification can be artificially engineered grouping all similar soils in 7 categories for climate (there is no lower montane dry soils) and 4 for geology (we do not take into consideration shale, sandstone, volcanic or unspecified because there are not existing in the dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Note:   First digit:  climatic zone             Second digit:  geologic zones\n",
    "                1.  lower montane dry                   1.  alluvium\n",
    "                2.  lower montane                       2.  glacial\n",
    "                3.  montane dry                         3.  shale\n",
    "                4.  montane                             4.  sandstone\n",
    "                5.  montane dry and montane             5.  mixed sedimentary\n",
    "                6.  montane and subalpine               6.  unspecified in the USFS ELU Survey\n",
    "                7.  subalpine                           7.  igneous and metamorphic\n",
    "                8.  alpine                              8.  volcanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Alluvium_Soil'] = data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[1][45679]$|^Soil_Type[2][01]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Glacial_Soil'] =data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[9]$|^Soil_Type[2][23]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Mixed_Sedimentary_Soil'] =data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[7-8]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Igneus_and_Metamorphic_Soil'] =data_train.loc[:,data_train.columns.str.contains(\"^Soil_Type[1-6]$|^Soil_Type[1][01238]$|^Soil_Type[3-4]\\d$|^Soil_Type[2][4-9]$\")].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the medium paper \"Preprocessing: Why you should generate polynominal features first before standardizing\" mention it is not good practice to standardize the variablesbefore before PolynominalFeatures. This should be done after to not loss the signal of the variables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and drop our target variable 'Cover_Type' from dataframe, isolating our independent variables\n",
    "X = data_train.drop('Cover_Type', axis = 1)\n",
    "\n",
    "# Isolate our dependent variable as a feature\n",
    "y = data_train['Cover_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split (70/30 size), drop duplicates and missing values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = .3, random_state = 33, stratify=y)\n",
    "\n",
    "X_train.drop_duplicates(inplace = True)\n",
    "X_train.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soil Type Groupings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Factorization\n",
    "\n",
    "The numerical values present a level of detail that may be much more fine-grained than we need. For instance, the soil level can be represented by different categories (soil family, complex or stony/rubberly). We aggregate the data up which can help to avoid overfitting when the data is more aggregate: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.6 Soil Type Family  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Discretization to bin the soil variable to the family type.<br>\n",
    "\n",
    "__Cathedral__ <br>\n",
    "1 Cathedral family - Rock outcrop complex, extremely stony.<br>\n",
    "\n",
    "__Ratake__ <br>\n",
    "2 Vanet - Ratake families complex, very stony.<br>\n",
    "4 Ratake family - Rock outcrop complex, rubbly.<br>\n",
    "\n",
    "__Vanet__<br>\n",
    "5 Vanet family - Rock outcrop complex complex, rubbly.<br>\n",
    "\n",
    "__Wetmore__<br>\n",
    "6 Vanet - Wetmore families - Rock outcrop complex, stony.<br>\n",
    "\n",
    "__Gothic__<br>\n",
    "7 Gothic family.<br>\n",
    "                    \n",
    "__Limber__ <br>\n",
    "8 Supervisor - Limber families complex. <br>\n",
    "\n",
    "__Troutville__<br>\n",
    "9 Troutville family, very stony.<br>\n",
    "\n",
    "__Legault__<br>\n",
    "12 Legault family - Rock land complex, stony.<br>\n",
    "29 Como - Legault families complex, extremely stony.<br>\n",
    "\n",
    "__Gateview__ <br>\n",
    "17 Gateview family - Cryaquolis complex.<br>\n",
    "\n",
    "__Rogert__<br>\n",
    "18 Rogert family, very stony.<br>\n",
    "\n",
    "\n",
    "__Como__<br>\n",
    "30 Como family - Rock land - Legault family complex, extremely stony.<br>\n",
    "\n",
    "__Bross__<br>\n",
    "36 Bross family - Rock land - Cryumbrepts complex, extremely stony.<br>\n",
    "\n",
    "\n",
    "\n",
    "__Catamount__<br>\n",
    "10 Bullwark - Catamount families - Rock outcrop complex, rubbly.<br>\n",
    "11 Bullwark - Catamount families - Rock land complex, rubbly.<br>\n",
    "13 Catamount family - Rock land - Bullwark family complex, rubbly.<br>\n",
    "26 Granile - Catamount families complex, very stony.<br>\n",
    "32 Catamount family - Rock outcrop - Leighcan family complex, extremely stony.<br>\n",
    "31 Leighcan - Catamount families complex, extremely stony.<br>\n",
    "33 Leighcan - Catamount families - Rock outcrop complex, extremely stony.<br>\n",
    "\n",
    "__Leighcan__<br>\n",
    "21 Typic Cryaquolls - Leighcan family, till substratum complex.<br>\n",
    "22 Leighcan family, till substratum, extremely bouldery.<br>\n",
    "23 Leighcan family, till substratum - Typic Cryaquolls complex.<br>\n",
    "24 Leighcan family, extremely stony.<br>\n",
    "25 Leighcan family, warm, extremely stony.<br>\n",
    "27 Leighcan family, warm - Rock outcrop complex, extremely stony.<br>\n",
    "28 Leighcan family - Rock outcrop complex, extremely stony.<br>\n",
    "\n",
    "__Moran__<br>\n",
    "38 Leighcan - Moran families - Cryaquolls complex, extremely stony.<br>\n",
    "39 Moran family - Cryorthents - Leighcan family complex, extremely stony.<br>\n",
    "40 Moran family - Cryorthents - Rock land complex, extremely stony.<br>\n",
    "\n",
    "__Others__<br> \n",
    "3 Haploborolis - Rock outcrop complex, rubbly.<br>\n",
    "15 unspecified in the USFS Soil and ELU Survey.<br>\n",
    "37 Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.<br>\n",
    "34 Cryorthents - Rock land complex, extremely stony.<br>\n",
    "35 Cryumbrepts - Rock outcrop - Cryaquepts complex.<br>\n",
    "20 Typic Cryaquepts - Typic Cryaquolls complex.<br>\n",
    "14 Pachic Argiborolis - Aquolis complex.<br>\n",
    "16 Cryaquolis - Cryoborolis complex.<br>\n",
    "19 Typic Cryaquolis - Borohemists complex.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soil Type\n",
    "family_soil_types = {\n",
    "    'Family_Cathedral': ['Soil_Type1'],\n",
    "    'Family_Retake': ['Soil_Type2', 'Soil_Type4'],\n",
    "    'Family_Vanet': ['Soil_Type5'],\n",
    "    'Family_Wetmore': ['Soil_Type6'],\n",
    "    'Family_Gothic': ['Soil_Type7'],\n",
    "    'Family_Limber': ['Soil_Type8'],\n",
    "    'Family_Troutville_': ['Soil_Type9'],\n",
    "    'Family_Legault': ['Soil_Type12', 'Soil_Type29'],\n",
    "    'Family_Gateview': ['Soil_Type17'],\n",
    "    'Family_Rogert': ['Soil_Type18'],\n",
    "    'Family_Como': ['Soil_Type30'],\n",
    "    'Family_Bross': ['Soil_Type36'],\n",
    "    'Family_Catamount': ['Soil_Type10','Soil_Type11','Soil_Type13','Soil_Type26','Soil_Type32','Soil_Type31','Soil_Type33'],\n",
    "    'Family_Leighcan': ['Soil_Type21','Soil_Type22','Soil_Type23','Soil_Type24','Soil_Type25','Soil_Type27','Soil_Type28'],\n",
    "    'Family_Moran': ['Soil_Type38','Soil_Type39','Soil_Type40'],\n",
    "    'Family_Others': ['Soil_Type3','Soil_Type15','Soil_Type37','Soil_Type34','Soil_Type35','Soil_Type20','Soil_Type14','Soil_Type16','Soil_Type19'],\n",
    "} \n",
    "\n",
    "for family in family_soil_types:\n",
    "    data_train[family] = 0\n",
    "    soil_types = family_soil_types[family]\n",
    "    for soil_type in soil_types:\n",
    "        data_train[family] += data_train[soil_type]\n",
    "\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.6 Soil Type Complex  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will group the soil types according to their family and according to the complex and stonyness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complex Group <br>\n",
    "__Rock_outcrop_complex__ <br>\n",
    "1 Cathedral family - Rock outcrop complex, extremely stony.<br>\n",
    "2 Vanet - Ratake families complex, very stony.<br>\n",
    "3 Haploborolis - Rock outcrop complex, rubbly.<br>\n",
    "4 Ratake family - Rock outcrop complex, rubbly.<br>\n",
    "5 Vanet family - Rock outcrop complex complex, rubbly.<br>\n",
    "6 Vanet - Wetmore families - Rock outcrop complex, stony.<br>\n",
    "10 Bullwark - Catamount families - Rock outcrop complex, rubbly.<br>\n",
    "27 Leighcan family, warm - Rock outcrop complex, extremely stony.<br>\n",
    "28 Leighcan family - Rock outcrop complex, extremely stony.<br>\n",
    "33 Leighcan - Catamount families - Rock outcrop complex, extremely stony.<br>\n",
    "\n",
    "__Ratake_families_complex__<br>\n",
    "2 Vanet - Ratake families complex, very stony.<br>\n",
    "\n",
    "\n",
    "__Limber families complex__<br>\n",
    "8 Supervisor - Limber families complex.<br>\n",
    "\n",
    "__rock land complex__<br>\n",
    "11 Bullwark - Catamount families - Rock land complex, rubbly.<br>\n",
    "12 Legault family - Rock land complex, stony.<br>\n",
    "34 Cryorthents - Rock land complex, extremely stony.<br>\n",
    "40 Moran family - Cryorthents - Rock land complex, extremely stony.<br>\n",
    "\n",
    "__Cryoborolis complex__<br>\n",
    "16 Cryaquolis - Cryoborolis complex.<br>\n",
    "17 Gateview family - Cryaquolis complex.<br>\n",
    "\n",
    "__Bullwark family complex__<br>\n",
    "13 Catamount family - Rock land - Bullwark family complex, rubbly.<br>\n",
    "\n",
    "__Aquolis complex__<br>\n",
    "14 Pachic Argiborolis - Aquolis complex.<br>\n",
    "\n",
    "__Borohemists complex__<br>\n",
    "19 Typic Cryaquolis - Borohemists complex.<br>\n",
    "\n",
    "__Cryaquolls complex__<br>\n",
    "20 Typic Cryaquepts - Typic Cryaquolls complex.<br>\n",
    "23 Leighcan family, till substratum - Typic Cryaquolls complex.<br>\n",
    "38 Leighcan - Moran families - Cryaquolls complex, extremely stony.<br>\n",
    "\n",
    "__till substratum complex__<br>\n",
    "21 Typic Cryaquolls - Leighcan family, till substratum complex.<br>\n",
    "\n",
    "__Catamount families complex__<br>\n",
    "26 Granile - Catamount families complex, very stony.<br>\n",
    "1 Leighcan - Catamount families complex, extremely stony.<br>\n",
    "31 Leighcan - Catamount families complex, extremely stony.<br>\n",
    "\n",
    "__Legault families complex__<br>\n",
    "29 Como - Legault families complex, extremely stony.<br>\n",
    "30 Como family - Rock land - Legault family complex, extremely stony.<br>\n",
    "\n",
    "__Leighcan family complex__<br>\n",
    "32 Catamount family - Rock outcrop - Leighcan family complex, extremely stony.<br>\n",
    "39 Moran family - Cryorthents - Leighcan family complex, extremely stony.<br>\n",
    "\n",
    "__Cryaquepts complex__<br>\n",
    "35 Cryumbrepts - Rock outcrop - Cryaquepts complex.<br>\n",
    "\n",
    "__Cryumbrepts complex__<br>\n",
    "36 Bross family - Rock land - Cryumbrepts complex, extremely stony.<br>\n",
    "\n",
    "__Cryorthents complex__<br>\n",
    "37 Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.<br>\n",
    "\n",
    "__others__ <br>\n",
    "7 Gothic family.<br>\n",
    "9 Troutville family, very stony.<br>\n",
    "22 Leighcan family, till substratum, extremely bouldery.<br>\n",
    "24 Leighcan family, extremely stony.<br>\n",
    "25 Leighcan family, warm, extremely stony.<br>\n",
    "18 Rogert family, very stony.<br>\n",
    "15 unspecified in the USFS Soil and ELU Survey.<br>\n",
    "\n",
    "\n",
    "Source: https://www.kaggle.com/competitions/forest-cover-type-prediction/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.6 Soil Type Stonyness <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex Type\n",
    "family_complex_types = {\n",
    "    'Rock_outcrop_complex': ['Soil_Type1','Soil_Type2','Soil_Type3','Soil_Type4','Soil_Type5','Soil_Type6','Soil_Type10','Soil_Type27','Soil_Type28','Soil_Type33'],\n",
    "    'Ratake_families_complex': ['Soil_Type2'],\n",
    "    'Limber_families_complex': ['Soil_Type8'],\n",
    "    'Rock_land_complex': ['Soil_Type11','Soil_Type12','Soil_Type34','Soil_Type40'],\n",
    "    'Cryoborolis_complex': ['Soil_Type16','Soil_Type17'],\n",
    "    'Bullwark_family_complex': ['Soil_Type13'],\n",
    "    'Aquolis_complex_': ['Soil_Type14'],\n",
    "    'Borohemists_complex': ['Soil_Type19'],\n",
    "    'Cryaquolls_complex': ['Soil_Type20','Soil_Type23','Soil_Type38'],\n",
    "    'Till_substratum_complex': ['Soil_Type21'],\n",
    "    'Catamount_families_complex': ['Soil_Type26','Soil_Type1','Soil_Type31'],\n",
    "    'Legault_families_complex': ['Soil_Type39','Soil_Type30'],\n",
    "    'Leighcan_family_complex': ['Soil_Type32','Soil_Type39'],\n",
    "    'Cryaquepts_complex': ['Soil_Type35'],\n",
    "    'Cryumbrepts_complex': ['Soil_Type36'],\n",
    "    'Cryorthents_complex': ['Soil_Type37'],\n",
    "    'others_complex': ['Soil_Type7','Soil_Type9','Soil_Type22','Soil_Type24','Soil_Type25','Soil_Type18','Soil_Type15'],\n",
    "} \n",
    "\n",
    "for family in family_complex_types:\n",
    "    data_train[family] = 0\n",
    "    complex_types = family_complex_types[family]\n",
    "    for complex_type in complex_types:\n",
    "        data_train[family] += data_train[complex_type]\n",
    "\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stony__ <br>\n",
    "1 Cathedral family - Rock outcrop complex, extremely stony.<br>\n",
    "2 Vanet - Ratake families complex, very stony.<br>\n",
    "6 Vanet - Wetmore families - Rock outcrop complex, stony.<br>\n",
    "9 Troutville family, very stony.<br>\n",
    "12 Legault family - Rock land complex, stony.<br>\n",
    "18 Rogert family, very stony.<br>\n",
    "24 Leighcan family, extremely stony.<br>\n",
    "25 Leighcan family, warm, extremely stony.<br>\n",
    "26 Granile - Catamount families complex, very stony.<br>\n",
    "27 Leighcan family, warm - Rock outcrop complex, extremely stony.<br>\n",
    "28 Leighcan family - Rock outcrop complex, extremely stony.<br>\n",
    "29 Como - Legault families complex, extremely stony.<br>\n",
    "30 Como family - Rock land - Legault family complex, extremely stony.<br>\n",
    "31 Leighcan - Catamount families complex, extremely stony.<br>\n",
    "32 Catamount family - Rock outcrop - Leighcan family complex, extremely stony.<br>\n",
    "33 Leighcan - Catamount families - Rock outcrop complex, extremely stony.<br>\n",
    "34 Cryorthents - Rock land complex, extremely stony.<br>\n",
    "36 Bross family - Rock land - Cryumbrepts complex, extremely stony.<br>\n",
    "37 Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.<br>\n",
    "38 Leighcan - Moran families - Cryaquolls complex, extremely stony.<br>\n",
    "39 Moran family - Cryorthents - Leighcan family complex, extremely stony.<br>\n",
    "40 Moran family - Cryorthents - Rock land complex, extremely stony.<br>\n",
    "\n",
    "__Rubbly__<br>\n",
    "3 Haploborolis - Rock outcrop complex, rubbly.<br>\n",
    "4 Ratake family - Rock outcrop complex, rubbly.<br>\n",
    "5 Vanet family - Rock outcrop complex complex, rubbly.<br>\n",
    "10 Bullwark - Catamount families - Rock outcrop complex, rubbly.<br>\n",
    "11 Bullwark - Catamount families - Rock land complex, rubbly.<br>\n",
    "13 Catamount family - Rock land - Bullwark family complex, rubbly.<br>\n",
    "\n",
    "__others__<br>\n",
    "7 Gothic family.<br>\n",
    "8 Supervisor - Limber families complex.<br>\n",
    "14 Pachic Argiborolis - Aquolis complex.<br>\n",
    "15 unspecified in the USFS Soil and ELU Survey.<br>\n",
    "16 Cryaquolis - Cryoborolis complex.<br>\n",
    "17 Gateview family - Cryaquolis complex.<br>\n",
    "19 Typic Cryaquolis - Borohemists complex.<br>\n",
    "20 Typic Cryaquepts - Typic Cryaquolls complex.<br>\n",
    "21 Typic Cryaquolls - Leighcan family, till substratum complex.<br>\n",
    "22 Leighcan family, till substratum, extremely bouldery.<br>\n",
    "23 Leighcan family, till substratum - Typic Cryaquolls complex.<br>\n",
    "35 Cryumbrepts - Rock outcrop - Cryaquepts complex.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soil Type\n",
    "family_types = {\n",
    "    'Type_Stony': ['Soil_Type1','Soil_Type2', 'Soil_Type6', 'Soil_Type9', 'Soil_Type12', 'Soil_Type18', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40'],\n",
    "    'Type_Rubbly': ['Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type10', 'Soil_Type11', 'Soil_Type13'],\n",
    "    'Type_Other': ['Soil_Type7','Soil_Type8', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type35']\n",
    "} \n",
    "\n",
    "for family in family_types:\n",
    "    data_train[family] = 0\n",
    "    soil_types = family_types[family]\n",
    "    for soil_type in soil_types:\n",
    "        data_train[family] += data_train[soil_type]\n",
    "\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 5.10 Summary <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th><b>Features</b></th>\n",
    "    <th><b>Transformation</b></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "<td>ID  </td>\n",
    "    <td> Drop</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Distance To Hydrology  </td>\n",
    "    <td><b><i>Square Root</i></b> of the length of the side of horizontal and vertical </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Horizontal Distance To Roadways</td>\n",
    "    <td><b>Square Root</b> of horizontal Distance to Roadways</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> Slope</td>\n",
    "    <td><b><i>Square Root</i></b> Slope</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> Horizontal_Distance To firepoints</td>\n",
    "    <td><b><i>Square Root</i></b> Horizontal Distance to firepoints</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Mean Hillshade</td>\n",
    "    <td><b><i>Box Cox Average</i></b> of all Hillshades features</td>\n",
    "  <tr>      \n",
    "  </tr>\n",
    "    <td>Hillshade 9am</td>\n",
    "    <td><b><i>Box Cox </i></b> Hillshade 9am</td>\n",
    "  <tr>      \n",
    "  </tr>\n",
    "    <td>Hillshade Noon</td>\n",
    "    <td><b><i>Box Cox </i></b> Hillshade Noon</td>\n",
    "  <tr>      \n",
    "  </tr>\n",
    "    <td>Hillshade 3pm</td>\n",
    "    <td><b><i>Box Cox</i></b> Hillshade 3pm</td>\n",
    "  <tr>      \n",
    "  </tr>\n",
    "        <td>Aspect</td>\n",
    "    <td><b><i>Square Root</i></b> Aspect</td>\n",
    "  <tr>      \n",
    "  </tr>\n",
    "    <td>Aspect North, East,South and West</td>\n",
    "    <td><b><i>Grouping</i></b> Aspect</td>\n",
    "  <tr>      \n",
    "  </tr>\n",
    "    <td>Geological Grouping</td>\n",
    "    <td><b><i>Grouping</i></b> Soil Types</td>\n",
    "  <tr>      \n",
    "  </tr>\n",
    "    <td>Climate Grouping</td>\n",
    "    <td><b><i>Grouping</i></b> Soil Types</td>\n",
    "  <tr>      \n",
    "  </tr>\n",
    "     <td>Soil Family</td>\n",
    "    <td><b><i>Grouping</i></b> Soil Families</td>\n",
    "  <tr>      \n",
    "  </tr>\n",
    "     <td>Soil Type Complex</td>\n",
    "    <td><b><i>Grouping</i></b> Soil Complex</td>\n",
    "  <tr>      \n",
    "  </tr> \n",
    "     <td>Soil Type Stonyness</td>\n",
    "    <td><b><i>Grouping</i></b> by Soil stonyness</td>\n",
    "  <tr>      \n",
    "  </tr>     \n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=green> 6.Feature Selection <font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to use several feature selection algorithms where we use them in combination of all the different selection method and will take the best score of all the used common algorithms score. Source: https://towardsdatascience.com/the-5-feature-selection-algorithms-every-data-scientist-need-to-know-3a6b566efd2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locking all features in a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only X_Train replacement\n",
    "data_train.to_csv('all_features_data_train.csv')\n",
    "data_train.to_csv('all_features_data_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green> 6.1. Normalization <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the most useful features to train the model can improve the performance of our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset to train and validation set, in order to test our models. We use stratify to have a balanced datset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_train.drop(['Cover_Type'], axis=1)\n",
    "y = data_train['Cover_Type']\n",
    "column_list = X.columns\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=42,stratify=y)\n",
    "print(\"The shape of validation data:{} and {} \".format(X_val.shape,y_val.shape))\n",
    "print(\"The shape of training data:{} and {} \".format(X_train.shape,y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the standardization we need only numerical values, since these has been aleady encoded we use the names to filter out the dummy variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_numerical  = [column for column in column_list if 'Soil' not in column and 'Wilderness_Area' not in  column and 'Aspect_North' not in  column and 'Climate' not in  column and 'Family' not in  column and 'Type' not in  column and 'complex' not in  column and 'Aspect_East' not in  column and 'Aspect_South' not in  column and 'Aspect_West' not in  column ]\n",
    "scale_categorial= [column for column in column_list if column not in scale_numerical ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only the dummy variables filtered \n",
    "data_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_train = data_train.filter(items=scale_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorial_train = data_train.filter(items=scale_categorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#scaler = StandardScaler()\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[scale_numerical] = scaler.fit_transform(X_train[scale_numerical])\n",
    "X_val[scale_numerical] = scaler.fit_transform(X_val[scale_numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green> 6.2. Number of feature selection  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Permutation Feature Importance__\n",
    "\n",
    "Permutation feature importance is a technique for calculating relative importance scores that is independent of the model used.\n",
    "\n",
    "First, a model is fit on the dataset, such as a model that does not support native feature importance scores. Then the model is used to make predictions on a dataset, although the values of a feature (column) in the dataset are scrambled. This is repeated for each feature in the dataset. Then this whole process is repeated 3, 5, 10 or more times. The result is a mean importance score for each input feature (and distribution of scores given the repeats).\n",
    "\n",
    "Permutation feature selection can be used via the permutation_importance() function that takes a fit model, a dataset (train or test dataset is fine), and a scoring function.\n",
    "\n",
    "Let’s take a look at this approach to feature selection with an algorithm that does not support feature selection natively, specifically k-nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permutation feature importance with knn for classification\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=50, n_informative=5, n_redundant=5, random_state=1)\n",
    "# define the model\n",
    "model = KNeighborsClassifier()\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# perform permutation importance\n",
    "results = permutation_importance(model, X, y, scoring='accuracy')\n",
    "# get importance\n",
    "importance = results.importances_mean\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "fig, ax = plt.subplots(figsize=(40,20)) \n",
    "# plot feature importance\n",
    "ax = pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no of maximum features we need to select\n",
    "num_feats=46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first fit a linear model to the initial dataset to have a baseline to evaluate the data cleaning and feature engineering impact.\n",
    "\n",
    "To facilitate the training process, we will use the `sklearn` library <https://scikit-learn.org/stable/index.html> that provides a wrapper for the preprocessing, training, and evaluation of many machine learning algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "initial_lm_mod = linear_model.LogisticRegression(multi_class='multinomial',\n",
    "   max_iter=3000, penalty='none')\n",
    "\n",
    "#initial_lm_mod = RandomForestRegressor(n_estimators=150)\n",
    "baseline_acc = np.mean(\n",
    "    cross_val_score(initial_lm_mod, X_train,y_train, cv=5))\n",
    "\n",
    "print(f\"Baseline model with Accuracy = {baseline_acc:.4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance(clf, feature_names):\n",
    "    \"\"\"\n",
    "    Function to print the most important features of a logreg classifier\n",
    "    based on the coefficient values\n",
    "    \"\"\"\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            'variable': feature_names, # Feature names\n",
    "            'coefficient': clf.coef_[0] # Feature Coeficients\n",
    "        }\n",
    "    ) \\\n",
    "    .round(decimals=2) \\\n",
    "    .sort_values('coefficient', ascending=False) \\\n",
    "    .style.bar(color=['red', 'green'], align='zero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_feature_importance(\n",
    "    initial_lm_mod.fit(X_val,y_val), \n",
    "    X_train.columns.get_level_values(0).tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green> Embedded Method <font>\n",
    "## <font color=green> 6.4. Ridge Regularization <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with Ridge (or L2) regularization.<br> We are going to make use of the Ridge Model in sklearn https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification. WE use Ridge instead of Lasso as we have a big number of predictors and for this case Ridge penalizes more when x is big and gives a more robust result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_mod = linear_model.LogisticRegression(multi_class='multinomial', max_iter=10000,penalty='l2')\n",
    "print(\"Accuracy = {:.4}\".format(np.mean(cross_val_score(ridge_mod, X_train, y_train, cv=5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_feature_importance(ridge_mod.fit(X_train,y_train), X_train.columns.get_level_values(0).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar performance w.r.t the un-regularized models. However, you can see how the feature coefficients are smaller than the original ones, due to the regularization.\n",
    "\n",
    "Let's look at how the coefficient weights and accuracy scores change along with the different regularization values.\n",
    "To that end, I have implemented the following piece of code. Do not be overwhelmed by it. It basically defines a list of regularization values to test and train a new Logistic Regression model for one of these regularization values. We keep track of the coefficient values and the accuracy of each of these models to plot them according to the defined regularization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a basic Logisitic Regresion Model that applies L2 (Ridge) regularization\n",
    "ridge_mod = linear_model.LogisticRegression(multi_class='multinomial',max_iter=10000,penalty='l2')\n",
    "\n",
    "# Define a list of 100 regularization values to test (from 0.1 to 0.0001)\n",
    "alphas = 10**np.linspace(-1,-4,100)\n",
    "\n",
    "coefs_ = [] # Array to store the value of the coefficients for each model\n",
    "scores_ = [] # Array to store the accuracy for each model\n",
    "\n",
    "# Go over the regularization values list defined above, train a logreg model for each of the regularization values and evaluate it.\n",
    "for a in alphas:\n",
    "    ridge_mod.set_params(C=a) # Set the regularization parameter \n",
    "    scores_.append(np.mean(cross_val_score(ridge_mod, X_train, y_train, cv=5))) # Appends the accuracy of the model\n",
    "    coefs_.append(ridge_mod.fit(X_train, y_train).coef_.ravel().copy()) # Appends the coefficient of the model\n",
    "\n",
    "# Conver the coefficient and scores arrays to numpy arrays\n",
    "coefs_ = np.array(coefs_)\n",
    "scores_ = np.array(scores_)\n",
    "\n",
    "# Define the figures to plot the values\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "fig.suptitle('Logistic Regression Path', fontsize=20)\n",
    "\n",
    "# Coeff Weights Plot\n",
    "ax1.plot(alphas, coefs_, marker='o')\n",
    "ymin, ymax = plt.ylim()\n",
    "ax1.set_ylabel('Coefficient Weights', fontsize = 15)\n",
    "ax1.set_xlabel('Alpha', fontsize = 15)\n",
    "ax1.axis('tight')\n",
    "\n",
    "# Accuracy Plot\n",
    "ax2.plot(alphas, scores_, marker='o')\n",
    "ymin, ymax = plt.ylim()\n",
    "ax2.set_ylabel('Accuracy Score', fontsize = 15)\n",
    "ax2.set_xlabel('Alpha', fontsize = 15)\n",
    "ax2.axis('tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the left figure, the smaller the alpha value (alpha), the larger the regularization and, consequently, the smaller the weights of the coefficients. This is because, if we check the sklearn documentation, we will see that this value is the: \"Inverse of regularization strength.\"\n",
    "When regularization is large enough (i.e., alpha is small), the values of the coefficients are close to 0 (i.e., null model).\n",
    "As there is a trade-off between variance (i.e., less over-fitted model --> more regularization) and bias (i.e., learning more from the training set --> less regularization), You must find the optimal alpha value. As you can see in the right figure, this value is achieved with small alpha values (i.e., more regularization). This specific value is not always the same since it depends on your data and the prediction problem.\n",
    "To automatize the process of finding the optimal value, you can make use of the LogisticRegressionCV function in sklearn (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html) that performs CV, testing different hyperparameters (that you can provide) and selecting the optimal one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "embeded_lr_selector = SelectFromModel(LogisticRegression(C=0.1, penalty=\"l2\",max_iter=1000), max_features=num_feats)\n",
    "embeded_lr_selector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_lr_support = embeded_lr_selector.get_support()\n",
    "embeded_lr_feature = X_train.loc[:,embeded_lr_support].columns.tolist()\n",
    "print(str(len(embeded_lr_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_lr_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green> 6.3. Filter Method <font>\n",
    "### <font color=green> 6.3.1 Chi-squared Selection <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code from class Forum, select the best features \n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "skb_selector=SelectKBest(score_func = chi2, k=num_feats)\n",
    "# Select the top most important features\n",
    "skb_selector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indexes for columns selected\n",
    "skb_support = skb_selector.get_support()\n",
    "# Get  columns from original dataframe\n",
    "skb_feature = X_train.iloc[:,skb_support].columns.tolist()\n",
    "print(str(len(skb_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skb_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green> 6.3. Filter Method <font>\n",
    "### <font color=green> 6.5. Anova F-value <font>\n",
    "It is a univariate filter method that uses variance to find out the separability of the individual features between classes. \n",
    "It applies to multi-class endpoints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "anov_selector = SelectKBest(f_classif, k=num_feats)\n",
    "anov_selector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anov_support = anov_selector.get_support()\n",
    "# Get  columns from original dataframe\n",
    "anov_feature = X_train.iloc[:,anov_support].columns.tolist()\n",
    "print(str(len(anov_feature)), 'selected features')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green> 6.3. Filter Method <font>\n",
    "### <font color=green> 6.5. Pearson correlation <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from yellowbrick.target import FeatureCorrelation\n",
    "\n",
    "#X, y = data_train['data'], data_train['Cover_Type']\n",
    "\n",
    "# Create a list of the feature names\n",
    "#features = np.array(data['feature_names'])\n",
    "fig, ax = plt.subplots(figsize=(10,40))         # Sample figsize in inches\n",
    "# Instantiate the visualizer\n",
    "visualizer = FeatureCorrelation(labels=None,sort=True)\n",
    "\n",
    "ax = visualizer.fit(X_train, y_train)        # Fit the data to the visualizer\n",
    "visualizer.show()           # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data_train.corrwith(data_train[\"Cover_Type\"])\n",
    "X_y = data_train.copy()\n",
    "X_y['Cover_Type'] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,50))         # Sample figsize in inches\n",
    "\n",
    "corr_matrix = X_y.corr()\n",
    "\n",
    "# Isolate the column corresponding to `exam_score`\n",
    "corr_target = corr_matrix[['Cover_Type']].drop(labels=['Cover_Type'])\n",
    "\n",
    "sns.heatmap(corr_target, annot=True, fmt='.3', cmap='RdBu_r',ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor_selector(X, y,num_feats):\n",
    "    cor_list = []\n",
    "    feature_name = X.columns.tolist()\n",
    "    # calculate the correlation with y for each feature\n",
    "    for i in X.columns.tolist():\n",
    "        cor = np.corrcoef(X[i], y)[0, 1]\n",
    "        cor_list.append(cor)\n",
    "    # replace NaN with 0\n",
    "    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n",
    "    # feature name\n",
    "    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()\n",
    "    # feature selection? 0 for not select, 1 for select\n",
    "    cor_support = [True if i in cor_feature else False for i in feature_name]\n",
    "    return cor_support, cor_feature\n",
    "cor_support, cor_feature = cor_selector(X_train, y_train,num_feats)\n",
    "print(str(len(cor_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green> 6.5. Recursive Feature Elimination <font>\n",
    "\n",
    "The goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "rfe_selector = RFE(estimator=LogisticRegression(max_iter=3000), n_features_to_select=num_feats, step=10, verbose=5)\n",
    "rfe_selector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_support = rfe_selector.get_support()\n",
    "rfe_feature = X_train.loc[:,rfe_support].columns.tolist()\n",
    "print(str(len(rfe_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green> 6.6. Tree-based: SelectFromModel <font>\n",
    "### <font color=green> 6.6.1 RandomForestClassifier<font>\n",
    "Embedded methods use algorithms that have built-in feature selection methods. We can also use RandomForest to select features based on feature importance. We calculate feature importance using node impurities in each decision tree. In Random forest, the final feature importance is the average of all decision tree feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=num_feats)\n",
    "embeded_rf_selector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_rf_support = embeded_rf_selector.get_support()\n",
    "embeded_rf_feature = X_train.loc[:,embeded_rf_support].columns.tolist()\n",
    "print(str(len(embeded_rf_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_rf_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 6.6.2 XgBoost <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is relatively straightforward to retrieve importance scores for each attribute.\n",
    "\n",
    "Generally, importance provides a score that indicates how useful or valuable each feature was in the construction of the boosted decision trees within the model. The more an attribute is used to make key decisions with decision trees, the higher its relative importance.This importance is calculated explicitly for each attribute in the dataset, allowing attributes to be ranked and compared to each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#y_train needs to be transformed from 1,2,3,4,5,6,7 to 0 1 2 3 4 5,6\n",
    "le = LabelEncoder()\n",
    "y_train1 = le.fit_transform(y_train)\n",
    "\n",
    "model=xgb.XGBClassifier(learning_rate=0.1,n_estimators = 400,max_depth = 3)\n",
    "\n",
    "embeded_xgb_selector = SelectFromModel(model, max_features=num_feats)\n",
    "embeded_xgb_selector.fit(X_train, y_train1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_xgb_support = embeded_xgb_selector.get_support()\n",
    "embeded_xgb_feature = X_train.loc[:,embeded_xgb_support].columns.tolist()\n",
    "print(str(len(embeded_xgb_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_xgb_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green> 6.6.2 ExtraTreesClassifier <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Decision Tree in the Extra Trees Forest is constructed from the original training sample. Then, at each test node, Each tree is provided with a random sample of k features from the feature-set from which each decision tree must select the best feature to split the data based on some mathematical criteria (typically the Gini Index). This random sample of features leads to the creation of multiple de-correlated decision trees.\n",
    "To perform feature selection using the above forest structure, during the construction of the forest, for each feature, the normalized total reduction in the mathematical criteria used in the decision of feature of split (Gini Index if the Gini Index is used in the construction of the forest) is computed. This value is called the Gini Importance of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "extra_tree_model= ExtraTreesClassifier()\n",
    "# Training the model\n",
    "extra_tree_forest_selector = SelectFromModel(ExtraTreesClassifier(max_features=num_feats,criterion='gini'))\n",
    "extra_tree_forest_selector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_tree_model.fit(X_train,y_train)\n",
    "extra_tree_model.feature_importances_\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(extra_tree_model.feature_importances_, index=X_train.columns)\n",
    "feat_importances.nlargest(num_feats).plot(kind='barh', colormap = 'rainbow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_tree_forest_support = extra_tree_forest_selector.get_support()\n",
    "extra_tree_forest_feature = X_train.loc[:,extra_tree_forest_support].columns.tolist()\n",
    "print(str(len(extra_tree_forest_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn \n",
    "chi2_score = pd.DataFrame(list(zip(X_train.columns, skb_selector.scores_,skb_selector.pvalues_)), columns = ['feature','score','pvalue'])\n",
    "chi2_score.sort_values('score', ascending = False)\n",
    "\n",
    "chi2_score.style.set_precision(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green> 6.6. Score of all methods together  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "# put all selection together\n",
    "feature_selection_df = pd.DataFrame({'Feature':feature_name,'ExtraTree':extra_tree_forest_support,'Kbest_Chi2':skb_support, 'Pearson':cor_support, 'RFE':rfe_support,'Anova':anov_support, 'Logistics':embeded_lr_support,\n",
    "                                    'Random Forest':embeded_rf_support, 'XGB':embeded_xgb_support})\n",
    "# count the selected times for each feature\n",
    "feature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\n",
    "# display the top 100\n",
    "feature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\n",
    "feature_selection_df.index = range(1, len(feature_selection_df)+1)\n",
    "feature_selection_df.head(num_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression \n",
    "Esemble Learning and Random Forest \n",
    "Decision Trees \n",
    "Random Forest (XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [(feature_selection_df['Total']>4)]\n",
    " \n",
    "new_df = feature_selection_df.loc[feature_selection_df['Total'] >4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = new_df['Feature'].to_list()\n",
    "feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Since we want to add all the features of the dummy variale family and climate, we will remove them first. we also remove complex and soil for the time being and see how the model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = [column for column in feature_list if 'Soil' not in column and 'Type' not in  column and 'complex' not in  column and 'Family' not in column and 'Climate' not in column ]\n",
    "new\n",
    "missing = (data_train.filter(regex='Family').columns)\n",
    "missing2 = (data_train.filter(regex='Climate').columns)\n",
    "missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trial=pd.concat([data_train[new], data_train[missing],data_train[missing2]])\n",
    "y_trial = data_train['Cover_Type']\n",
    "X_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only X_Train replacement\n",
    "X_trial.to_csv('X_trial.csv')\n",
    "y_trial.to_csv('y_trial.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selected = data_train[feature_list]\n",
    "y_selected = data_train['Cover_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only X_Train replacement\n",
    "X_selected.to_csv('X_selected.csv')\n",
    "y_selected.to_csv('y_selected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X_selected1 = pd.read_csv(\"X_selected.csv\")\n",
    "y_selected1 = pd.read_csv(\"y_selected.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selected1 = X_selected[X_selected.columns.drop(list(X_selected.filter(regex='Unnamed:')))]\n",
    "y_selected1 = y_selected[y_selected.columns.drop(list(y_selected.filter(regex='Unnamed:')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_selected.shape)\n",
    "print(y_selected.shape)\n",
    "\n",
    "if X.shape[0] != y.shape[0]:\n",
    "  print(\"X and y rows are mismatched, check dataset again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = X_selected.columns\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_selected, y_selected, test_size=0.20, random_state=42,stratify=y)\n",
    "print(\"The shape of validation data:{} and {} \".format(X_val.shape,y_val.shape))\n",
    "print(\"The shape of training data:{} and {} \".format(X_train.shape,y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_selected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=green> 7 Re-run models with the new selected features  <font>\n",
    "## <font color=green> Dataset split on the new features  <font>\n",
    "* Decision Trees\n",
    "* Random Forest (XGB Boost)\n",
    "* KNN\n",
    "* Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Correlation among features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap \n",
    "matrix = X_selected.corr()\n",
    "mask = np.zeros_like(matrix)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "fig, ax = plt.subplots(figsize=(50,20))\n",
    "heatmap = sns.heatmap(matrix, center=0, fmt=\".3f\", square=True, annot=True, linewidth=1.3, mask = mask,vmax=0.9);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new,X_test_new,y_val_new, y_val_new = train_test_split(X_selected, y_selected, test_size=0.2,random_state=23, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green> 6.6. Dimensionality reduction with PCA <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principle components capture most of the variance of the data. The first principle components hold the most variance in the data, Each subsequent PCS is orthogonal to the last and has a lesser variance. In this way, given a set of x correlated variables over y samples, you achieve a set of uncorrelated PCS over the same y samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://plotly.com/python/pca-visualization/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default='notebook'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# First we will observe with a 3D Diagram the first threee PCAs\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "components = pca.fit_transform(X_train)\n",
    "\n",
    "total_var = pca.explained_variance_ratio_.sum() * 100\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    components, x=0, y=1, z=2, color=y_train,\n",
    "    title=f'Total Explained Variance: {total_var:.2f}%',\n",
    "    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Run PCA that holds 95% of all variance of the data \n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "x_fit = pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d=np.argmax(cumsum>=0.98)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at the two first PCA to see if these can explain the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can set the components to the ratio of variance you wish to preserve"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a bar chart where the height of each bar is the percentage of variance explained by the associated PC. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "var = pca.explained_variance_[0:10] #percentage of variance explained\n",
    "labels = ['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10']\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.bar(labels,var,color=['purple'])\n",
    "plt.xlabel('Pricipal Component')\n",
    "plt.ylabel('Proportion of Variance Explained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cumsum)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "exp_var_cumul = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "px.area(\n",
    "    x=range(1, exp_var_cumul.shape[0] + 1),\n",
    "    y=exp_var_cumul,\n",
    "    labels={\"x\": \"# Components\", \"y\": \"Explained Variance\"})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On top of that, we can also look at the combinations of variables that created each principal component with pca.components_**2. We could use a heat map to showcase this"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Heatmap PCA\n",
    "fig, ax = plt.subplots(figsize=(40,20)) \n",
    "col_name = ['x' + str(idx) for idx in range(0, X_train.shape[1])]\n",
    "\n",
    "_ = sns.heatmap(pca.components_**2,\n",
    "                 yticklabels=[\"PCA\"+str(x) for x in range(1,pca.n_components_+1)],\n",
    "                 xticklabels=list(col_name),\n",
    "                 annot=True,\n",
    "                 fmt='.2f',\n",
    "                 square=True,\n",
    "                 linewidths=0.05,\n",
    "                 cbar_kws={\"orientation\": \"horizontal\"})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Convert back to DataFrame for readability\n",
    "df_pca = pd.DataFrame(data=x_fit)\n",
    "df_pca.columns = ['PC' + str(col+1) for col in df_pca.columns.values]\n",
    "df_pca['label'] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://towardsdatascience.com/principal-component-analysis-pca-explained-visually-with-zero-math-1cbf392b9e7d"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Plot Principal Component\n",
    "_ = sns.set(style='ticks', font_scale=1.2)\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "_ = sns.scatterplot(data=df_pca, x='PC1', y='PC2', hue=df_pca['label'], palette=['red', 'green', 'blue'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.codecademy.com/article/fe-filter-methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
