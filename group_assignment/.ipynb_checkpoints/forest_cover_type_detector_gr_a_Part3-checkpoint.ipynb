{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ml2_group_assignment.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=green> Introduction </font>\n",
    "\n",
    "This is a continuation of forest_cover_type_detector_gr_a_Part2.\n",
    "\n",
    "Above we import the files created in the previous notebook so that this notebook can run independently\n",
    "\n",
    "Table of contents is an extension of the previous notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tree_types.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=green> Table of contents </font>\n",
    "\n",
    "* [Libaries used](#0)\n",
    "* [1. Import Data](#1)\n",
    "  * [1.1.Original Data & Standardization](#1.1)\n",
    "  * [1.2.All features & Standardization](#1.2)  \n",
    "  * [1.3.Features selected & Standardization](#1.3)   \n",
    "  \n",
    "* [2.__Rerun the model on selected Features__](#2)  \n",
    "  * [2.1.Correlation Heatmap](#2.1)\n",
    "      * [2.1.1 Removing correlated features](#2.1.1)\n",
    "      \n",
    "* [__7. ML Algorithms after feature selection__](#7)\n",
    "  * [7.1 Decision Trees](#7.1)\n",
    "      * [7.1.1 Single Tree](#7.1.1)\n",
    "  * [7.2 XGBoost](#7.2)  \n",
    "  * [7.3 Extra Tree Classifier](#7.3)\n",
    "  * [7.4 Random Forest](#7.4)    \n",
    "      * [7.4.1 Bagging](#7.1.1)  \n",
    "  * [7.5 KNN](#7.5)\n",
    "  * [7.6 SVM](#7.6)\n",
    "  * [7.7 Naive Bayes](#7.7)\n",
    "  * [7.8 Logistic Regression](#7.8)\n",
    "  * [7.9 Ensemble Methods](#7.4)\n",
    "  \n",
    "  \n",
    "* [__8. ML Algorithms after Dimensionality Reduction__](#8)\n",
    "  * [8.1 PCA Dimensionality reduction ](#8.1)\n",
    "      * [8.1.1 Random Forest](#8.1.1)\n",
    "      * [8.1.2 XGBoost](#8.1.2)      \n",
    "      * [8.1.3 Logistic Regression](#8.1.3)     \n",
    "      * [8.1.4 Extra Tree Classifier](#8.1.4)\n",
    "      * [8.1.5 Ensemble Methods](#8.1.5)      \n",
    "  * [8.2 LDA Dimensionality reduction ](#8.2)      \n",
    "  \n",
    "* [9. Final Submission](#9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "# <font color=green> Libraries used </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install squarify\n",
    "#!pip install GraphViz\n",
    "#pip install pygraphviz\n",
    "#!pip install pydotplus\n",
    "#!pip install xgboost\n",
    "#!pip install dtreeviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.9.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import seaborn as sns\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "import pydotplus\n",
    "import xgboost as xgb\n",
    "import matplotlib\n",
    "import squarify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ks/5bc1x9p158vgc4774v7r2tq40000gn/T/ipykernel_794/627927963.py:66: DeprecationWarning:\n",
      "\n",
      "Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.rendered_html { font-size: 16px; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report,plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, learning_curve, ShuffleSplit\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve, ShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB #BernoulliNB is designed for binary/boolean features\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "from dtreeviz.trees import *\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier \n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from IPython.display import Image  \n",
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import Image  \n",
    "\n",
    "from scipy.stats import uniform, randint\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "from yellowbrick.classifier import ClassPredictionError\n",
    "from yellowbrick.style.palettes import PALETTES, SEQUENCES, color_palette\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\")) # Increase cell width\n",
    "display(HTML(\"<style>.rendered_html { font-size: 16px; }</style>\")) # Increase font size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "#  <font color=darkgreen>1.Import the Data </font>\n",
    "<a id='1.1'></a>\n",
    "###  <font color=green>1.1. Original Data </font>\n",
    "Letâ€™s load the previous data and results from previous notebook 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(\"test.csv\")\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(565892, 55)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = data_train.columns\n",
    "num  = [column for column in column_list if 'Soil' not in column and 'Wilderness_Area' not in  column and 'Aspect_North' not in  column and 'Climate' not in  column and 'Family' not in  column and 'Type' not in  column and 'complex' not in  column and 'Aspect_East' not in  column and 'Aspect_South' not in  column and 'Aspect_West' not in  column ]\n",
    "cat= [column for column in column_list if column not in num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.2'></a>\n",
    "## <font color=green> 1.2.All features & Standardization  <font>\n",
    " \n",
    "Due to excel import, it transforms a new column unnamed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "all_feat_df = pd.read_csv(\"all_features_data_train.csv\")\n",
    "all_feat_df = all_feat_df[all_feat_df.columns.drop(list(all_feat_df.filter(regex='Unnamed:')))]\n",
    "pd.set_option('display.max_columns', None)\n",
    "all_feat_df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "all_feat_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 55 features, we engineered a total of 165 additional ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For scaling we need to exclude the dummy variables "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "column_list = all_feat_df.columns\n",
    "numerical  = [column for column in column_list if 'Soil' not in column and 'Wilderness_Area' not in  column and 'Aspect_North' not in  column and 'Climate' not in  column and 'Family' not in  column and 'Type' not in  column and 'complex' not in  column and 'Aspect_East' not in  column and 'Aspect_South' not in  column and 'Aspect_West' not in  column ]\n",
    "categorial= [column for column in column_list if column not in numerical]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_all = all_feat_df.drop(['Cover_Type'], axis=1)\n",
    "y_all = all_feat_df['Cover_Type']\n",
    "column_list = X_all.columns\n",
    "\n",
    "X_train_all, X_val_all, y_train_all, y_val_all = train_test_split(X_all, y_all, test_size=0.20, random_state=37,stratify=y_all)\n",
    "print(\"The shape of validation data:{} and {} \".format(X_val_all.shape,y_val_all.shape))\n",
    "print(\"The shape of training data:{} and {} \".format(X_train_all.shape,y_train_all.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.3'></a>\n",
    "##  <font color=green>1.3. Selected Model after Feature Selection & Standardization </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selected features from the feature selection are transferred to cvs and used for further analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selected1 = pd.read_csv(\"X_selected.csv\")\n",
    "y_selected1 = pd.read_csv(\"y_selected.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selected1 = X_selected1[X_selected1.columns.drop(list(X_selected1.filter(regex='Unnamed:')))]\n",
    "y_selected1 = y_selected1[y_selected1.columns.drop(list(y_selected1.filter(regex='Unnamed:')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15120, 28)\n",
      "(15120, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_selected1.shape)\n",
    "print(y_selected1.shape)\n",
    "\n",
    "if X_selected1.shape[0] != y_selected1.shape[0]:\n",
    "  print(\"X and y rows are mismatched, check dataset again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to filter out the dummy variables for the normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = X_selected1.columns\n",
    "scale_numerical  = [column for column in column_list if 'Soil' not in column and 'Wilderness_Area' not in  column and 'Aspect_North' not in  column and 'Climate' not in  column and 'Family' not in  column and 'Type' not in  column and 'complex' not in  column and 'Aspect_East' not in  column and 'Aspect_South' not in  column and 'Aspect_West' not in  column ]\n",
    "scale_categorial= [column for column in column_list if column not in scale_numerical ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#scaler = MinMaxScaler()\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into train and validation test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of validation data:(3024, 28) and (3024, 1) \n",
      "The shape of training data:(12096, 28) and (12096, 1) \n"
     ]
    }
   ],
   "source": [
    "column_list = X_selected1.columns\n",
    "\n",
    "X_train_new, X_val_new, y_train_new, y_val_new = train_test_split(X_selected1, y_selected1, test_size=0.20, random_state=37,stratify=y_selected1)\n",
    "print(\"The shape of validation data:{} and {} \".format(X_val_new.shape,y_val_new.shape))\n",
    "print(\"The shape of training data:{} and {} \".format(X_train_new.shape,y_train_new.shape))\n",
    "y_val_new = y_val_new.values.ravel()\n",
    "y_train_new = y_train_new.values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "# <font color=darkgreen> 2.Re-run models with the new selected features  <font>\n",
    "Some classes such as SDG classifier , Random Forest classifier and naive Bayes classifier can handle mutliple classes naively. \n",
    "    \n",
    "Others like logistic regression or Support Vector Machine classifier are stricly binary classifier. However there are various strategies to perform multiclass classification with multiple binary classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataFrame to compare performance of Classifier Models in the End\n",
    "classifiers_compare = pd.DataFrame(columns =['Algorithm','Mean CV Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.1'></a>\n",
    "## <font color=green> 2.1 Correlation Heatmap  <font>\n",
    "Checking Correlation among features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Heatmap - Checking for autocorrelation among variables \n",
    "matrix = X_selected1.corr()\n",
    "mask = np.zeros_like(matrix)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "fig, ax = plt.subplots(figsize=(50,20))\n",
    "heatmap = sns.heatmap(matrix, center=0, fmt=\".3f\", square=True, annot=True, linewidth=1.3, mask = mask,vmax=0.9);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.1.1'></a>\n",
    "## <font color=green> 2.1.1 Removing autocorrelation <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these features are heavily correlated one another, before running the models like Logistic Regression, where mutlicorrelaty is an issue, we should ensure, only relevant features are considered, taking a threshold of 0.7 these are:\n",
    "\n",
    "- Wilderness Area 1 \n",
    "- Subalpine Climate (synthetic)\n",
    "- Wilderness Area 3\n",
    "- Elevation\n",
    "- Family Moran (synthetic)\n",
    "- Soil Type 3\n",
    "- Soil Type 12\n",
    "- Rock outcrop complex (synthetic)\n",
    "- Lower Montane Climate (synthetic)\n",
    "- Family Catamount (synthetic)\n",
    "\n",
    "TOTAL: 11 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for logistic regression, Stephanie will decide if we keep it or not eventually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_collinear_features(x, threshold):\n",
    "    '''\n",
    "    Objective:\n",
    "        Remove collinear features in a dataframe with a correlation coefficient\n",
    "        greater than the threshold. Removing collinear features can help a model \n",
    "        to generalize and improves the interpretability of the model.\n",
    "\n",
    "    Inputs: \n",
    "        x: features dataframe\n",
    "        threshold: features with correlations greater than this value are removed\n",
    "\n",
    "    Output: \n",
    "        dataframe that contains only the non-highly-collinear features\n",
    "    '''\n",
    "\n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = x.corr()\n",
    "    iters = range(len(corr_matrix.columns) - 1)\n",
    "    drop_cols = []\n",
    "\n",
    "    # Iterate through the correlation matrix and compare correlations\n",
    "    for i in iters:\n",
    "        for j in range(i+1):\n",
    "            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n",
    "            col = item.columns\n",
    "            row = item.index\n",
    "            val = abs(item.values)\n",
    "\n",
    "            # If correlation exceeds the threshold\n",
    "            if val >= threshold:\n",
    "                # Print the correlated features and the correlation value\n",
    "                #print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n",
    "                drop_cols.append(col.values[0])\n",
    "\n",
    "    # Drop one of each pair of correlated columns\n",
    "    drops = set(drop_cols)\n",
    "    x = x.drop(columns=drops)\n",
    "    print('Removed Columns {}'.format(drops))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Collinearity features for logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logist = remove_collinear_features(X_selected1, 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_log, X_val_log, y_train_log, y_val_log = train_test_split(df_logist, y_selected1, test_size=0.20, random_state=37,stratify=y_selected1)\n",
    "print(\"The shape of validation data:{} and {} \".format(X_val_new.shape,y_val_new.shape))\n",
    "print(\"The shape of training data:{} and {} \".format(X_train_new.shape,y_train_new.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to plot the ROC curve later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ROC_curve(model, X_train,y_train, X_val, y_val):\n",
    "\n",
    "    # Creating visualization with the readable labels\n",
    "    visualizer = ROCAUC(model, encoder={1: 'Spruce/Fir', \n",
    "                                        2: 'Lodgepole Pine', \n",
    "                                        3: 'Ponderosa Pine',\n",
    "                                       4: 'Cottonwood/Willow',\n",
    "                                       5: 'Aspen',\n",
    "                                       6: 'Douglas-fir',\n",
    "                                       7: 'Krummholz'})\n",
    "\n",
    "                                        \n",
    "    # Fitting to the training data first then scoring with the test data                                    \n",
    "    visualizer.fit(X_train,y_train)\n",
    "    visualizer.score(X_val, y_val)\n",
    "    visualizer.show()\n",
    "    \n",
    "    return visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'></a>\n",
    "# <font color=darkgreen> 7.ML Algorithms before Dimensionality Reduction  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green> 7.1. Decision Trees  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to plot the trees is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tree(tree, feature_names):\n",
    "    dot_data = StringIO()\n",
    "    export_graphviz(tree, out_file=dot_data, feature_names=feature_names, filled=True, rounded=True,special_characters=True)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "    return Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7.1.1'></a>\n",
    "### <font color=green> 7.1.1. Single Tree <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First attempt is with default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tree = DecisionTreeClassifier(random_state=18)\n",
    "model_tree = single_tree.fit(X_train_new, y_train_new)\n",
    "print(\"Accuracy = {0:.4f}\".format(-np.mean(cross_val_score(single_tree, X_train_new, y_train_new, scoring='accuracy'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(model_tree, X_train_new.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two aspects can be highlighted after taking a look at the tree:\n",
    " - The tree is huge! As we have not set any complexity pruning or max_depth we have allow the tree to grow without any limit\n",
    " - Alpine Climate and Binned elevantion seem to be the most important features in order to predict the value of cover type. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADAPT THIS SO IT CAN BE A MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prune the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'max_depth': range(1,30)}\n",
    "\n",
    "single_tree_a = GridSearchCV(single_tree,\n",
    "                            param_grid,\n",
    "                            scoring='accuracy',\n",
    "                            cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "single_tree_a.fit(X_train_new, y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(single_tree_a.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = single_tree_a.cv_results_['mean_test_score']\n",
    "stds = single_tree_a.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, single_tree_a.cv_results_['params']):\n",
    "    print(\"Accuracy = %0.3f (+/-%0.03f) for %r\" % (-mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I stop here, to be continued later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.errorbar(range(1,30,1), [-m for m in means], yerr=stds, fmt='-o')\n",
    "plt.title('MSE for different Depths', fontsize=20)\n",
    "plt.xlabel(\"Depth\", fontsize=16)\n",
    "plt.ylabel(\"MSE\", fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tree_pruned = DecisionTreeClassifier(random_state=18, max_depth=15)\n",
    "\n",
    "print(\"MSE = {0:.4f}\".format(-np.mean(cross_val_score(single_tree_pruned, X_train_new, y_train_new, scoring='neg_mean_squared_error'))))\n",
    "print(\"Accuracy Train= {0:.4f}\".format(np.mean(cross_val_score(single_tree_pruned, X_train_new, y_train_new))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the plot, the optimal value for the depth of the decision tree is 15. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(single_tree_pruned.fit(X_train_new,y_train_new), X_train_new.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dtreeviz(single_tree_pruned, X_train_new,y_train_new, target_name='Cover_Type', feature_names=X_train_new.columns, \n",
    "        fontname='DejaVu Sans', scale=1.5,label_fontsize=10, fancy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran the same decision tree with tuned hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_classifier = DecisionTreeClassifier(random_state=37)\n",
    "print(\"Accuracy = {0:.4f}\".format(np.mean(cross_val_score(tree_classifier, X_train_new, y_train_new))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_depth\": randint(12,25), # default 100\n",
    "    \"max_leaf_nodes\":[5, 10, 20, 50, 100],\n",
    "    'criterion': [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "forest_hyper = RandomizedSearchCV(tree_classifier, param_distributions=params, random_state=37, n_iter=150, cv=5, verbose=1, n_jobs=-1, return_train_score=True)\n",
    "forest_hyper.fit(X_train_new, y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(forest_hyper.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = forest_hyper.cv_results_['mean_test_score']\n",
    "stds = forest_hyper.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, forest_hyper.cv_results_['params']):\n",
    "    print(\"Accuracy = %0.3f (+/%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_best= forest_hyper.best_estimator_\n",
    "dt_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_classifier = DecisionTreeClassifier(criterion = forest_hyper.best_params_['criterion'], max_depth=forest_hyper.best_params_['max_depth'], max_leaf_nodes= forest_hyper.best_params_['max_leaf_nodes'], random_state=37)\n",
    "\n",
    "# fit the model with the training data\n",
    "model_treeclassifer = tree_classifier.fit(X_train_new,y_train_new)\n",
    "\n",
    "# predict the target on the train dataset\n",
    "predict_tree = model_treeclassifer.predict(X_train_new)\n",
    "print('\\nTarget on train data',predict_tree)\n",
    "\n",
    "# Accuracy Score on train dataset\n",
    "accuracy_tree_train = accuracy_score(y_train_new, predict_tree)\n",
    "print('\\nAccuracy_score on train dataset : ', accuracy_tree_train)\n",
    "\n",
    "# Predict the target on the test dataset\n",
    "predict_tree_test = model_treeclassifer.predict(X_val_new)\n",
    "print('\\nTarget on test data',predict_tree_test) \n",
    "\n",
    "# Accuracy Score on test dataset\n",
    "accuracy_tree_test = accuracy_score(y_val_new,predict_tree_test)\n",
    "print('\\nAccuracy on test dataset : ', accuracy_tree_test)\n",
    "\n",
    "#Best Paramater \n",
    "dt_best = forest_hyper.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.DataFrame(forest_hyper.cv_results_)\n",
    "score_df.nlargest(3,\"mean_test_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and reshape confusion matrix data\n",
    "matrix = confusion_matrix(y_val_new, predict_tree_test)\n",
    "matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Build the plot\n",
    "plt.figure(figsize=(16,7))\n",
    "sns.set(font_scale=1.4)\n",
    "sns.heatmap(matrix, annot=True, annot_kws={'size':10},\n",
    "            cmap=plt.cm.Greens, linewidths=0.2)\n",
    "\n",
    "# Add labels to the plot\n",
    "class_names = ['Spruce/Fir', 'Lodgepole Pine', 'Ponderosa Pine', \n",
    "               'Cottonwood/Willow', 'Aspen', 'Douglas-fir',    \n",
    "               'Krummholz']\n",
    "tick_marks = np.arange(len(class_names))\n",
    "tick_marks2 = tick_marks + 0.5\n",
    "plt.xticks(tick_marks, class_names, rotation=25)\n",
    "plt.yticks(tick_marks2, class_names, rotation=0)\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix for Random Forest Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val_new, predict_tree_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC_curve(tree_classifier, X_train_new,y_train_new, X_val_new, y_val_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = ClassPredictionError(\n",
    "    tree_classifier)\n",
    "visualizer.fit(X_train_new, y_train_new)\n",
    "visualizer.score(X_val_new, y_val_new)\n",
    "visualizer.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7.2'></a>\n",
    "## <font color=green> 7.2. XGB Boost  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train1 = le.fit_transform(y_train_new)\n",
    "y_val1 = le.fit_transform(y_val_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First run, hyperparameters NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8526\n"
     ]
    }
   ],
   "source": [
    "xgboost = xgb.XGBClassifier()\n",
    "print(\"Accuracy = {0:.4f}\".format(np.mean(cross_val_score(xgboost, X_train_new, y_train1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "params = {\n",
    "    \"max_depth\": randint(10,20,2), # default 100\n",
    "    \n",
    "    \"colsample_bytree\": uniform(0.7, 0.85),\n",
    "    \"learning_rate\": uniform(0.03, 0.3), # default 0.1 \n",
    "    \"n_estimators\": randint(200, 400,50), # default 100\n",
    "    \"subsample\": uniform(0.6, 0.95),\n",
    "    \"nthread\": [4] # parallel processing and number of cores in the system should be entered.\n",
    "}\n",
    "\n",
    "\n",
    "xgb_hyper = RandomizedSearchCV(xgboost, param_distributions=params, random_state=37, n_iter=150, cv=5, verbose=1, n_jobs=-1, return_train_score=True)\n",
    "xgb_hyper.fit(X_train_new, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(forest_hyper.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = forest_hyper.cv_results_['mean_test_score']\n",
    "stds = forest_hyper.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, forest_hyper.cv_results_['params']):\n",
    "    print(\"Accuracy = %0.3f (+/%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping is an approach to training complex machine learning models to avoid overfitting.\n",
    "\n",
    "It works by monitoring the performance of the model that is being trained on a separate test dataset and stopping the training procedure once the performance on the test dataset has not improved after a fixed number of training iterations.\n",
    "\n",
    "It avoids overfitting by attempting to automatically select the inflection point where performance on the test dataset starts to decrease while performance on the training dataset continues to improve as the model starts to overfit.\n",
    "\n",
    "Source: https://machinelearningmastery.com/avoid-overfitting-by-early-stopping-with-xgboost-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:18:05] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[08:18:25] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[08:18:45] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[08:19:01] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[08:19:16] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "Accuracy = 0.8633\n",
      "[08:19:33] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"criterion\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephaniegessler/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:793: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "/Users/stephaniegessler/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:793: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-merror:0.18519\n",
      "[1]\tvalidation_0-merror:0.15906\n",
      "[2]\tvalidation_0-merror:0.14980\n",
      "[3]\tvalidation_0-merror:0.14385\n",
      "[4]\tvalidation_0-merror:0.14319\n",
      "[5]\tvalidation_0-merror:0.13856\n",
      "[6]\tvalidation_0-merror:0.13261\n",
      "[7]\tvalidation_0-merror:0.13095\n",
      "[8]\tvalidation_0-merror:0.13393\n",
      "[9]\tvalidation_0-merror:0.13360\n",
      "[10]\tvalidation_0-merror:0.13624\n",
      "[11]\tvalidation_0-merror:0.13194\n",
      "[12]\tvalidation_0-merror:0.13161\n",
      "[13]\tvalidation_0-merror:0.13095\n",
      "[14]\tvalidation_0-merror:0.12996\n",
      "[15]\tvalidation_0-merror:0.12963\n",
      "[16]\tvalidation_0-merror:0.12731\n",
      "[17]\tvalidation_0-merror:0.12632\n",
      "[18]\tvalidation_0-merror:0.12599\n",
      "[19]\tvalidation_0-merror:0.12533\n",
      "[20]\tvalidation_0-merror:0.12202\n",
      "[21]\tvalidation_0-merror:0.12302\n",
      "[22]\tvalidation_0-merror:0.12368\n",
      "[23]\tvalidation_0-merror:0.12302\n",
      "[24]\tvalidation_0-merror:0.12235\n",
      "[25]\tvalidation_0-merror:0.12202\n",
      "[26]\tvalidation_0-merror:0.12302\n",
      "[27]\tvalidation_0-merror:0.12169\n",
      "[28]\tvalidation_0-merror:0.12169\n",
      "[29]\tvalidation_0-merror:0.12136\n",
      "[30]\tvalidation_0-merror:0.12037\n",
      "[31]\tvalidation_0-merror:0.12037\n",
      "[32]\tvalidation_0-merror:0.12004\n",
      "[33]\tvalidation_0-merror:0.11938\n",
      "[34]\tvalidation_0-merror:0.11905\n",
      "[35]\tvalidation_0-merror:0.11938\n",
      "[36]\tvalidation_0-merror:0.12004\n",
      "[37]\tvalidation_0-merror:0.12070\n",
      "[38]\tvalidation_0-merror:0.12037\n",
      "[39]\tvalidation_0-merror:0.11971\n",
      "[40]\tvalidation_0-merror:0.12070\n",
      "[41]\tvalidation_0-merror:0.12169\n",
      "[42]\tvalidation_0-merror:0.12037\n",
      "[43]\tvalidation_0-merror:0.12037\n",
      "[44]\tvalidation_0-merror:0.11971\n",
      "[45]\tvalidation_0-merror:0.12136\n",
      "[46]\tvalidation_0-merror:0.12070\n",
      "[47]\tvalidation_0-merror:0.12136\n",
      "[48]\tvalidation_0-merror:0.12136\n",
      "[49]\tvalidation_0-merror:0.12103\n",
      "[50]\tvalidation_0-merror:0.12037\n",
      "[51]\tvalidation_0-merror:0.12235\n",
      "[52]\tvalidation_0-merror:0.12070\n",
      "[53]\tvalidation_0-merror:0.12235\n",
      "[54]\tvalidation_0-merror:0.12070\n",
      "[55]\tvalidation_0-merror:0.12037\n",
      "[56]\tvalidation_0-merror:0.11971\n",
      "[57]\tvalidation_0-merror:0.12169\n",
      "[58]\tvalidation_0-merror:0.12136\n",
      "[59]\tvalidation_0-merror:0.12269\n",
      "[60]\tvalidation_0-merror:0.12202\n",
      "[61]\tvalidation_0-merror:0.12103\n",
      "[62]\tvalidation_0-merror:0.12037\n",
      "[63]\tvalidation_0-merror:0.12070\n",
      "[64]\tvalidation_0-merror:0.12103\n",
      "[65]\tvalidation_0-merror:0.12070\n",
      "[66]\tvalidation_0-merror:0.12136\n",
      "[67]\tvalidation_0-merror:0.11971\n",
      "[68]\tvalidation_0-merror:0.12004\n",
      "[69]\tvalidation_0-merror:0.11938\n",
      "[70]\tvalidation_0-merror:0.11938\n",
      "[71]\tvalidation_0-merror:0.11905\n",
      "[72]\tvalidation_0-merror:0.11872\n",
      "[73]\tvalidation_0-merror:0.11839\n",
      "[74]\tvalidation_0-merror:0.11905\n",
      "[75]\tvalidation_0-merror:0.11872\n",
      "[76]\tvalidation_0-merror:0.11872\n",
      "[77]\tvalidation_0-merror:0.11905\n",
      "[78]\tvalidation_0-merror:0.11905\n",
      "[79]\tvalidation_0-merror:0.11839\n",
      "[80]\tvalidation_0-merror:0.11739\n",
      "[81]\tvalidation_0-merror:0.11806\n",
      "[82]\tvalidation_0-merror:0.11872\n",
      "[83]\tvalidation_0-merror:0.11806\n",
      "[84]\tvalidation_0-merror:0.11905\n",
      "[85]\tvalidation_0-merror:0.11938\n",
      "[86]\tvalidation_0-merror:0.11905\n",
      "[87]\tvalidation_0-merror:0.11872\n",
      "[88]\tvalidation_0-merror:0.11839\n",
      "[89]\tvalidation_0-merror:0.11839\n",
      "[90]\tvalidation_0-merror:0.11839\n",
      "[91]\tvalidation_0-merror:0.11839\n",
      "[92]\tvalidation_0-merror:0.11806\n",
      "[93]\tvalidation_0-merror:0.11839\n",
      "[94]\tvalidation_0-merror:0.11872\n",
      "[95]\tvalidation_0-merror:0.11872\n",
      "[96]\tvalidation_0-merror:0.11839\n",
      "[97]\tvalidation_0-merror:0.11872\n",
      "[98]\tvalidation_0-merror:0.11905\n",
      "[99]\tvalidation_0-merror:0.11905\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "need to call fit or load_model beforehand",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy = \u001b[39m\u001b[38;5;132;01m{0:.4f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(np\u001b[38;5;241m.\u001b[39mmean(cross_val_score(xgboost, X_train_new, y_train1))))\n\u001b[1;32m      3\u001b[0m xgboost\u001b[38;5;241m.\u001b[39mfit(X_train_new, y_train1, early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerror\u001b[39m\u001b[38;5;124m\"\u001b[39m, eval_set\u001b[38;5;241m=\u001b[39m[(X_val_new, y_val1)])\n\u001b[0;32m----> 4\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mxgboost2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val_new\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_val1, pred);\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy:\u001b[39m\u001b[38;5;132;01m%0.2f\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39m(accuracy\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1434\u001b[0m, in \u001b[0;36mXGBClassifier.predict\u001b[0;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1427\u001b[0m     X: ArrayLike,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1432\u001b[0m     iteration_range: Optional[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1433\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m-> 1434\u001b[0m     class_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mntree_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mntree_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1440\u001b[0m \u001b[43m        \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1441\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_margin:\n\u001b[1;32m   1443\u001b[0m         \u001b[38;5;66;03m# If output_margin is active, simply return the scores\u001b[39;00m\n\u001b[1;32m   1444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m class_probs\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1044\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[0;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1003\u001b[0m     X: ArrayLike,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     iteration_range: Optional[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1009\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;124;03m\"\"\"Predict with `X`.  If the model is trained with early stopping, then `best_iteration`\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;124;03m    is used automatically.  For tree models, when data is on GPU, like cupy array or\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;124;03m    cuDF dataframe and `predictor` is not specified, the prediction is run on GPU\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \n\u001b[1;32m   1042\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m     iteration_range \u001b[38;5;241m=\u001b[39m _convert_ntree_limit(\n\u001b[0;32m-> 1044\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, ntree_limit, iteration_range\n\u001b[1;32m   1045\u001b[0m     )\n\u001b[1;32m   1046\u001b[0m     iteration_range \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iteration_range(iteration_range)\n\u001b[1;32m   1047\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_use_inplace_predict():\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:590\u001b[0m, in \u001b[0;36mXGBModel.get_booster\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__sklearn_is_fitted__():\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NotFittedError\n\u001b[0;32m--> 590\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneed to call fit or load_model beforehand\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\n",
      "\u001b[0;31mNotFittedError\u001b[0m: need to call fit or load_model beforehand"
     ]
    }
   ],
   "source": [
    "xgboost = xgb.XGBClassifier(criterion = \"gini\", max_depth = 20, random_state=37, n_jobs=-1)\n",
    "print(\"Accuracy = {0:.4f}\".format(np.mean(cross_val_score(xgboost, X_train_new, y_train1))))\n",
    "xgboost.fit(X_train_new, y_train1, early_stopping_rounds=40, eval_metric=\"merror\", eval_set=[(X_val_new, y_val1)])\n",
    "pred = xgboost2.predict(X_val_new)\n",
    "\n",
    "accuracy = accuracy_score(y_val1, pred);\n",
    "print ('accuracy:%0.2f%%'%(accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy = {0:.4f}\".format(np.mean(cross_val_score(xgboost, X_val_new, y_val1))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7980\n"
     ]
    }
   ],
   "source": [
    "xgboost2 = xgb.XGBClassifier (missing=np.nan, max_depth=10, n_estimators=350, random_state=37, learning_rate=0.03, nthread=4, subsample=0.95, colsample_bytree=0.85, seed=42)\n",
    "print(\"Accuracy = {0:.4f}\".format(np.mean(cross_val_score(xgboost2, X_val_new, y_val1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7.3'></a>\n",
    "## <font color=green> 7.3. Extra Tree Classifier  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephaniegessler/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:306: UserWarning:\n",
      "\n",
      "The total space of parameters 2 is smaller than n_iter=10. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score:  0.8754\n",
      "The best parameter:  {'n_estimators': 113, 'criterion': 'gini'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ExtraTreesClassifier(n_estimators=113, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ExtraTreesClassifier</label><div class=\"sk-toggleable__content\"><pre>ExtraTreesClassifier(n_estimators=113, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "ExtraTreesClassifier(n_estimators=113, random_state=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a forest and compute the feature importances\n",
    "n_estimators = list(range(50, 250,5))\n",
    "n_estimators = [113]\n",
    "criterion=['gini','entropy']\n",
    "#min_samples_leaf = list(range(5, 25))\n",
    "#min_samples_split = list(range(5, 25))\n",
    "#max_depth = list(range(8, 50))\n",
    "max_depth = [22]\n",
    "# create a parameter grid: map the parameter names to the values that should be searched\n",
    "param_grid = dict(n_estimators=n_estimators, criterion=criterion)\n",
    "\n",
    "forest = ExtraTreesClassifier(random_state=0)\n",
    "grid_etc = RandomizedSearchCV(forest, param_grid, cv=5, scoring=\"accuracy\" ,return_train_score=False)\n",
    "grid_etc.fit(X_train_new, y_train_new)\n",
    "print(\"The best score: \",grid_etc.best_score_.round(4))\n",
    "#Parameter setting that gave the best results on the hold out data.\n",
    "print(\"The best parameter: \",grid_etc.best_params_)\n",
    "grid_etc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score of the ExtraTreesClassifier with the selected features:  0.89\n"
     ]
    }
   ],
   "source": [
    "#Find Feature importance\n",
    "etc_selected = ExtraTreesClassifier(n_estimators=grid_etc.best_params_['n_estimators'],\n",
    "                            criterion=grid_etc.best_params_['criterion'],random_state=0)\n",
    "etc_selected.fit(X_train_new, y_train_new)\n",
    "etc_sel_acc = etc_selected.score(X_val_new,y_val_new)\n",
    "print(\"The accuracy score of the ExtraTreesClassifier with the selected features: \",round(etc_sel_acc,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8125\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = {0:.4f}\".format(np.mean(cross_val_score(etc_selected, X_val_new, y_val_new))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7.4'></a>\n",
    "## <font color=green> 7.4. Random Forest  <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=20)\n",
    "model_forest = forest.fit(X_train_new,y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.score(X_val_new,y_val_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for the test set\n",
    "y_pred_test_forest = forest.predict(X_val_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and reshape confusion matrix data\n",
    "matrix = confusion_matrix(y_val_new, y_pred_test_forest)\n",
    "matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Build the plot\n",
    "plt.figure(figsize=(16,7))\n",
    "sns.set(font_scale=1.4)\n",
    "sns.heatmap(matrix, annot=True, annot_kws={'size':10},\n",
    "            cmap=plt.cm.Greens, linewidths=0.2)\n",
    "\n",
    "# Add labels to the plot\n",
    "class_names = ['Spruce/Fir', 'Lodgepole Pine', 'Ponderosa Pine', \n",
    "               'Cottonwood/Willow', 'Aspen', 'Douglas-fir',    \n",
    "               'Krummholz']\n",
    "tick_marks = np.arange(len(class_names))\n",
    "tick_marks2 = tick_marks + 0.5\n",
    "plt.xticks(tick_marks, class_names, rotation=25)\n",
    "plt.yticks(tick_marks2, class_names, rotation=0)\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix for Random Forest Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val_new, y_pred_test_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=20, random_state=37)\n",
    "print(\"Accuracy = {0:.4f}\".format(np.mean(cross_val_score(forest, X_train_new, y_train_new))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7.4.1'></a>\n",
    "### <font color=green> 7.4.1 Bagging <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, max_samples=100, \n",
    "                               bootstrap=True, n_jobs = 1)\n",
    "bag_clf.fit(X_train_new, y_train_new)\n",
    "y_pred = bag_clf.predict(X_val_new)\n",
    "print(\"Accuracy = {0:.4f}\".format(np.mean(cross_val_score(bag_clf, X_train_new, y_train_new))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train_new\n",
    "y = y_train_new\n",
    "tree_1 = DecisionTreeClassifier(random_state=42)\n",
    "print(\"Accuracy = {0:.4f}\".format(np.mean(cross_val_score(tree_1, X, y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7.5'></a>\n",
    "  ## <font color=green> 7.5. KNN  <font>\n",
    "KNN is better on smaller dataset, however we will evaluate the performance of this model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out the best value for k we will use a for loop, and with the range from 1 to 200 range (1,200,10). I reduced the range to 10 but will use iterations of 1. And it gives me the best K = 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_neighbors_list = range (1,100,1)\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for k in k_neighbors_list:\n",
    "    clf = KNeighborsClassifier(n_neighbors=k, p=1) # using manhattan_distance \n",
    "    clf.fit(X_train_new, y_train_new)\n",
    "    y_pred_test = clf.predict(X_val_new)\n",
    "    acc_k = accuracy_score(y_val_new, y_pred_test)\n",
    "    \n",
    "    results_list.append({\n",
    "        \"k\": k,\n",
    "        \"acc\": acc_k\n",
    "    })\n",
    "    \n",
    "results = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p = 1 (Manhatten Distance), the best k = 8 with accuracy= 0.759259\n",
    "p = 2 (Euclidean Distance), the best k = 1 with accuracy = 0.761905"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the best value for K and accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.658069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.636574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.634590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.628307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.627646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>94</td>\n",
       "      <td>0.520833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>0.520503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>0.519180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>0.517196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>0.515212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     k       acc\n",
       "0    1  0.658069\n",
       "2    3  0.636574\n",
       "3    4  0.634590\n",
       "5    6  0.628307\n",
       "4    5  0.627646\n",
       "..  ..       ...\n",
       "93  94  0.520833\n",
       "96  97  0.520503\n",
       "95  96  0.519180\n",
       "97  98  0.517196\n",
       "98  99  0.515212\n",
       "\n",
       "[99 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = results.sort_values('acc', ascending = False) \n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Testing Accuracy')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(results['k'],results['acc'])\n",
    "plt.xlabel('Valie for k for KNN')\n",
    "plt.ylabel('Testing Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_neighbors_list = range (1,40,1)\n",
    "\n",
    "K_scores = []\n",
    "\n",
    "for k in k_neighbors_list:\n",
    "    # Run KNeighborsClassifier with k neighbours \n",
    "    knn = KNeighborsClassifier(n_neighbors=k, p=1) # using manhattan_distance \n",
    "    # Obtain Cross_val_score for KneighborsClassifier with k neighbors \n",
    "    scores = cross_val_score(knn,X_train_new, y_train_new, cv= 5, scoring= \"accuracy\")\n",
    "    # Append mean of scores for K neighbors to k_scores list \n",
    "    K_scores.append({\n",
    "        \"k\": k,\n",
    "        \"acc\": scores.mean\n",
    "    })\n",
    "    \n",
    "results = pd.DataFrame(K_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = results.sort_values(K_scores, ascending = False) \n",
    "#test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Grridsearch and cross validation we will adapt the model. <br>\n",
    "We first create a KNN classifier instance and then prepare a range of values of hyperparameter K from 1 to 31 that will be used by GridSearchCV to find the best value of K.\n",
    "\n",
    "Furthermore, we set our cross-validation batch sizes cv = 5 and set scoring metrics as accuracy as our preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_valid_scores = {}\n",
    "parameters = {\n",
    "    \"n_neighbors\": list(range(1,20,1)),\n",
    "    \"leaf_size\":list(range(1,100,20)),\n",
    "    \"metric\":[\"euclidean\",\"manhattan\"]    \n",
    "}\n",
    "\n",
    "model_KNN = KNeighborsClassifier()\n",
    "\n",
    "model_KNN = GridSearchCV(\n",
    "    model_KNN,\n",
    "    parameters,\n",
    "    cv=5,\n",
    "    scoring = \"accuracy\"\n",
    "\n",
    ")\n",
    "KNN_results = model_KNN.fit(X_train_new, y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_results.best_score_\n",
    "KNN_results.best_estimator_\n",
    "KNN_results.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = KNN_results.best_score_ *100\n",
    "print(\"Accuracy for our training dataset with tuning is : {:.2f}%\".format(accuracy) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy = {0:.4f}\".format(np.mean(cross_val_score(model_KNN, X_train_new, y_train_new))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy = {0:.4f}\".format(np.mean(cross_val_score(model_KNN, X_val_new, y_val_new))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the best model to calculate the mean cross validation score to compare the model later with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1,leaf_size=1,metric='euclidean')\n",
    "\n",
    "scores = cross_val_score(knn, X_train_new, y_train_new, cv = 10, scoring = \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy = {0:.4f}\\n\".format(accuracy_score(model_KNN.predict(X_val_new), y_val_new)))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(model_KNN.predict(X_val_new), y_val_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model_KNN,X_val_new, y_val_new,cmap=plt.cm.Blues,values_format='d' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_compare = classifiers_compare.append({'Algorithm': \"KNN\", 'Mean CV Score':(scores.mean())},ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kNN is simple to use and can return very accurate and meaningful results. It also has a tendency to be computationally expensive, so it may not be your best choice for larger datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7.6'></a>\n",
    "## <font color=green> 7.6. SVM  <font>\n",
    "The multiclass problem is broken down to multiple binary classification cases, which is also called one-vs-one. In scikit-learn one-vs-one is not default and needs to be selected explicitly (as can be seen further down in the code). One-vs-rest is set as default. It basically divides the data points in class x and rest. Consecutively a certain class is distinguished from all other classes.  The penalty term C is set to 1 for all classifiers. For the multiclass classification, the type one-versus-one is specified, as can be seen in decision_function_shape=â€™ovoâ€™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr_clf = OneVsRestClassifier(svm.SVC(C=10,kernel='rbf'))\n",
    "ovr_clf.fit(X_train_new, y_train_new)\n",
    "print(\"SVM Accuracy = {:.4f}\".format(np.mean(cross_val_score(ovr_clf, X_train_new, y_train_new))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Model output is not very good, lets fine tune the SVM parameters to further improve the model performance. \n",
    "_C_ is the balance between margin violations (low C) or better generalization of the models. _Gamma_ acts like a regularization hyperparamter. If the model is overfitting it is advisable to redeuce it.If it is underfitting you should increase it. The best Kernel to use if the dataset is large is _Gaussoan RBF kernel_. Since we already used Polynominal in our dataset we will disregard this option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_set = OneVsRestClassifier(SVC(kernel=\"rbf\"))\n",
    "\n",
    "parameters = {\n",
    "    \"estimator__C\": [1,10,100],\n",
    "    \"estimator__gamma\":[0.01,0.0001,0.00001],\n",
    "}\n",
    "\n",
    "estimator = GridSearchCV(model_to_set, param_grid=parameters, cv=5, n_jobs=-1)\n",
    "\n",
    "estimator.fit(X_train_new, y_train_new)\n",
    "\n",
    "print(estimator.best_score_)\n",
    "print (estimator.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (estimator.best_params_, estimator.best_score_))\n",
    "\n",
    "\n",
    "scores = estimator.cv_results_['mean_test_score'].reshape(len(parameters['estimator__C']), len(parameters['estimator__gamma']))\n",
    "\n",
    "# Draw heatmap of the validation accuracy as a function of gamma and C\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n",
    "plt.imshow(scores, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('C')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(parameters['estimator__gamma'])), parameters['estimator__gamma'], rotation=45)\n",
    "plt.yticks(np.arange(len(parameters['estimator__C'])), parameters['estimator__C'])\n",
    "plt.title('Validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = estimator.best_estimator_\n",
    "Y_pred = final_model.predict(X_val_new)\n",
    "print(\"Training set score for SVM: %f\" % final_model.score(X_train_new , y_train_new))\n",
    "print(\"Testing  set score for SVM: %f\" % final_model.score(X_val_new  , y_val_new ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(final_model, X_train_new, y_train_new, cv = 10, scoring = \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(final_model.predict(X_val_new), y_val_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(final_model,X_val_new, y_val_new,cmap=plt.cm.Blues,values_format='d' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_compare = classifiers_compare.append({'Algorithm': \"SVM\", 'Mean CV Score':(scores.mean())},ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7.2'></a>\n",
    "## <font color=green> 7.7. Naive Bayes  <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a classification problem with multiple discrete values we use the multinominal Naive Bayes method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "mnb = BernoulliNB()\n",
    "\n",
    "# Train the model using the training sets\n",
    "mnb.fit(X_train_new,y_train_new)\n",
    "\n",
    "#Predict Output\n",
    "y_pred= mnb.predict(X_val_new)\n",
    "scores = cross_val_score(mnb,X_train_new,y_train_new,cv = 5, scoring = \"accuracy\")\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Mean Accuracy:\",scores.mean())\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_val_new, y_pred))\n",
    "\n",
    "classifiers_compare = classifiers_compare.append({'Algorithm': \"Naive Bayes\", 'Mean CV Score':(scores.mean())},ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7.8'></a>\n",
    "## <font color=green> 7.8. Logistic Regression <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_log = y_train_log.values.ravel()\n",
    "y_val_log = y_val_log.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Logistic Regression with only the New PCA reduced dataset provided & L1 Regularization\n",
    "C_range=[0.001,0.01,0.02,0.04]\n",
    "scores_logreg=[]\n",
    "for c in C_range:\n",
    "    #Saga is good for larger Datasets \n",
    "    logreg = LogisticRegression(C=c, penalty ='l1', solver='saga', multi_class='multinomial',max_iter=5000)\n",
    "    logreg.fit(X_train_log, y_train_log)\n",
    "    y_pred = logreg.predict(X_val_log)\n",
    "    scores_logreg.append(metrics.accuracy_score(y_val_log,y_pred))\n",
    "\n",
    "    \n",
    "#Plot Logistic Regression with only the data provided\n",
    "plt.plot(C_range,scores_logreg)\n",
    "plt.xlabel(\"Values of Regularization Parameter\")\n",
    "plt.ylabel(\"Testing accuracy with Feature Engineering\")\n",
    "max(scores_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7.9'></a>\n",
    "## <font color=green> 7.9. Ensemble methods<font>\n",
    "\n",
    "__Voting Classifier__   \n",
    "\n",
    "A very simple way to create an even better classifier is to aggregate the best predictions of each classifier and predict the class that gets the most votes. Normally this method is a better predictor than the single models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclf = VotingClassifier(estimators=[\n",
    "    ('etc', etc_selected), ('knn',KNN_results),('xgboost',xgboost2)],voting='hard')\n",
    "eclf.fit(X_train_new, y_train1)\n",
    "y_val_pred = eclf.predict(X_val_new)\n",
    "print(\"Accuracy achieved by ensembling\",metrics.accuracy_score(y_val1,y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='9'></a>\n",
    "#  <font color=darkgreen>9.Final Submission </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pred=forest.predict(data_test.drop(labels=['Id','Wilderness','Soil'], axis=1))\n",
    " \n",
    " \n",
    " \n",
    "submission=pd.DataFrame(data=pred,columns=['Cover_Type'])\n",
    "submission\n",
    " \n",
    " \n",
    " \n",
    "submission['Id']=data_test['Id']\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
